\makeheading{Lecture 5}{\printdate{2022-09-21}}%chktex 8
\begin{Definition}{}{}
    Fix some event $ B\in\mathcal{F} $ with $ \Prob{B}>0 $.
    Define $ \mu\colon \mathcal{F}\to\mathbf{R} $ by
    \[ \mu(A)=\Prob{A\given B}. \]
\end{Definition}
\begin{Theorem}{}{}
    $ \mu $ is a probability measure on $ (\Omega,\mathcal{F}) $.
    Conditional probabilities are a probability measure.
    \tcblower{}
    \textbf{Proof}: We need to check properties (i)--(iii)
    for $ \mu $.
    \begin{enumerate}[(i)]
        \item \begin{align*}
                  \Prob{\Omega}
                   & =\Prob{\Omega\given B}                \\
                   & =\frac{\Prob{\Omega\cap B}}{\Prob{B}} \\
                   & =\frac{\Prob{B}}{\Prob{B}}            \\
                   & =1.
              \end{align*}
        \item $ \forall A\in\mathcal{F} $,
              $ \mu(A)=\dfrac{\Prob{A\cap B}}{\Prob{B}}\ge 0 $.
        \item Suppose $ A_1,A_2,\ldots $ are disjoint events.
              Then,
              \begin{align*}
                  \mu(A_1\cup A_2\cup\cdots)
                   & =\Prob{A_1\cup A_2\cdots\given B}                                 \\
                   & =\frac{\Prob{(A_1\cup A_2\cup\cdots)\cap B}}{\Prob{B}}            \\
                   & =\frac{\Prob{(A_1\cap B)\cup (A_2\cap B)\cup \cdots }}{\Prob{B}}.
              \end{align*}
              Note that for all $ 1\le i<j $, $ (A_i\cap B)\cap (A_j\cap B)=
                  A_i\cap A_j\cap B=\emptyset\cap B=\emptyset $,
              so the events $ (A_1\cap B),(A_2\cap B),\ldots $ are pairwise disjoint.
              Thus, by the countable additivity of $ \mathbb{P} $,
              \begin{align*}
                  \mu(A_1\cup A_2\cup\cdots)
                   & =\frac{\Prob{A_1\cap B}+\Prob{A_2\cap B}+\cdots}{\Prob{B}} \\
                   & =\mu(A_1)+\mu(A_2)+\cdots,
              \end{align*}
              as desired.
    \end{enumerate}
\end{Theorem}
\begin{Remark}{Expected value of Geometric Series}{}
    \begin{align*}
        \frac{\E{X}-(1-p)\E{X}}{p}
         & =\sum_{k=1}^{\infty}k(1-p)^{k-1}-\sum_{j=1}^{\infty}j(1-p)^j                             \\
         & =\sum_{j=0}^{\infty}(j+1)(1-p)^j-\sum_{j=1}^{\infty}j(1-p)^j &  & \text{sum index }j=k+1 \\
         & =1\cdot(1-p)^0+\sum_{j=1}^{\infty}(1-p)^j[(j+1)-j]                                       \\
         & =1+\frac{(1-p)^1}{1-(1-p)}                                                               \\
         & =1+\frac{1-p}{p}                                                                         \\
         & =1+\frac{1}{p}-\frac{p}{p}                                                               \\
         & =\frac{1}{p}.
    \end{align*}
    Therefore, we have
    \begin{align*}
        \E{X}\frac{1-(1-p)}{p} & =\frac{1}{p}  \\
        \E{X}\frac{p}{p}       & =\frac{p}{p}  \\
        \E{X}                  & =\frac{1}{p}.
    \end{align*}
\end{Remark}
\begin{Example}{}{}
    Roll a 6-sided die until we get a 6. Let $ X= $ the number of rolls.
    Let $ B=\Set{\text{all rolls are even numbers}} $.
    What is $ \E{X\given B} $?
    \tcblower{}
    \textbf{Solution}:
    \[ \Prob{B\given X=k}=\biggl(\frac{2}{5}\biggr)^{\!k-1}. \]
    Hence,
    \begin{align*}
        \Prob{B}
         & =\sum_{k=1}^{\infty}\Prob{\Set{X=k}}\Prob{B\given X=k}                                            \\
         & =\sum_{k=1}^{\infty}\biggl(\frac{5}{6}\biggr)^{\!k-1}\frac{1}{6}\biggl(\frac{2}{5}\biggr)^{\!k-1} \\
         & =\frac{1}{6}\sum_{k=1}^{\infty}\biggl(\frac{2}{6}\biggr)^{\!k-1}                                  \\
         & =\frac{1}{6}\cdot\frac{1}{1-1/3}                                                                  \\
         & =\frac{1}{6}\cdot\frac{3}{2}                                                                      \\
         & =\frac{1}{4}.
    \end{align*}
    Hence,
    \begin{align*}
        \E{X\given B}
         & =\sum_{k=1}^{\infty}k\Prob{X=k\given B}                                                                                                               \\
         & =\sum_{k=1}^{\infty}k \frac{\Prob{\Set{X=k}\cap B}}{\Prob{B}}                                                                                         \\
         & =\frac{1}{\Prob{B}}\sum_{k=1}^{\infty}k \Prob{\Set{X=k}}\Prob{B\given X=k}                                                                            \\
         & =\frac{1}{1/4}\sum_{k=1}^{\infty}k \frac{1}{6}\biggl(\frac{1}{3}\biggr)^{\!k-1}                                                                       \\
         & =4\cdot \frac{1}{6}\cdot \frac{3}{2}\underbrace{\sum_{k=1}^{\infty}k\biggl(\frac{1}{3}\biggr)^{\!k-1}\frac{2}{3}}_{\text{EV of $\GEO*{\frac{2}{3}}$}} \\
         & =4\cdot \frac{1}{6}\cdot \frac{3}{2}\cdot \frac{3}{2}                                                                                                 \\
         & =\frac{3}{2}.
    \end{align*}
\end{Example}
\begin{Theorem}{}{}
    $ p\colon S\to\mathbf{R} $ is a PMF for some RV if and only if
    \begin{enumerate}[(i)]
        \item $ p(v)\ge 0 $ for all $ v\in S $, and
        \item $ \sum_{v\in S}p(v)=1 $.
    \end{enumerate}
    \underline{Remark}: This implies that $ \Set{v\in S\given p(s)>0\text{ is countable}} $.
\end{Theorem}
\begin{Exercise}{}{}
    The sum of uncountably infinitely many positive numbers always
    diverges to infinity.
\end{Exercise}
\begin{Theorem}{}{}
    $ f\colon \mathbf{R}\to\mathbf{R} $ is a PDF for some RV if and only if
    \begin{enumerate}[(i)]
        \item $ f(x)\ge 0 $ for all $ x\in\mathbf{R} $, and
        \item $ \int_{-\infty}^{\infty}f(x)\odif{x}=1 $.
    \end{enumerate}
\end{Theorem}
\begin{Example}{}{}
    Let $ U\sim \text{Uniform}[0,1] $. The PDF is
    \[ f_U(t)=\begin{cases}
            1, & 0\le t\le 1,      \\
            0, & \text{otherwise}.
        \end{cases}. \]
    Technically, the derivative doesn't exist at $0$ since there's a
    change of direction, but it doesn't matter since we only integrate
    PDFs.
\end{Example}
\begin{Example}{}{}
    Let $ U \sim \text{Uniform}[0,1/2] $. The PDF is
    \[ f_U(t)=\begin{cases}
            2, & 0\le t\le 1/2,    \\
            0, & \text{otherwise}.
        \end{cases} \]
\end{Example}
\begin{Definition}{}{}
    A standard logistic distribution is defined by the CDF
    \[ F(t)=\frac{1}{1+e^{-t}}. \]
    The PDF is
    \[ f_X(t)=\odv{F}{x}=(-1)\frac{1}{(1+e^{-t})^2}(-e^{-t})=\frac{e^{-t}}{(1+e^{-t})^2}. \]
    The PDF looks like a bell curve, but with heavier tails.
\end{Definition}
\begin{Example}{}{}
    Calculate $ \Prob{\Set{-1\le X\le 1}} $ for the standard logistic distribution.
    \tcblower{}
    \textbf{Solution}:
    \begin{itemize}
        \item \underline{Method 1}:
              \[ \Prob{\Set{-1\le X\le 1}}=F(1)-F(-1). \]
        \item \underline{Method 2}:
              \[  \Prob{\Set{-1\le X\le 1}}=\int_{-1}^{1}f(t)\odif{t}. \]
    \end{itemize}
\end{Example}
\begin{Example}{}{}
    Calculate $ \E{X} $ for the standard logistic distribution.
    \tcblower{}
    \textbf{Solution}:
    \begin{align*}
        \E{X}
         & =\int_{-\infty}^{\infty}t f(t)\odif{t}                        \\
         & =\int_{-\infty}^{\infty}\frac{t e^{-t}}{(1+e^{-t})^2}\odif{t} \\
         & =\text{IBP}.
    \end{align*}
\end{Example}
\makeheading{Lecture 6}{\printdate{2022-09-23}}%chktex 8
\begin{Definition}{}{}
    If $ f $ is a function $ f\colon A\to B $, then
    the \textbf{pre-image} of a set
    $ C\subseteq B $ under $ f $ is
    \[ f^{-1}(C)=\Set{x\in A\given f(x)\in C}. \]
    The \textbf{image} of a set $ D\subseteq A $
    under $ f $ is
    \[ f(D)=\Set{f(x)\given x\in D}. \]
\end{Definition}
\begin{Example}{}{preim}
    If $ f\colon \mathbf{R}\to\mathbf{R} $ is the function
    $ f(x)=x^2 $, then the pre-image
    \begin{align*}
        f^{-1}\bigl([0,4]\bigr)   & =[-2,2].            \\
        f^{-1}\bigl([1,9]\bigr)   & =[-3,-1]\cup [1,3]. \\
        f^{-1}\bigl([-5,-2]\bigr) & =\emptyset.
    \end{align*}
\end{Example}
\begin{Remark}{}{}
    $\forall C,D\subseteq A $,
    \[ f(C\cup D)=f(C)\cup f(D). \]
    It is \underline{not} always the case (for non-injective functions)
    that
    \[ f(C\cap D)=f(C)\cap f(D). \]
    In~\ref{ex:preim}, if we consider $ C=[-2,-1] $ and $ D=[1,2] $, then
    $ C\cap D=\emptyset $, $ f(C\cap D)=\emptyset $, and
    $ f(C)\cap f(D)=[1,4]\cap [1,4]=[1,4] $.
\end{Remark}
\begin{Proposition}{}{}
    \begin{align*}
        f^{-1}(C\cup D)=f^{-1}(C)\cup f^{-1}(D). \\
        f^{-1}(C\cap D)=f^{-1}(C)\cap f^{-1}(D).
    \end{align*}
    \tcblower{}
    \textbf{Proof}: Exercise.
\end{Proposition}
Suppose $ X\colon \Omega\to\mathbf{R} $ is a discrete
random variable and $ Y=g(X) $ for some
$ g\colon\mathbf{R}\to\mathbf{R} $.
How would we find the PMF of $ Y $ using the PMF $ p_X $ of $ X $?
\begin{align*}
    p_Y(9)
     & =\Prob{\Set{Y=9}}                   \\
     & =\Prob{\Set{X\in g^{-1}(\Set{9})}}  \\
     & =\sum_{j\in g^{-1}(\Set{9})}p_X(j).
\end{align*}
In general,
\[ p_Y(k)=\sum_{j\in g^{-1}(\Set{k})}p_X(j). \]
\begin{Theorem}{}{}
    If $ Y=g(X) $ for some random variable $ X $
    and some (measurable) function $ g\colon\mathbf{R}\to\mathbf{R} $,
    then for any set of $ A\subseteq \mathbf{R} $,
    \[ \Prob{\Set{Y\in A}}=\Prob{\Set{X\in g^{-1}(A)}}. \]
\end{Theorem}
\begin{Example}{}{}
    Suppose $ g(x)=\sqrt{x} $, $ X $ is a non-negative random variable,
    and $ Y=\sqrt{X} $. Find the CDF of $ Y $.
    \tcblower{}
    \textbf{Solution}:
    \begin{align*}
        F_Y(v)
         & =\Prob{\Set{Y\le v}}        &  & v\ge 0 \\
         & =\Prob{\Set{\sqrt{X}\le v}}             \\
         & =\Prob{\Set{X\le v^2}}                  \\
         & =F_X(v^2).
    \end{align*}
    $ \sqrt{x} $ was a monotone increasing function, so it
    preserved the inequality.
\end{Example}
\begin{Theorem}{}{}
    Let $ Y=g(X) $.
    \begin{itemize}
        \item If $ g(X) $ is a strictly increasing function, then
              \[ F_Y(v)=F_X(g^{-1}(v)). \]
        \item If $ g(X) $ is a strictly decreasing function, then
              \[ F_Y(v)=1-F_X(g^{-1}(v)). \]
    \end{itemize}
\end{Theorem}
\begin{Example}{}{}
    Let $ X\sim \text{Uniform}[0,1] $.
    \[ f_X(t)=\begin{cases}
            1, & t\in[0,1],    \\
            0, & t\notin[0,1],
        \end{cases}\qquad
        F_X(t)=\begin{cases}
            0, & t<0,       \\
            t, & t\in[0,1], \\
            1, & t>1.
        \end{cases} \]
    If $ Y=-\log{X} $. Note that $ \log{1}=0 $ and $ \lim\limits_{{t} \to {0}}\log{t}=-\infty $.
    Also,
    \[ g(x)=-\log{x}\iff -g(x)=\log{x}\iff x=e^{-g(x)}, \]
    so $ g^{-1}(v)=e^{-v} $.
    For $ v\ge 0 $,
    \begin{align*}
        F_Y(v)
         & =1-F_X(g^{-1}(v)) \\
         & =1-F_X(e^{-v})    \\
         & =1-e^{-v}.
    \end{align*}
    Hence, $ Y $ is a continuous RV with PDF
    \[ f_Y(v)=\odv*{
            \begin{cases}
                0,        & v<0,   \\
                1-e^{-v}, & v\ge 0
            \end{cases}
        }{v}=\begin{cases}
            0,      & v<0,    \\
            e^{-v}, & v\ge 0.
        \end{cases} \]
    Thus, $ Y\sim \EXP{1} $.
\end{Example}
\begin{Definition}{}{}
    The \textbf{quantile function} of a random variable $ X $
    is the right-continuous (almost) left-inverse of the CDF of $ X $,
    \[ Q_X(v)=\inf\Set{t\in\mathbf{R}:F_X(t)> v}. \]
    Hence, if $ F_X $ is strictly increasing at $ t $, then
    \[ Q_X(F_X(t))=t. \]
    $ Q_X(90\%) $ is the $ 90\textsuperscript{th} $ percentile
    of the value of $ X $ --- the value that $ X $ is less than 90\% of the time.
\end{Definition}
\begin{Theorem}{}{}
    If $ U\sim\text{Uniform}[0,1] $ and $ F $ is a continuous CDF that is
    strictly increasing, then
    $ F^{-1}(U) $ is a random variable whose CDF is $ F $.
\end{Theorem}
\begin{Remark}{}{}
    Suppose $ X $ is a continuous random variable, $ g $ is a differentiable and
    strictly increasing. Before, we had $ Y=g(X) $,
    $ F_Y(v)=F_X\circ g^{-1}(v) $, so the PDF of $ Y $ is
    \begin{align*}
        f_Y(v)
         & =\odv*{F_X(g^{-1}(v))}{v}                      \\
         & =f_X(g^{-1}(v))(g^{-1})' (v).                  \\
         & =f_X(g^{-1}(v))\frac{1}{g'(g^{-1}(v))}         \\
         & =\frac{f_X\circ g^{-1}(v)}{g'\circ g^{-1}(v)}.
    \end{align*}
    You can think of it as taking
    the reflection along the line $ y=x $ for $ g $.

    If $ g $ is differentiable and strictly decreasing, then
    \[ f_Y(v)=-\frac{f_X\circ g^{-1}(v)}{g'\circ g^{-1}(v)}. \]

    We can simplify these formulas for \underline{any} differentiable
    function $ g $ (strictly increasing or decreasing) as
    \[ f_Y(v)=\frac{f_X\circ g^{-1}(v)}{\abs{g'\circ g^{-1}(v)}}. \]
\end{Remark}
\begin{Example}{}{}
    What if our function is neither strictly increasing nor decreasing?
    In general,
    \[ f_Y(v)=\sum_{f\in g^{-1}(\Set{v})}\frac{f_X(t)}{\abs{g'(t)}}, \]
    for all
    $ t $ such that $ g(t)=v $.
    We require that $ g $ is differentiable, and $ g $ is
    not constant on any interval.

    Let $ X \sim \text{Uniform}\interval[open right]{0}{2\pi}$, and $ Y=\sin^2(X) $.
    Find $ \Prob{\Set{Y\le t}} $.
\end{Example}
\begin{Definition}{Expectation}{}
    If $ X $ is discrete, then
    \[ \E{X}=\sum_v v\Prob{\Set{X=v}}=\sum_v p_X(v). \]
    If $ X $ is continuous, then
    \[ \E{X}=\int_{-\infty}^{\infty}x f_X(x)\odif{x}. \]
    Furthermore, if $ X $ is discrete then
    \[ \E{g(X)}=\sum_v g(v)p_X(v), \]
    or if $ X $ is continuous then
    \[ \E{g(X)}=\int_{-\infty}^{\infty}g(x)f_X(x)\odif{x}. \]
\end{Definition}
\begin{Definition}{Variance}{}
    The variance (or $ 2\textsuperscript{nd} $
    central moment) of a random variable $ X $ is
    \[ \Var{X}=\E{X^2}=\E{X}^2=\E*{(X-\E{X})^2}. \]
\end{Definition}
\begin{Definition}{Moments}{}
    For an integer $ p\ge 1 $, the $ p\textsuperscript{th} $
    moment of $ X $ is $ \E{X^p} $. The $ p\textsuperscript{th} $
    central moment of $ X $ is
    $ \E*{(X-\E{X})^p} $.
\end{Definition}
\begin{Definition}{Moment Generating Function (MGF)}{}
    The \textbf{moment generating function} (MGF) of a random variable
    $ X $ is the function
    \[ M_X(t)=\E{e^{tX}}. \]
\end{Definition}
\begin{Remark}{}{}
    For each value of $ t $ that we plug in, we're calculating a different
    expected value. Why?
    \begin{enumerate}[(1)]
        \item The MGF uniquely specifies the probability distribution.
        \item Grants easy access to all moments.
        \item Easy to handle sums of independent random variables.
    \end{enumerate}
\end{Remark}
\begin{Theorem}{}{}
    Suppose $ X $ and $ Y $ are random variables and
    their MGFs are both defined (integrals exist)
    in some interval $ (-\delta,\delta) $ for some $ \delta>0 $.
    If $ M_X(t)=M_Y(t) $ for all $ -\delta<t<\delta $, then
    $ X\dist Y $.
\end{Theorem}
\begin{Theorem}{}{}
    Suppose $ X,X_1,X_2\ldots $ all have MGFs that are defined
    on $ (-\delta,\delta) $ for some $ \delta>0 $. If
    $ M_{X_n}(t)\to M_X(t) $ as $ n\to\infty $
    for all $ -\delta<t<\delta $, then
    $ F_{X_n}(x)\to F_X(x) $ as $ n\to\infty $
    for all $ x\in\mathbf{R} $.
\end{Theorem}
\begin{Theorem}{}{mgf_0}
    For $ p\ge 1 $, if the MGF of $ X $ is
    differentiable $ p $ times at $ t=0 $, then
    \[ \E{X^p}=M_X^{(p)}(0). \]
    \tcblower{}
    \textbf{(Rough) Proof}:
    \[ \biggl(\frac{d}{dt}\biggr)^p M_X(t)
        =\biggl(\frac{d}{dt}\biggr)^p \E{e^{tX}}
        \underbrace{=}_{\text{next lecture}}
        \E*{\biggl(\frac{d}{dt}\biggr)^p e^{tX}}=
        \E{X^p e^{tX}}. \]
    At $ t=0 $, this is $ \E{X^p\cdot 1}=\E{X^p} $.
\end{Theorem}
\begin{Example}{}{}
    Let $ G\sim \GEO{p} $. Find the MGF
    of $ G $, and then calculate the first moment.
    \tcblower{}
    \textbf{Solution}: The MGF is given by
    \begin{align*}
        M_G(t)
         & =\E{e^{tG}}                                \\
         & =\sum_{k=1}^{\infty}e^{tk}(1-p)^{k-1}p     \\
         & =\sum_{k=1}^{\infty}(e^{t})^k(1-p)^{k-1}p  \\
         & =p e^t \sum_{k=1}^{\infty}((1-p)e^t)^{k-1} \\
         & =p e^t \frac{1}{1-(1-p)e^t}                \\
         & =\frac{pe^t}{1-(1-p)e^t}                   \\
         & =\frac{p}{e^{-t}-1+p}.
    \end{align*}
    We can calculate the first moment (expected value) as follows:
    \[ M_G'(t)=(-1)\frac{p}{(e^{-t}-1+p)^2}(-e^{-t})
        \implies M_G'(0)=\frac{p}{(1-1+p)^2}(1)=\frac{p}{p^2}=\frac{1}{p}. \]
\end{Example}
\begin{Theorem}{}{}
    If $ X_1,\ldots,X_n $ are jointly independent random variables,
    $ S=X_1+X_2+\cdots+X_n $, and these random variables' MGFs are
    all defined at some value $ t $, then
    \[ M_S(t)=M_{X_1}(t) M_{X_2}(t)\cdots M_{X_n}(t). \]
    \tcblower{}
    \textbf{Proof}: Since $ X_1,\ldots,X_n $ are jointly independent,
    we have
    \[ \E{e^{tS}}=
        \E{e^{t(X_1+\cdots+X_n)}}=\E{e^{tX_1+\cdots+tX_n}}=
        \E{e^{tX_1}\cdots e^{tX_n}}=
        \E{e^{tX_1}}\cdots \E{e^{tX_n}}=
        \prod_{i=1}^n M_{X_i}(t). \]
\end{Theorem}
\begin{Example}{}{}
    Suppose $ I_1,I_2,\ldots $ are a sequence of independent
    and identically distributed (IID) $ \BERN{p} $
    trials $ p_{I_j}(0)=1-p $, $ p_{I_j}(1)=p $. Find the
    MGF of $ I_j $, and then find the MGF of
    $ \BIN{n,p} $.
    \tcblower{}
    \textbf{Solution}: For a single Bernoulli RV,
    \[ M_{I_j}(t)=(1-p)\cdot 1+p\cdot e^{1t}=(1-p)+pe^t. \]
    Now, note that the Binomial RV is the sum of $ n $ IID
    Bernoulli trials, so
    \[ M_S(t)=(1-p+pe^t)^n. \]
\end{Example}
\begin{Example}{}{}
    Suppose $ N\sim \POI{\lambda} $; that is,
    \[ p_N(k)=e^{-\lambda}\frac{\lambda^k}{k!},\; k\ge 0. \]
    Find the MGF of $ N $ and calculate $ \E{N} $ using the MGF\@.
    \tcblower{}
    \textbf{Solution}: The MGF of $ N $ is
    \begin{align*}
        M_N(t)
         & =\sum_{k=0}^{\infty}e^{tk}e^{-\lambda}\frac{\lambda^k}{k!} \\
         & =e^{-\lambda}\sum_{k=0}^{\infty}\frac{(e^t \lambda)^k}{k!} \\
         & =e^{-\lambda}e^{e^{t}\lambda}                              \\
         & =e^{\lambda(e^t-1)}.
    \end{align*}
    Therefore, the expected value is
    \[ M_N'(t)=\lambda e^{\lambda(e^t-1)}e^t
        \implies M_N'(0)=\E{N}=\lambda e^{\lambda(1-1)}e^0=\lambda. \]
\end{Example}
\begin{Proposition}{}{bin_to_poi}
    If $ S_k \sim\BIN{k,\lambda/k} $
    and $ N\sim\POI{\lambda} $, then
    $ M_{S_k}(t)\to M_N(t) $ for all $ t\in\mathbf{R} $.
    \tcblower{}
    \textbf{Proof}: Note that
    \[ \biggl(1+\frac{a}{n}\biggr)^{\!bn}\xrightarrow{n\to\infty} e^{ab}. \]
    Hence,
    \begin{align*}
        M_{S_k}(t)
         & =\biggl(1-\frac{\lambda}{k}+\frac{\lambda}{k}e^t\biggr)^{\!k} \\
         & =\biggl(1-\frac{\lambda(e^t-1)}{k}\biggr)^{\!k}               \\
         & \xrightarrow{k\to\infty}
        e^{\lambda(e^t-1)}=M_N(t),
    \end{align*}
    which is the MGF for $ N $, as desired.
\end{Proposition}
\begin{Proposition}{}{}
    In the same setup as~\Cref{prop:bin_to_poi},
    $ p_{S_k}(j)\to p_N(j) $ as $ k\to\infty $
    for all $ j\ge 0 $.
    \tcblower{}
    \textbf{Proof}:
    \begin{align*}
        \binom{k}{j}\biggl(\frac{\lambda}{k}\biggr)^j
        \biggl(1-\frac{\lambda}{k}\biggr)^{k-j}
         & =\frac{k(k-1)\cdots (k-j+1)}{j!}\frac{\lambda^j}{k^j}
        \underbrace{\biggl(1-\frac{\lambda}{k}\biggr)^k}_{\xrightarrow{k\to\infty} e^{-\lambda}} \underbrace{\biggl(1-\frac{\lambda}{k}\biggr)^{-j}}_{\xrightarrow{k\to\infty} 1} \\
         & \xrightarrow{k\to\infty}\frac{k(k-1)\cdots (k-j+1)}{k^j}\frac{\lambda^j}{j!}e^{-\lambda}(1)                                                                            \\
         & \xrightarrow{k\to\infty}\underbrace{\frac{k}{k^j}\cdot \frac{k-1}{k^j}\cdots
        \frac{k-j+1}{k^j}}_{\xrightarrow{k\to\infty} 1} \frac{\lambda^j}{j!}e^{-\lambda}                                                                                          \\
         & \xrightarrow{k\to\infty}\frac{\lambda^j}{j!}e^{-\lambda}                                                                                                               \\
    \end{align*}
\end{Proposition}