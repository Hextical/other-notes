\makeheading{Lecture 13}{\printdate{2022-11-02}}%chktex 8
\begin{Example}{}{}
    Suppose $ X \sim \N{\mu,\sigma^2} $. Find $ M_X(t) $.
    \tcblower{}
    \textbf{Solution}:
    Recall that
    \[ f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp*{-\frac{(x-\mu)^2}{2\sigma^2}}. \]
    Hence,
    \begin{align*}
        M_X(t)
         & =\int_{-\infty}^{\infty}\exp*{tx}\frac{1}{\sqrt{2\pi\sigma^2}}\exp*{-\frac{(x-\mu)^2}{2\sigma^2}}\odif{x}                                 \\
         & =\frac{1}{\sqrt{2\pi\sigma^2}}
        \int_{-\infty}^{\infty}\exp*{-\frac{1}{2\sigma^2}(x^2-2\mu x+\mu^2-2\sigma^2tx)}\odif{x}                                                     \\
         & =\frac{1}{\sqrt{2\pi\sigma^2}}
        \int_{-\infty}^{\infty}    \exp*{-\frac{1}{2\sigma^2}\bigl(x^2-2(\mu+\sigma^2 t)x+(\mu+\sigma^2 t)^2-(\mu+\sigma^2 t)^2+\mu^2\bigr)}\odif{x} \\
         & =\frac{1}{\sqrt{2\pi\sigma^2}}
        \exp*{\frac{(\mu+\sigma^2 t)^2-\mu^2}{2\sigma^2}}
        \int_{-\infty}^{\infty}\exp*{-\frac{\bigl(x-(\mu+\sigma^2 t)^2\bigr)}{2\sigma^2}}\odif{x}                                                    \\
         & =\exp*{\frac{2\mu\sigma^2 t+\sigma^4 t^2}{2\sigma^2}}                                                                                     \\
         & =\exp*{\mu t+\frac{\sigma^2 t^2}{2}}.
    \end{align*}
\end{Example}
\begin{Example}{}{}
    If $ X_1 \sim \N{\mu_1,\sigma_1^2} $, $ X_2 \sim \N{\mu_2,\sigma_2^2} $ (independent),
    and $ Y=X_1+X_2 $, then
    \begin{align*}
        M_Y(t)
         & =M_{X_1}(t)M_{X_2}(t)                                                  \\
         & =\exp*{\mu_1 t+\frac{\sigma_1^2t^2}{2}+\mu_2t+\frac{\sigma_2^2t^2}{2}} \\
         & =\exp*{(\mu_1+\mu_2)t+\frac{(\sqrt{\sigma_1^2+\sigma_2^2})^2t^2}{2}}.
    \end{align*}
    Therefore, $ Y \sim \N{\mu_1+\mu_2,\sigma_1^2+\sigma_2^2} $,
\end{Example}
\underline{Recall}: If $ X $ is a continuous random variable with pdf
$ f_X $ and $ g $ is a differentiable function
$ g\colon\mathbf{R}\to\mathbf{R} $ whose derivative only equals $ 0 $
at countably many points then the pdf of $ Y=g(X) $
is
\[ f_Y(y)=\sum_{x\colon g(x)=y,\; g'(x)\ne 0}\frac{f_X(x)}{\abs{g'(x)}}. \]
\begin{Example}{}{}
    Suppose $ X \sim \EXP{3} $ and $ Y=10X $. Find $ f_Y(y) $.
    \tcblower{}
    \textbf{Solution}:
    \underline{Aside}:
    \[ g(x)=10x\implies g^{-1}(y)=\frac{x}{10}\implies g'(y)=10. \]
    Hence,
    \begin{align*}
        f_X(x) & =3e^{-3x},\; x\ge 0.                                          \\
        f_Y(y) & =\frac{f_X(y/10)}{g'(y/10)}=\frac{3e^{-3y/10}}{10},\; y\ge 0.
    \end{align*}
\end{Example}
\begin{Example}{}{}
    Suppose $ Z \sim \N{0,1} $ and $ X=Z^2 $.
    \tcblower{}
    \textbf{Solution}:
    \[ f_Z(t)=\phi(t)=\frac{1}{\sqrt{2\pi}}\exp{-\frac{t^2}{2}}. \]
    \underline{Aside}:
    \[ g(t)=t^2\implies g'(t)=2t. \]
    Hence,
    \begin{align*}
        f_X(v)
         & =\sum_{t\colon t^2=v}\frac{\phi(t)}{\abs{g'(t)}}                                                   \\
         & =\frac{\phi(\sqrt{v})}{\abs{g'(\sqrt{v})}}+\frac{\phi(-\sqrt{v})}{\abs{g'(-\sqrt{v})}},\; v>0      \\
         & =\frac{1}{2\sqrt{v}}\frac{1}{\sqrt{2\pi}}e^{-v/2}+\frac{1}{2\sqrt{v}}\frac{1}{\sqrt{2\pi}}e^{-v/2} \\
         & =\frac{1}{\sqrt{2\pi}}v^{-1/2}e^{-v/2},
    \end{align*}
    which is $ \GAM{1/2,1/2} $.
\end{Example}
\begin{Example}{}{}
    If $ Y_1,Y_2,\ldots,Y_p \iid \GAM{1/2,1/2} $, then
    \[ Y_1+\cdots+Y_p \sim \GAM*{\frac{p}{2},\frac{1}{2}}. \]
\end{Example}
\begin{Definition}{Chi-squared}{}
    The Gamma distribution with shape $ p/2 $ and rate $ 1/2 $
    for any positive integer $ p $ is also called the
    \textbf{Chi-squared} distribution with $ p $ degrees of freedom,
    and we write $ Y \sim \chi^2_p $.
    This is the distribution of the sum of squares of $ p $
    independent standard normal variables. It has pdf
    \[ f(x)=\frac{(1/2)^{p/2}}{\Gamma(p/2)}x^{p/2-1}e^{-x/2}. \]
\end{Definition}
\begin{Theorem}{Markov's Inequality}{}
    If $ X $ is a non-negative random variable, then
    \[ \Prob{X\ge a}\le \frac{\E{X}}{a}. \]
    \tcblower{}
    \textbf{Proof}:
    Suppose $ X $ is a non-negative random variable. Then,
    \[ \E{X}=\int_{0}^{\infty}t\Prob{X\in \odif{t}}. \]
    Fix $ a>0 $,
    \begin{align*}
        \E{X}
         & =\int_{0}^{a}t\Prob{X\in\odif{t}}+\int_{a}^{\infty}t\Prob{X\in\odif{t}}     \\
         & \ge \int_{0}^{a}0\Prob{X\in \odif{t}}+\int_{a}^{\infty}a\Prob{X\in\odif{t}} \\
         & =a\bigl(1-F_X(a)\bigr)                                                      \\
         & =a\Prob{X\ge a}.
    \end{align*}
    Therefore,
    \[ \E{X}\ge a\Prob{X\ge a}. \]
\end{Theorem}
\begin{Remark}{Triangle Flip Trick}{}
    Suppose $ X $ is non-negative and discrete.
    \begin{align*}
        \E{X}
         & =0\Prob{X=0}+1\Prob{X=1}+2\Prob{X=2}+\cdots                               \\
         & =\Prob{X=1}+\Prob{X=2}+\Prob{X=2}+\Prob{X=3}+\Prob{X=3}+\Prob{X=3}+\cdots \\
         & =\Prob{X\ge 1}+\Prob{X\ge 2}+\Prob{X\ge 3}+\cdots                         \\
         & =\sum_{k=1}^{\infty}\Prob{X\ge k}.
    \end{align*}
    Suppose $ X $ is non-negative and continuous with pdf $ f_X $.
    \begin{align*}
        \E{X}
         & =\int_{0}^{\infty}t f_X(t)\odif{t}                        \\
         & =\int_{0}^{\infty}f_X(t)\int_{0}^{t}1\odif{s}\odif{t}     \\
         & =\int_{0}^{\infty}\int_{s}^{\infty}f_X(t)\odif{t}\odif{s} \\
         & =\int_{0}^{\infty}1-F_X(t)\odif{s}.
    \end{align*}
\end{Remark}
\begin{Theorem}{Chebyshev's Inequality}{}
    For any random variable $ Y $,
    \[ \Prob[\Big]{\abs[\big]{Y-\E{Y}}\ge a}\le\frac{\Var{Y}}{a^2}. \]
    \tcblower{}
    \textbf{Proof}:
    Consider a random variable $ Y $ (does not have to be non-negative).
    \begin{align*}
        \Prob[\big]{\abs{Y-\E{Y}}\ge a}
         & =\Prob[\big]{(Y-\E{Y})^2\ge a^2}                              \\
         & \le \frac{\E*{(Y-\E{Y})^2}}{a^2}\text{by Markov's inequality} \\
         & =\frac{\Var{Y}}{a^2}.
    \end{align*}
\end{Theorem}
\begin{Example}{}{}
    If $ X \sim \EXP{3} $, then $ \E{X}=1/3 $. Using Markov's inequality,
    \[ \Prob{X\ge 5}\le \frac{1/3}{5}=\frac{1}{15}. \]
    \[ \Prob{X\ge 5}\le \Prob[\big]{\abs{X-\E{X}}\ge 14/3}\le \frac{\Var{X}}{(14/3)^2}=\frac{(1/3)^2}{(14/3)^2}=\frac{1}{14^2}=\frac{1}{196}. \]
\end{Example}
\makeheading{Lecture 14}{\printdate{2022-11-04}}%chktex 8
\begin{Definition}{}{}
    If $ X $ is a discrete random variable and $ A $ is an event
    with $ \Prob{A}>0 $, then the \textbf{conditional pmf} of $ X $ given $ A $ is
    \[ p_{X\mid A}(k)=\frac{\Prob{\Set{X=k}\cap A}}{\Prob{A}}. \]
    This is another probability mass function:
    \begin{itemize}
        \item Non-negative (ratio of probabilities);
        \item Sums to 1:
              \begin{align*}
                  \sum_k p_{X\mid A}(k)
                   & =\frac{\Prob{\Set{X=k}\cap A}}{\Prob{A}}               \\
                   & =\frac{1}{\Prob{A}}\sum_k\Prob{\Set{X=k}\cap A}        \\
                   & =\frac{1}{\Prob{A}}\Prob*{\bigcup_k (\Set{X=k}\cap A)} \\
                   & =\frac{1}{\Prob{A}}\Prob*{A\cap \bigcup_k\Set{X=k} }   \\
                   & =\frac{1}{\Prob{A}}\Prob{A\cap \Omega}                 \\
                   & =1.
              \end{align*}
    \end{itemize}
\end{Definition}
\begin{Definition}{}{}
    If $ Y $ is another discrete RV then we can define the conditional pmf of $ X $
    given $ Y=y $ in the same manner:
    \[ p_{X\mid Y}(k\mid y)=\frac{\Prob{\Set{X=k}\cap\Set{Y=y}}}{\Prob{\Set{Y=y}}}. \]
    If $ y $ is fixed, then this is a pmf over different values of $ k $.
\end{Definition}
\begin{Definition}{}{}
    The \textbf{conditional expectation} of $ X $ given $ Y=y $ is:
    \[ \E{X\given Y=y}=\sum_k k p_{X\mid Y}(k\mid y). \]
\end{Definition}
\begin{Theorem}{}{}
    If $ g\colon \mathbf{R}^2\to\mathbf{R} $,
    \[ \E{g(X,Y)\given Y=y}=\sum_k g(k,y)p_{X\mid Y}(k\mid y), \]
    then
    \[ \sum_y \E{g(X,Y)\mid Y=y}p_Y(y)=\E{g(X,Y)}. \]
    That is,
    \[ \E{\E{g(X,Y)\given Y}}=\E{g(X\given Y)}. \]
    The expectation of the conditional expectation equals the expectation.
\end{Theorem}
\begin{Example}{}{}
    \[ f_{X,Y}(x,y)=\begin{cases}
            2x^2, & x\in[0,1],\; y\in[-x,x], \\
            0,    & \text{otherwise}.
        \end{cases} \]
    \underline{Recall}: The marginal density for $ X $ is
    \[ f_X(x)=\int_{-\infty}^{\infty}f(x,y)\odif{y}. \]
    \begin{Definition}{}{}
        In this setting, for $ y\in\mathbf{R} $ with $ f_Y(y)>0 $,
        the conditional pdf for $ X $ given $ Y=y $ is
        \[ f_{X\mid Y}=\frac{f_{X,Y}(x,y)}{f_{Y}(y)}. \]
        For $ y $ fixed, we can check if this is a pdf:
        \begin{itemize}
            \item Non-negative;
            \item Integrate to 1:
                  \begin{align*}
                      \int_{-\infty}^{\infty}f_{X\mid Y}(x\mid y)\odif{x}
                       & =\int_{-\infty}^{\infty}\frac{f_{X,Y}(x,y)}{f_Y(y)}\odif{x} \\
                       & =\frac{\int_{-\infty}^{\infty}f_{X,Y}(x,y)\odif{x}}{f_Y(y)} \\
                       & =\frac{f_Y(y)}{f_Y(y)}                                      \\
                       & =1.
                  \end{align*}
        \end{itemize}
    \end{Definition}
    In our example, $ -1\le -x\le y\le x\le 1 $, so
    \begin{align*}
        f_Y(y)
         & =\int_{-\infty}^{\infty}f_{X,Y}(x,y)\odif{x}    \\
         & =\int_{\abs{y}}^{1}2x^2\odif{x}                 \\
         & =\biggl[\frac{2}{3}x^3\biggr]_{x=\abs{y}}^{x=1} \\
         & =\frac{2}{3}(1-\abs{y}^3),\; -1\le y\le 1.
    \end{align*}
    Check:
    \begin{align*}
        \int_{-1}^{1}\frac{2}{3}(1-\abs{y}^3)\odif{y}
         & =\int_{-1}^{0}\frac{2}{3}(1+y^3)\odif{y}+\int_{0}^{1}\frac{2}{3}(1-y^3)\odif{y}                               \\
         & =\biggl[\frac{2}{3}y+\frac{1}{6}y^4\biggr]_{y=-1}^{y=0}+\biggl[\frac{2}{3}y-\frac{1}{6}y^4\biggr]_{y=0}^{y=1} \\
         & =-\biggl(-\frac{2}{3}+\frac{1}{6}\biggr)+\biggl(\frac{2}{3}-\frac{1}{6}  \biggr)                              \\
         & =\frac{1}{2}+\frac{1}{2}                                                                                      \\
         & =1.
    \end{align*}
    Thus,
    \[ f_{X\mid Y}(x\mid y)=\begin{dcases}
            3 \frac{x^2}{1-\abs{y}^3},\; y\le x\le 1, \\
            0, & \text{otherwise},
        \end{dcases}\; y\in[-1,1] \]
    For example, if $ y=-1/2 $, then
    \[ f_{X\mid Y}(x\mid -1/2)=\begin{dcases}
            \frac{24}{7}x^2,\; \frac{1}{2}\le x\le 1, \\
            0, & \text{otherwise}.
        \end{dcases} \]
    We can compute
    \begin{align*}
        \E{X\given Y=y}
         & =\int_{-\infty}^{\infty}x f_{X\mid Y}(x\mid y)\odif{x}               \\
         & =\int_{\abs{y}}^{1}3 \frac{x^3}{1-\abs{y}^3}\odif{x}                 \\
         & =\frac{3}{1-\abs{y}^3}\int_{\abs{y}}^{1}x^3\odif{x}                  \\
         & =\frac{3}{1-\abs{y}^3}\biggl[\frac{1}{4}x^4\biggr]_{x=\abs{y}}^{x=1} \\
         & =\frac{3}{4(1-\abs{y}^3)}(1-\abs{y}^4).
    \end{align*}
    So,
    \[ \E*{X\given Y=-\frac{1}{2}}=\frac{3}{4(7/8)}\frac{15}{16}=\frac{45}{56}. \]
    If $ y $ was not fixed, we would have
    \[ \E{X\given Y}=\frac{3}{4(1-\abs{Y}^3)}(1-Y^4), \]
    which is a random variable.
    \begin{Remark}{Why do we care about $ \E{X\given Y} $?}{}
        $ \E{X\given Y} $ is the best guess for the value of $ X $,
        based on $ Y $, in the sense that it minimizes
        \[ \E*{(X-\E{X\given Y})^2}. \]
        That is, $ \E{Y\given X} $ in statistics is the
        \textbf{true regression function}.
        \tcblower{}
        \[ \E{g(X,Y)\given Y=y}=\int_{-\infty}^{\infty}g(x,y)f_{X\mid Y}(x\mid y)\odif{x}. \]
        Thus, $ \E{g(X,Y)\given Y} $
        is the best guess for $ g(X,Y) $, based on $ Y $. It has the property that
        \[ \E{\E{g(X,Y)\given Y}}=\E{g(X,Y)} \]
    \end{Remark}
\end{Example}
\begin{Example}{}{}
    Suppose $ X $ is the first arrival time for a Poisson process with
    rate $ \lambda=2 $, and $ Y $ is the second arrival time. So, the joint
    pdf is
    \[ f_{X,Y}(x,y)=\begin{cases}
            4e^{-2y},\; 0\le x<y,  \\
            0, & \text{otherwise}.
        \end{cases} \]
    Check:
    \begin{align*}
        \int_{0}^{\infty}\int_{x}^{\infty}4e^{-2y}\odif{y}\odif{x}
         & =\int_{0}^{\infty}\bigl[-2e^{-2y}\bigr]_{y=x}^{y\to\infty}\odif{x} \\
         & =\int_{0}^{\infty}2e^{-2x}\odif{x}                                 \\
         & =\bigl[-e^{-2x}\bigr]_{x=0}^{x\to\infty}                           \\
         & =0-(-1)                                                            \\
         & =1.
    \end{align*}
    The marginal pdf for $ Y $ is
    \begin{align*}
        f_Y(y)
         & =\int_{0}^{\infty}f_{X,Y}(x,y)\odif{x} \\
         & =\int_{0}^{y}4e^{-2y}\odif{x}          \\
         & =\bigl[x4e^{-2y}\bigr]_{x=0}^{x=y}     \\
         & =4ye^{-2y},\; y\ge 0,
    \end{align*}
    which is $ \GAM{2,2} $. The conditional pdf of $ X $ given $ Y $ is
    \[ f_{X\mid Y}(x\mid y)=\begin{dcases}
            \frac{4e^{-2y}}{4ye^{-2y}}, & x\in[0,y],       \\
            0,                          & \text{otherwise}
        \end{dcases}=
        \begin{dcases}
            \frac{1}{y}, & x\in[0,y],        \\
            0,           & \text{otherwise}.
        \end{dcases}
    \]
    For a fixed $ Y $-value $ Y=y $, $ X $ is conditionally
    $ \text{Uniform}[0,y] $. That is,
    \[ \frac{X}{Y}\sim \text{Uniform}[0,1], \]
    and is independent of $ Y $.
\end{Example}