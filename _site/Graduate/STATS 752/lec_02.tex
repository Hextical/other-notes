\makeheading{Lecture 2}{\printdate{2023-01-12}}%chktex 8
\section{Lecture 2: Quadratic Forms and Distributions}
\begin{Definition}{Quadratic Form}{}
    The \textbf{quadratic form} associated $ \Matrix{A}\in\R^{n\times n} $
    is defined as
    \[ \Vector{x}' \Matrix{A}\Vector{x}=\sum_{i=1}^{n}\sum_{j=1}^{n}x_i a_{ij}x_j. \]
    \tcblower{}
    If $ \tilde{\Matrix{A}}=\dfrac{\Matrix{A}+\Matrix{A}'}{2} $,
    then note that $ \tilde{\Matrix{A}} $ is symmetric and
    \begin{align*}
        \Vector{x}' \tilde{\Matrix{A}}\Vector{x}
         & =\sum_{i=1}^{n}\sum_{j=1}^{n}x_j \frac{a_{ij}+a_{ji}}{2}x_j \\
         & =\sum_{i=1}^{n}\sum_{j=1}^{n}x_i a_{ij}x_j                  \\
         & =\Vector{x}' \Matrix{A}\Vector{x}.
    \end{align*}
    Therefore, there is a one-to-one correspondence between quadratic forms and symmetric matrices.
\end{Definition}
\begin{Example}{}{}
    Let $ S^2 $ denote the sample variance of a random sample $ X_1,\ldots,X_n $. Set
    \[ \Vector{X}=\begin{pmatrix}
            X_1    \\
            \vdots \\
            X_n
        \end{pmatrix},\qquad \Vector{\mu}=\E{\Vector{X}}=\begin{pmatrix}
            \mu_1  \\
            \vdots \\
            \mu_n
        \end{pmatrix}. \]
    Write $ (n-1)S^2 $ as a quadratic form and identify the matrix $ \Matrix{A} $.
    \tcblower{}
    \textbf{Solution}:
    \begin{align*}
        (n-1)S^2
         & =\sum_{i=1}^{n}(X_i-\bar{X})^2                                                \\
         & =\sum_{i=1}^{n}X_i^2-2\biggl(\sum_{i=1}^{n}X_i\biggr)\bar{X}+n\bar{X}^2       \\
         & =\sum_{i=1}^{n}X_i^2-\frac{1}{n}\biggl(\sum_{i=1}^{n}X_i\biggr)^{\!2}         \\
         & =\Vector{X}'\Vector{X}-\tfrac{1}{n}(\Vector{j}' \Vector{X})^2                 \\
         & =\Vector{X}'\Vector{X}-\tfrac{1}{n}\Vector{X}'\Vector{j}\Vector{j}'\Vector{X} \\
         & =\Vector{X}'(\Matrix{I}-\tfrac{1}{n}\Vector{j}\Vector{j}')\Vector{X}          \\
         & =\Vector{X}'(\Matrix{I}-\tfrac{1}{n}\Matrix{J})\Vector{X}.
    \end{align*}
    Hence, $ \Matrix{A}=\Matrix{I}-\tfrac{1}{n}\Matrix{J} $ is symmetric.
\end{Example}
\begin{Theorem}{}{}
    If $ \Vector{X} $ is a random vector with mean $ \Vector{\mu} $, covariance matrix $ \Matrix{\Sigma} $,
    and $ \Matrix{A} $ is a symmetric matrix of constants, then
    \[ \E{\Vector{X}'\Matrix{A}\Vector{X}}=\tr{\Matrix{A\Sigma}}+\Vector{\mu}'\Matrix{A}\Vector{\mu}. \]
    \tcblower{}
    \textbf{Proof}:
    \begin{align*}
        \E{\Vector{X}'\Matrix{A}\Vector{X}}
         & =\E{\tr{\Vector{X}'\Matrix{A}\Vector{X}}}                                                                  \\
         & =\E{\tr{\Matrix{A}\Vector{X}\Vector{X}'}}                                                                  \\
         & =\tr{\E{\Matrix{A}\Vector{X}\Vector{X}'}}                                                                  \\
         & =\tr{\Matrix{A}\E{\Vector{X}\Vector{X}'}}                                                                  \\
         & =\tr*{\Matrix{A}\E[\big]{(\Vector{X}-\Vector{\mu}+\Vector{\mu})(\Vector{X}'-\Vector{\mu}'+\Vector{\mu}')}} \\
         & =\tr[\big]{\Matrix{A}[\Matrix{\Sigma}+\Vector{\mu}\Vector{\mu}']}                                          \\
         & =\tr{\Matrix{A\Sigma}}+\tr{\Matrix{A}\Vector{\mu}\Vector{\mu}'}                                            \\
         & =\tr{\Matrix{A\Sigma}}+\tr{\Vector{\mu}'\Matrix{A}\Vector{\mu}}                                            \\
         & =\tr{\Matrix{A\Sigma}}+\Vector{\mu}'\Matrix{A}\Vector{\mu}.
    \end{align*}
\end{Theorem}
\begin{Example}{}{}
    Assume that $ \mu_1=\cdots=\mu_n=\mu $ and $ \Matrix{\Sigma}=\sigma^2 \Matrix{I} $. Find $ \E{S^2} $.
    \tcblower{}
    \textbf{Solution}:
    \begin{align*}
        \E{S^2}
         & =\tfrac{1}{n-1}\E*{(n-1)S^2}                                                                                                                                                                                                 \\
         & =\tfrac{1}{n-1}\E*{\Vector{X}'(\Matrix{I}-\tfrac{1}{n}\Matrix{J})\Vector{X}}                                                                                                                                                 \\
         & =\tfrac{1}{n-1}\E*{\tr*{(\Matrix{I}-\tfrac{1}{n}\Matrix{J})\Matrix{\Sigma}}+\Vector{\mu}'(\Matrix{I}-\tfrac{1}{n}\Matrix{J})\Vector{\mu}}                                                                                    \\
         & =\tfrac{1}{n-1}\E*{\tr*{(\Matrix{I}-\tfrac{1}{n}\Matrix{J})\sigma^2\Matrix{I}}+\mu^2 \Vector{j}'(\Matrix{I}-\tfrac{1}{n}\Matrix{J})\Vector{j}}                                                                               \\
         & =\tfrac{1}{n-1}\E*{\sigma^2(n-1)+\mu^2(\Vector{j}'\Vector{j}-\tfrac{1}{n}\Vector{j}'\Matrix{J}\Vector{j})}                                                                                                                   \\
         & =\tfrac{1}{n-1}\E{\sigma^2(n-1)+0}                                                                                                             &  & \text{since $ \Vector{j}'\Matrix{J}\Vector{j}=n \Vector{j}'\Vector{j} $} \\
         & =\sigma^2.
    \end{align*}
\end{Example}
\begin{Remark}{Multivariate Normal Distribution}{}
    Let $ \Vector{X}=(X_1,X_2,\ldots,X_n) $
    be a $ 1\times n $ random vector with $ \E{X_i}=\mu_i $
    and $ \Cov{X_i,X_j}=\sigma_{ij} $, for $ i,j=1,2,\ldots,n $.
    Let $ \Vector{\mu}=(\mu_1,\mu_2,\ldots,\mu_n) $
    be the mean vector and $ \Matrix{\Sigma} $ be the $ n\times n $
    symmetric covariance matrix
    whose $ (i,j) $ entry is $ \sigma_{ij} $. Suppose that also
    the inverse matrix of $ \Matrix{\Sigma} $, $ \Matrix{\Sigma}^{-1} $ exists.
    If the joint probability density function of $ (X_1,\ldots,X_n) $ is given by
    \[ f(\Vector{x})=\frac{1}{(2\pi)^{n/2}\abs{\Matrix{\Sigma}}^{1/2}}
        \exp*{-\frac{1}{2}(\Vector{x}-\Vector{\mu})'\Matrix{\Sigma}^{-1}(\Vector{x}-\Vector{\mu})}\;
        \text{for $ \Vector{x}\in\R^{n} $} \]
    where $ \Vector{x}=(x_1,x_2,\ldots,x_n) $, then $ \Vector{X} $
    is said to have a \textbf{multivariate normal distribution}. We write
    $ \Vector{X}\sim \MN{\Vector{\mu},\Matrix{\Sigma}} $.
\end{Remark}
\begin{Remark}{Aitken's Integral}{}
    For any positive definite matrix $ \Matrix{A}\in\R^{n\times n} $, we have
    \[ \int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}\exp*{-\frac{1}{2}\Vector{x}'\Matrix{A}\Vector{x}}\odif{\Vector{x}}
        =(2\pi)^{n/2}\abs{A}^{-1/2}. \]
\end{Remark}
\begin{Theorem}{}{}
    If $ \Vector{X}\sim \MN{\Vector{\mu},\Matrix{\Sigma}} $, its moment generating function is given by
    \[ M_{\Vector{X}}(\Vector{t}')=\E{e^{\Vector{t}'\Vector{X}}}=\exp*{\Vector{t}'\Vector{\mu}+\frac{\Vector{t}'\Matrix{\Sigma}\Vector{t}}{2}}. \]
    \tcblower{}
    \textbf{Proof}:
    \begin{align*}
        M_{\Vector{X}}(\Vector{t}')
         & =(2\pi)^{-n/2}\abs{\Matrix{\Sigma}}^{-1/2}
        \int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}\exp*{\Vector{t}'\Vector{x}-\frac{1}{2}(\Vector{x}-\Vector{\mu})'\Matrix{\Sigma}^{-1}(\Vector{x}-\Vector{\mu})}\odif{\Vector{x}}                                                                                          \\
         & =(2\pi)^{-n/2}\abs{\Matrix{\Sigma}}^{-1/2}
        \int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}\exp*{-\frac{1}{2}\bigl[(\Vector{x}-\Vector{\mu})'\Matrix{\Sigma}^{-1}(\Vector{x}-\Vector{\mu})-2 \Vector{t}'\Vector{x}\bigr]}\odif{\Vector{x}}                                                                           \\
         & =(2\pi)^{-n/2}\abs{\Matrix{\Sigma}}^{-1/2}
        \int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}                                                                                                                                                                                                                          \\
         & \exp*{-\frac{1}{2}\bigl[(\Vector{x}'-\Vector{\mu}'-\Vector{t}'\Matrix{\Sigma}'+\Vector{t}'\Matrix{\Sigma}')\Matrix{\Sigma}^{-1}(\Vector{x}-\Vector{\mu}-\Matrix{\Sigma}\Vector{t}+\Matrix{\Sigma}\Vector{t})-2 \Vector{t}'\Vector{x}\bigr]}\odif{\Vector{x}}               \\
         & =(2\pi)^{-n/2}\abs{\Matrix{\Sigma}}^{-1/2}
        \int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}                                                                                                                                                                                                                          \\
         & \exp*{-\frac{1}{2}(\Vector{x}'-\Vector{\mu}'-\Vector{t}'\Matrix{\Sigma}'+\Vector{t}'\Matrix{\Sigma}')\Matrix{\Sigma}^{-1}(\Vector{x}-\Vector{\mu}-\Matrix{\Sigma}\Vector{t})}                                                                                              \\
         & \exp*{-\frac{1}{2}[\Vector{x}'\Vector{t}-\Vector{\mu}'\Vector{t}-\Vector{t}'\Matrix{\Sigma}'\Vector{t}+\Vector{t}'\Vector{x}-\Vector{t}'\Vector{\mu}-\Vector{t}'\Matrix{\Sigma}\Vector{t}+\Vector{t}'\Matrix{\Sigma}'\Vector{t}-2 \Vector{t}'\Vector{x}]}\odif{\Vector{x}} \\
         & =\underbrace{\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}(2\pi)^{-n/2}\abs{\Matrix{\Sigma}}^{-1/2}
        \exp*{-\frac{1}{2}(\Vector{x}-\Vector{\mu}-\Matrix{\Sigma}\Vector{t})' \Matrix{\Sigma}^{-1}(\Vector{x}-\Vector{\mu}-\Matrix{\Sigma}\Vector{t})}}_{=1}\odif{\Vector{x}}                                                                                                        \\
         & \exp*{\Vector{t}'\Vector{\mu}+\frac{\Vector{t}' \Matrix{\Sigma}\Vector{t}}{2}}                                                                                                                                                                                             \\
         & =\exp*{\Vector{t}'\Vector{\mu}+\frac{\Vector{t}' \Matrix{\Sigma}\Vector{t}}{2}}
    \end{align*}
\end{Theorem}
\begin{Remark}{Gamma Distribution}{}
    $ Y $ is said to have a \textbf{Gamma distribution}
    with shape $ \alpha $ and scale $ \beta $ when
    \[ f_Y(y)=\frac{y^{\alpha-1}e^{-y/\beta}}{\Gamma(\alpha)\beta^\alpha},\text{ for $y>0,\; \alpha>0,\;\beta>0$}, \]
    and $ 0 $ otherwise. We write $ Y \sim \GAM{\alpha,\beta} $.
    \begin{enumerate}[(1)]
        \item $ \E{Y}=\alpha\beta $, $ \Var{Y}=\alpha\beta^2 $.
        \item $ M_Y(t)=(1-\beta t)^{-\alpha} $ for $ t<1/\beta $.
    \end{enumerate}
\end{Remark}
\begin{Remark}{Chi-Squared Distribution}{}
    $ Q $ is said to have a \textbf{Chi-squared distribution} with
    $ n\in\mathbb{Z}^+ $ degrees of freedom when
    $ Q \sim \GAM{n/2,2} $. We write $ Q \sim \chi^2(n) $.
    \begin{enumerate}[(1)]
        \item $ \E{Q}=k $, $ \Var{Q}=2k $.
        \item $ M_Q(t)=(1-2t)^{-n/2} $ for $ t<1/2 $.
    \end{enumerate}
\end{Remark}
\begin{Theorem}{}{}
    Let $ \Vector{X}\sim \MN{\Vector{0},\Matrix{\Sigma}} $. Then $ \Vector{X}'\Matrix{\Sigma}^{-1}\Vector{X}\sim \chi^2(n) $.
    \tcblower{}
    \textbf{Proof}:
    Let $ Y=\Vector{X}'\Matrix{\Sigma}^{-1}\Vector{X} $. Then,
    \begin{align*}
        M_Y(t)
         & =\E{e^{tY}}                                                                                                              \\
         & =\E{e^{\Vector{X}'(t \Matrix{\Sigma}^{-1})\Vector{X}}}                                                                   \\
         & =(2\pi)^{-n/2}\abs{\Matrix{\Sigma}}^{-1/2}\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}
        \exp*{\Vector{x}'(t \Matrix{\Sigma}^{-1})\Vector{x}-\frac{1}{2}\Vector{x}' \Matrix{\Sigma}^{-1}\Vector{x}}\odif{\Vector{x}} \\
         & =(2\pi)^{-n/2}\abs{\Matrix{\Sigma}}^{-1/2}\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}
        \exp*{-\frac{1}{2}\Vector{x}'(1-2t)\Matrix{\Sigma}^{-1}\Vector{x}}\odif{\Vector{x}}                                         \\
         & =\abs{\Matrix{\Sigma}}^{-1/2}\abs[\Big]{\bigl((1-2t)\Matrix{\Sigma}^{-1}\bigr)^{-1}}^{1/2}                               \\
         & =(1-2t)^{-n/2}
    \end{align*}
\end{Theorem}
\begin{Remark}{Non-Central Chi-Squared Distribution}{}
    Let $ X_1,\ldots,X_n $ be independent and $ X_i \sim \N{\mu_i,1} $. Set $ \lambda=\frac{1}{2}\Vector{\mu}'\Vector{\mu}=\frac{1}{2}\sum_{i=1}^{n}\mu_i^2 $
    and $ W=\sum_{i=1}^{n}X_i^2 $. Then, $ W $ has a \textbf{non-central chi-squared distribution}
    with degrees of freedom $ n $ and non-centrality parameter
    $ \lambda $. We write $ W \sim \chi^2(n,\lambda) $.
    The usual chi-square corresponds to $ \lambda=0 $.

    Note: The factor $ \frac{1}{2} $ is used for this course.
    \tcblower{}
    \textbf{Not covered in notes}:
    \[ M_W(t)=(1-2t)^{-n/2}\exp*{\frac{\lambda 2t}{1-2t}}. \]
\end{Remark}
\begin{Remark}{$F$-Distribution}{}
    If $ X \sim \chi^2(n) $ independently of $ Y \sim \chi^2(m) $
    for $ n,m>0 $, then
    we say $ U=\frac{X/n}{Y/m} $ has a (central) \textbf{$F$-distribution}.
    We write $ U \sim F(n,m) $.
    \tcblower{}
    If $ X \sim \chi^2(n,\lambda) $ independently of $ Y \sim \chi^2(m) $
    for $ n,m,\lambda>0 $, then we say
    $ U=\frac{X/n}{Y/m} $ has a \textbf{non-central $F$-distribution}.
    We write $ U \sim F(n,m,\lambda) $. If $ \lambda=0 $, then
    $ U \sim F(n,m) $.
\end{Remark}
\subsection*{Transformation of Multivariate Normal}
If $ \Matrix{\Sigma} $ is symmetric, then by the spectral theorem, there exists an orthogonal matrix
$ \Matrix{\Gamma} $ such that
\[ \Matrix{\Sigma}=\Matrix{\Gamma}'\diag{\lambda_1,\ldots,\lambda_n}\Matrix{\Gamma}, \]
where $ \lambda_1,\ldots,\lambda_n $ are the eigenvalues of $ \Matrix{\Sigma} $.
Note that $ \lambda_i>0 $ for all $ i\in[1,n] $ since $ \Matrix{\Sigma} $ is
positive definite. Furthermore, if we set
\[ \Matrix{\Sigma}^{1/2}=\Matrix{\Gamma}'\diag{\sqrt{\lambda_1},\ldots,\sqrt{\lambda_n}}\Matrix{\Gamma}, \]
we see that
\begin{align*}
    \Matrix{\Sigma}^{1/2}\Matrix{\Sigma}^{1/2}
     & =\Matrix{\Gamma}'\diag{\sqrt{\lambda_1},\ldots,\sqrt{\lambda_n}}\underbrace{\Matrix{\Gamma}\Matrix{\Gamma}'}_{\Matrix{I}}\diag{\sqrt{\lambda_1},\ldots,\sqrt{\lambda_n}}\Matrix{\Gamma} \\
     & =\Matrix{\Gamma}'\diag{\lambda_1,\ldots,\lambda_n}\Matrix{\Gamma}                                                                                                                       \\
     & =\Matrix{\Sigma}.
\end{align*}
Therefore, $ \Matrix{\Sigma}^{1/2} $ is well-defined and is called the \textbf{square root} of
$ \Matrix{\Sigma} $.

\begin{Remark}{}{}
    If $ \Vector{X}\sim \MN{\Vector{\mu},\Matrix{\Sigma}} $, and
    $ \Vector{Z}=\Matrix{\Sigma}^{-1/2}(\Vector{X}-\Vector{\mu}) $,
    where $ \Matrix{\Sigma}^{-1/2}=(\Matrix{\Sigma}^{1/2})^{-1} $, then
    \[ \Vector{Z} \sim \MN{\Vector{0},\Matrix{I}}. \]
    \tcblower{}
    \textbf{Proof}:
    \begin{align*}
        M_{\Vector{Z}}(\Vector{t}')
         & =\E{e^{\Vector{t}'\Vector{Z}}}                                                                                                                                                                       \\
         & =\E*{\exp{\Vector{t}'\Matrix{\Sigma}^{-1/2}(\Vector{X}-\Vector{\mu})}}                                                                                                                               \\
         & =\exp{-\Vector{t}'\Matrix{\Sigma}^{-1/2}\Vector{\mu}}\E*{\exp{\Vector{t}'\Matrix{\Sigma}^{-1/2}\Vector{X}}}                                                                                          \\
         & =\exp{-\Vector{t}'\Matrix{\Sigma}^{-1/2}\Vector{\mu}}\exp*{\Vector{t}'\Matrix{\Sigma}^{-1/2}\Vector{\mu}+\frac{\Vector{t}'\Matrix{\Sigma}^{-1/2}\Matrix{\Sigma}\Matrix{\Sigma}^{-1/2}\Vector{t}}{2}} \\
         & =\exp*{\frac{\Vector{t}'\Matrix{\Sigma}^{-1/2}\Matrix{\Sigma}^{1/2}\Matrix{\Sigma}^{1/2}\Matrix{\Sigma}^{-1/2}\Vector{t}}{2}}                                                                        \\
         & =\exp*{\frac{\Vector{t}'\Vector{t}}{2}}                                                                                                                                                              \\
         & =\exp*{\frac{1}{2}\sum_{i=1}^{n}t_i^2}.
    \end{align*}
\end{Remark}
