\makeheading{Lecture 1}{\printdate{2023-01-09}}%chktex 8
\section{Lecture 1: Review of Linear Algebra}
\begin{Definition}{Vectors in $ \R^n $}{}
    For any positive integer $ n $, we define
    $ n $-dimensional Euclidean space $ \R^n $ by
    \[ \R^n=\Set*{\begin{pmatrix}
                x_1    \\
                \vdots \\
                x_n
            \end{pmatrix}\given x_1,\ldots,x_n\in\R}. \]
    If $ \Vector{x}\in\R^n $, then
    there exists $ x_1,\ldots,x_n\in\R $ such that
    \[ \Vector{x}=\begin{pmatrix}
            x_1    \\
            \vdots \\
            x_n
        \end{pmatrix}. \]
\end{Definition}
\begin{Definition}{Matrix}{}
    An $ n\times m $ \textbf{matrix} $ \Matrix{A} $ is a rectangular array with $ n $ rows and $ m $ columns. We denote the
    entry in the $ i\textsuperscript{th} $ row and $ j\textsuperscript{th} $ column by $ a_{ij} $
    or $ (\Matrix{A})_{ij} $. That is,
    \[ \Matrix{A}=\begin{pmatrix}
            a_{11} & a_{12} & \cdots & a_{1j} & \cdots & a_{1m} \\
            a_{21} & a_{22} & \cdots & a_{2j} & \cdots & a_{2m} \\
            \vdots & \vdots &        & \vdots &        & \vdots \\
            a_{i1} & a_{i2} & \cdots & a_{ij} & \cdots & a_{im} \\
            \vdots & \vdots &        & \vdots &        & \vdots \\
            a_{n1} & a_{n2} & \cdots & a_{nj} & \cdots & a_{nm}
        \end{pmatrix}. \]
    The set of all $ n\times m $ matrices with real entries is denoted by $ \R^{n\times m} $.
\end{Definition}
\begin{Definition}{Transpose}{}
    Let $ \Matrix{A}\in\R^{n\times m} $. We define the \textbf{transpose} of $ \Matrix{A} $,
    denoted $ \Matrix{A}' $, to be the $ m\times n $ matrix whose
    $ ij\textsuperscript{th} $ entry is the $ ji\textsuperscript{th} $ entry of $ \Matrix{A} $. That is,
    \[ (\Matrix{A}')_{ij}=(\Matrix{A})_{ji}. \]
\end{Definition}
\begin{Definition}{Square Matrix}{}
    An $ n\times n $ matrix is called a \textbf{square matrix}.
\end{Definition}
\begin{Definition}{Symmetric Matrix}{}
    A matrix is called \textbf{symmetric} if $ \Matrix{A}'=\Matrix{A} $.
\end{Definition}
\begin{Definition}{Diagonal Matrix}{}
    An $ n\times n $ matrix $ \Matrix{D} $ is said to be a \textbf{diagonal matrix} if $ d_{ij}=0 $ for all $ i\ne j $. We denote
    a diagonal matrix by
    \[ \Matrix{D}=\diag{d_{1},d_{2},\ldots,d_{n}}. \]
\end{Definition}
\begin{Definition}{Identity Matrix}{}
    The $ n\times n $ matrix $ \Matrix{I} $ (or $ \Matrix{I}_n $) such that
    $ (\Matrix{I})_{ii} $ for $ 1\le i\le n $, and $ (\Matrix{I})_{ij}=0 $ whenever
    $ i\ne j $ is called the \textbf{identity matrix}.
\end{Definition}
\begin{Definition}{Upper Triangular, Lower Triangular}{}
    An $ n\times m $ matrix $ \Matrix{U} $ is said to be \textbf{upper triangular} if $ u_{ij}=0 $ whenever $ i>j $. An
    $ n\times m $ matrix $ \Matrix{L} $ is said to be \textbf{lower triangular} if $ \ell_{ij}=0 $ whenever $ i<j $.
    \begin{itemize}
        \item Upper triangular:
              \[ \Matrix{U}=\begin{pmatrix}
                      u_{11} & u_{12} & \cdots & u_{1m}     \\
                      0      & u_{22} & \ddots & \vdots     \\
                      \vdots & \ddots & \ddots & u_{(n-1)m} \\
                      0      & \cdots & 0      & u_{nm}
                  \end{pmatrix}. \]
        \item Lower triangular:
              \[ \Matrix{L}=\begin{pmatrix}
                      \ell_{11} & 0         & \cdots        & 0         \\
                      \ell_{21} & \ell_{22} & \ddots        & \vdots    \\
                      \vdots    & \ddots    & \ddots        & 0         \\
                      \ell_{n1} & \cdots    & \ell_{n(m-1)} & \ell_{nm}
                  \end{pmatrix}. \]
    \end{itemize}
\end{Definition}
\begin{Definition}{Vector/Matrix of $1$'s and $ 0 $'s}{}
    Vector of $ 1 $'s:
    \[ \Vector{j}=\begin{pmatrix}
            1      \\
            \vdots \\
            1
        \end{pmatrix}\in\R^n. \]
    Matrix of $ 1 $'s:
    \[ \Matrix{J}=\begin{pmatrix}
            1      & \cdots & 1      \\
            \vdots & \ddots & \vdots \\
            1      & \cdots & 1
        \end{pmatrix}\in\R^{n\times n}. \]
    Zero vector:
    \[ \Vector{0}=\begin{pmatrix}
            0      \\
            \vdots \\
            0
        \end{pmatrix}\in\R^n. \]
    Zero Matrix:
    \[ \Matrix{O}=\begin{pmatrix}
            0      & \cdots & 0      \\
            \vdots & \ddots & \vdots \\
            0      & \cdots & 0
        \end{pmatrix}\in\R^{n\times n}. \]
\end{Definition}
\begin{Definition}{Matrix Mapping}{}
    If $ \Matrix{A} $ is an $ n\times m $ matrix, then we can define a function $ T\colon \R^m\to\R^n $
    by $ T(\Vector{x})=\Matrix{A}\Vector{x} $ called a \textbf{matrix mapping}. For this mapping, we define:
    \begin{itemize}
        \item \textbf{Kernel}
              \[ \Ker{T}=\Set{\Vector{x}\in\R^m\given \Matrix{A}\Vector{x}=\Vector{0}}. \]
        \item \textbf{Image}
              \[ \Image{T}=\Set{\Matrix{A}\Vector{x}\in\R^n\given \Vector{x}\in\R^m}. \]
        \item \textbf{Rank}
              \[ \rank{T}=\dim{\Image{T}}. \]
        \item \textbf{Nullity}
              \[ \nullity{T}=\dim{\Ker{T}}. \]
    \end{itemize}
\end{Definition}
\begin{Remark}{Rank-Nullity Theorem}{}
    \[ \rank{T}+\nullity{T}=m. \]
\end{Remark}
\begin{Remark}{}{}
    Note that
    \[ \Matrix{A}=\begin{pmatrix}
            a_{11} & \cdots & a_{1m} \\
            \vdots & \ddots & \vdots \\
            a_{n1} & \cdots & a_{nm}
        \end{pmatrix}
        =\begin{pmatrix}
            \Vector{a}_1 \\
            \vdots       \\
            \Vector{a}_n
        \end{pmatrix}=\begin{pmatrix}
            \Vector{a}^1 & \cdots & \Vector{a}^m
        \end{pmatrix}. \]
    Clearly, the image of $ T $ is the space generated by $ \Vector{a}^1,\ldots,\Vector{a}^m $. Therefore,
    \[ \rank{T}=\text{column rank of }\Matrix{A}. \]
\end{Remark}
\begin{Theorem}{}{}
    $ \Matrix{A} $ and $ \Matrix{A}' $ have the same column rank.
    \tcblower{}
    \textbf{Proof}:
    Let $ \Matrix{A}\to T $, $ \Matrix{A}'\Matrix{A}\to \tilde{T} $, and $ \Matrix{A}'\to \hat{T} $.
    \begin{enumerate}[(1)]
        \item Let $ \Vector{x}\in \Ker{T} $, so we have
              $ \Matrix{A}\Vector{x}=\Vector{0}\implies \Matrix{A}'\Matrix{A}\Vector{x}=\Vector{0} $.
              Hence, $ \Vector{x}\in\Ker{\tilde{T}} $. So, $ \Ker{T}\subset \Ker{\tilde{T}} $.
        \item $ \Matrix{A}'\Matrix{A}\Vector{x}=\Vector{0}\implies \Vector{x}'\Matrix{A}'\Matrix{A}\Vector{x}=0\implies (\Matrix{A}\Vector{x})'\Matrix{A}\Vector{x}=0\implies \Matrix{A}\Vector{x}=0 $.
              So, $ \Ker{\tilde{T}}\subset \Ker{T} $.
    \end{enumerate}
    Therefore, $ \rank{T}=\rank{\tilde{T}} $. By Rank-Nullity theorem, $ \Matrix{A} $ and $ \Matrix{A}' $ have the same column rank,
    noting that $ \Image{\tilde{T}}\subset \Image{\hat{T}} $. Hence, the column rank of $ \Matrix{A} $ is less than or equal to
    the column rank of $ \Matrix{A}' $. By symmetry, the column rank of $ \Matrix{A}' $ is less than or equal to the column rank of $ (\Matrix{A}')'=\Matrix{A} $.
    Therefore, $ \Matrix{A} $ and $ \Matrix{A}' $ have the same column rank.
\end{Theorem}
\begin{Remark}{}{}
    The column rank of $ \Matrix{A}' $ is the row rank of $ \Matrix{A} $. Hence,
    \begin{align*}
        \rank{\Matrix{A}}
         & =\text{maximum number of linearly independent rows of }\Matrix{A}     \\
         & =\text{maximum number of linearly independent columns of }\Matrix{A}.
    \end{align*}
\end{Remark}
\begin{Definition}{Full Rank}{}
    Let $ \Matrix{A}\in\R^{n\times m} $. We say $ \Matrix{A} $ has \textbf{full rank}
    if $ \rank{\Matrix{A}}=\min{n,m} $
\end{Definition}
\begin{Theorem}{}{}
    Let $ \Matrix{A}\in\R^{n\times m} $ and $ \Matrix{B}\in\R^{m\times p} $.
    \begin{enumerate}[(1)]
        \item $ (\Matrix{AB})'=\Matrix{B}'\Matrix{A}'\in\R^{p\times n} $.
        \item $ \Vector{j}'\Vector{j}=n $ and $ \Vector{j}\Vector{j}'=\Matrix{J} $.
        \item $ \Matrix{J}\Matrix{J}=n \Matrix{J} $.
    \end{enumerate}
\end{Theorem}
\begin{Theorem}{}{}
    \begin{enumerate}[(1)]
        \item $ \rank{\Matrix{A}}=\rank{\Matrix{A}'} $.
        \item $ \rank{\Matrix{A}'\Matrix{A}}=\rank{\Matrix{A}\Matrix{A}'}=\rank{\Matrix{A}}=\rank{\Matrix{A}'} $.
        \item $ \rank{\Matrix{AB}}\le \min{\rank{\Matrix{A}},\rank{\Matrix{B}}} $.
    \end{enumerate}
    \tcblower{}
    \textbf{Proof}: We have already shown (1) and (2). For (3), we have
    \[ \rank{\Matrix{AB}}\le \rank{\Matrix{A}}. \]
    On the other hand,
    \[ \rank{\Matrix{AB}}=\rank{\Matrix{B}'\Matrix{A}'}\le \rank{\Matrix{B}'}=\rank{\Matrix{B}}. \]
\end{Theorem}
\begin{Remark}{Invertible Matrix Theorem}{}
    $ \Matrix{A}\in\R^{n\times n} $ is invertible (non-singular) if and only if $ \rank{\Matrix{A}}=n $,
    and we denote the inverse of $ \Matrix{A} $ by $ \Matrix{A}^{-1} $.
\end{Remark}
\begin{Remark}{Properties of Invertible Matrices}{}
    \begin{enumerate}[(1)]
        \item $ \Matrix{A}^{-1}\Matrix{A}=\Matrix{A}\Matrix{A}^{-1}=\Matrix{I} $.
        \item $ (\Matrix{A}')^{-1}=(\Matrix{A}^{-1})' $.
        \item $ (\Matrix{A}^{-1})^{-1}=\Matrix{A} $.
        \item $ (\Matrix{AB})^{-1}=\Matrix{B}^{-1}\Matrix{A}^{-1} $.
    \end{enumerate}
\end{Remark}
\begin{Definition}{Positive Definite, Positive Semidefinite}{}
    \begin{itemize}
        \item $ \Matrix{A} $ is \textbf{positive definite} when $ \Vector{x}'\Matrix{A}\Vector{x}>0 \iff \Vector{x}\ne\Vector{0} $.
        \item $ \Matrix{A} $ is \textbf{positive semidefinite} when $ \Vector{x}'\Matrix{A}\Vector{x}\ge 0 $ for all $ \Vector{x} $
              and there exists $ \Vector{x}\ne \Vector{0} $ such that $ \Vector{x}'\Matrix{A}\Vector{x}=0 $.
    \end{itemize}
\end{Definition}
\begin{Definition}{Orthogonal}{}
    $ \Matrix{A}\in\R^{n\times n} $ is \textbf{orthogonal} if $ \Matrix{A}'=\Matrix{A}^{-1} $.
\end{Definition}
\begin{Definition}{Eigenvalue, Eigenvector, Spectrum}{}
    Let $ \Matrix{A}\in\R^{n\times n} $. If there exists a vector $ \Vector{x}\ne \Vector{0} $ such that
    $ \Matrix{A}\Vector{x}=\lambda \Vector{x} $, then $ \lambda $ is called an \textbf{eigenvalue} of $ \Matrix{A} $
    and $ \Vector{v} $ is called an \textbf{eigenvector} of $ \Matrix{A} $ corresponding to $ \lambda $. The
    set of all eigenvalues of $ \Matrix{A} $ is called the \textbf{spectrum} for $ \Matrix{A} $.
\end{Definition}
\begin{Example}{}{}
    \[ \underbrace{\begin{pmatrix}
                1 & 2 \\
                2 & 1
            \end{pmatrix}}_{\Matrix{A}}\underbrace{\begin{pmatrix}
                1 \\
                1
            \end{pmatrix}}_{\Vector{v}}=\underbrace{3}_{\lambda}\underbrace{\begin{pmatrix}
                1 \\
                1
            \end{pmatrix}}_{\Vector{v}}. \]
\end{Example}
\begin{Theorem}{Spectral Decomposition}{}
    Let $ \Matrix{A}\in\R^{n\times n} $ with eigenvalues $ \lambda_1,\ldots,\lambda_n $. $ \Matrix{A} $ is symmetric if and only if
    \[ \Matrix{A}=\Matrix{Q}'\diag{\lambda_1,\ldots,\lambda_n}\Matrix{Q}, \]
    where $ \Matrix{Q} $ is an orthogonal matrix, that is, $ \Matrix{Q}\Matrix{Q}'=\Matrix{I} $.
\end{Theorem}
\begin{Theorem}{}{}
    Let $ \Matrix{A}\in\R^{n\times n} $ be a symmetric matrix.
    \begin{enumerate}[(i)]
        \item $ \Matrix{A} $ is positive definite (semidefinite) if and only if all eigenvalues are positive (non-negative).
        \item $ \Matrix{A} $ is positive definite if and only if there exists a unique lower triangular matrix $ \Matrix{L} $
              with positive diagonal elements such that $ \Matrix{A}=\Matrix{L}\Matrix{L}' $ (Cholesky decomposition).
    \end{enumerate}
\end{Theorem}
\begin{Definition}{Idempotent, Trace}{}
    \begin{itemize}
        \item $ \Matrix{A}\in\R^{n\times n} $ is \textbf{idempotent} if $ \Matrix{A}=\Matrix{A}^2 $.
        \item Let $ \text{tr}\colon\R^{n\times n}\to\R $ be defined by
              \[ \tr{\Matrix{A}}=\sum_{i=1}^{n}a_{ii} \]
              (called the \textbf{trace} of a matrix).
    \end{itemize}
\end{Definition}
\begin{Theorem}{}{}
    Let $ \Matrix{A}\in\R^{n\times m} $, $ \Matrix{B}\in\R^{m\times p} $, $ \Matrix{C}\in\R^{p\times n} $,
    and $ a,b\in\R $.
    \begin{enumerate}[(1)]
        \item $ \operatorname{tr} $ is linear: $ \tr{a \Matrix{A}+b \Matrix{B}}=a\tr{\Matrix{A}}+b\tr{\Matrix{B}} $.
        \item Cyclic property: $ \tr{\Matrix{ABC}}=\tr{\Matrix{CAB}}=\tr{\Matrix{BCA}} $.
    \end{enumerate}
    \tcblower{}
    \textbf{Proof}:
    \begin{enumerate}[(1)]
        \item By definition.
        \item Note that
              \begin{align*}
                  \tr{\Matrix{ABC}}
                   & =\sum_{i=1}^{n}\sum_{k=1}^{m}\sum_{j=1}^{p}a_{ik}b_{kj}c_{ji} \\
                   & =\sum_{j=1}^{p}\sum_{i=1}^{n}\sum_{k=1}^{m}c_{ji}a_{ik}b_{kj} \\
                   & =\tr{\Matrix{CAB}}                                            \\
                   & =\sum_{k=1}^{m}\sum_{j=1}^{p}\sum_{i=1}^{n}b_{kj}c_{ji}a_{ik} \\
                   & =\tr{\Matrix{BCA}}.
              \end{align*}
    \end{enumerate}
\end{Theorem}
\begin{Remark}{Properties of Idempotent Matrices}{}
    \begin{enumerate}[(1)]
        \item Eigenvalues of idempotent matrices are $ 1 $ or $ 0 $ since
              \begin{align*}
                   & \phantom{{}\implies{}}\Matrix{A}\Vector{x}=\lambda \Vector{x}                   \\
                   & \implies \Matrix{AA}\Vector{x}=\lambda\Matrix{A}\Vector{x}=\lambda^2 \Vector{x} \\
                   & \implies \lambda \Vector{x}=\lambda^2 \Vector{x}                                \\
                   & \implies \lambda=\lambda^2                                                      \\
                   & \implies \lambda=0\text{ or }1.
              \end{align*}
        \item Idempotent matrices are diagonalizable, that is, there exists an invertible matrix $ \Matrix{P} $ such that
              $ \Matrix{A}=\Matrix{P}^{-1}\Matrix{D}\Matrix{P} $, where
              \[ \Matrix{D}=\diag{\lambda_1,\ldots,\lambda_n},\; \forall \lambda_i=0\text{ or }1. \]
        \item If $ \Matrix{A} $ is idempotent, then $ \tr{\Matrix{A}}=\rank{\Matrix{A}} $.
              \begin{align*}
                  \tr{\Matrix{A}}
                   & =\tr{\Matrix{P}^{-1}\Matrix{D}\Matrix{P}} \\
                   & =\tr{\Matrix{D}\Matrix{PP}^{-1}}          \\
                   & =\tr{\Matrix{D}}                          \\
                   & =\lambda_1+\cdots+\lambda_n               \\
                   & =\text{\# of non-zero $ \lambda_i $'s}    \\
                   & =\rank{\Matrix{A}}.
              \end{align*}
    \end{enumerate}
\end{Remark}