\section{Lecture 6: Cochran's Theorem}
\makeheading{Lecture 6}{\printdate{2023-01-26}}%chktex 8
\begin{Lemma}{}{}
    Let $ \Matrix{C}=\Matrix{A}+\Matrix{B} $. Assume that
    $ \Matrix{A} $, $ \Matrix{B} $ are both $ n\times n $ symmetric.
    If $ \Matrix{C}^2=\Matrix{C} $, $ \Matrix{A}^2=\Matrix{A} $,
    $ \Matrix{B} $ positive semidefinite,
    \[ \rank{\Matrix{A}}+\rank{\Matrix{B}}=\rank{\Matrix{C}}, \]
    then
    \[ \Matrix{A}\Matrix{B}=\Matrix{O}. \]
    \tcblower{}
    \textbf{Proof}: Let $ \rank{\Matrix{A}}=r $, $ \rank{\Matrix{B}}=s $,
    $ \rank{\Matrix{C}}=t=r+s $. If $ \Matrix{C}^2=\Matrix{C} $, then there
    exists an orthogonal matrix $ \Matrix{\Gamma} $ such that
    \[ \Matrix{\Gamma}'\Matrix{C}\Matrix{\Gamma}=
        \begin{pmatrix}
            \Matrix{I}_t & \Matrix{O} \\
            \Matrix{O}   & \Matrix{O}
        \end{pmatrix}. \]
    Since $ \Matrix{C}=\Matrix{A}+\Matrix{B} $,
    \[ \Matrix{\Gamma}'\Matrix{A}\Matrix{\Gamma}+\Matrix{\Gamma}'\Matrix{B}\Matrix{\Gamma}
        =\begin{pmatrix}
            \Matrix{I}_t & \Matrix{O} \\
            \Matrix{O}   & \Matrix{O}
        \end{pmatrix}. \]
    $ \Matrix{A} $ and $ \Matrix{B} $ are positive semidefinite implies that
    $ \Matrix{\Gamma}'\Matrix{A\Gamma} $ and $ \Matrix{\Gamma}'\Matrix{B\Gamma} $
    are positive semidefinite. If the element on the diagonal is zero, then the corresponding
    row and columns are zeros. Hence,
    \[ \Matrix{\Gamma}'\Matrix{A\Gamma}=\begin{pmatrix}
            \Matrix{G}_t & \Matrix{O} \\
            \Matrix{O}   & \Matrix{O}
        \end{pmatrix},\qquad
        \Matrix{\Gamma}'\Matrix{B\Gamma}=\begin{pmatrix}
            \Matrix{H}_t & \Matrix{O} \\
            \Matrix{O}   & \Matrix{O}
        \end{pmatrix}. \]
    Since $ \Matrix{A}^2=\Matrix{A} $, we have
    \begin{align*}
        \Matrix{\Gamma}'\Matrix{A\Gamma}\Matrix{\Gamma}'\Matrix{A\Gamma}=
        \Matrix{\Gamma}'\Matrix{A\Gamma}
         & =
        \begin{pmatrix}
            \Matrix{G}_t & \Matrix{O} \\
            \Matrix{O}   & \Matrix{O}
        \end{pmatrix}                                                                             \\
        \implies\Matrix{\Gamma}'\Matrix{A\Gamma}\begin{pmatrix}
                                                    \Matrix{I}_t & \Matrix{O} \\
                                                    \Matrix{O}   & \Matrix{O}
                                                \end{pmatrix}
         & =\Matrix{\Gamma}'\Matrix{A\Gamma}+\Matrix{\Gamma}'\Matrix{A\Gamma}\Matrix{\Gamma}'\Matrix{B\Gamma} \\
        \implies
        \begin{pmatrix}
            \Matrix{G}_t & \Matrix{O} \\
            \Matrix{O}   & \Matrix{O}
        \end{pmatrix}\begin{pmatrix}
                         \Matrix{I}_t & \Matrix{O} \\
                         \Matrix{O}   & \Matrix{O}
                     \end{pmatrix}
         & =\begin{pmatrix}
                \Matrix{G}_t & \Matrix{O} \\
                \Matrix{O}   & \Matrix{O}
            \end{pmatrix}+
        \begin{pmatrix}
            \Matrix{G}_t & \Matrix{O} \\
            \Matrix{O}   & \Matrix{O}
        \end{pmatrix}
        \begin{pmatrix}
            \Matrix{H}_t & \Matrix{O} \\
            \Matrix{O}   & \Matrix{O}
        \end{pmatrix}                                                                             \\
         & =\begin{pmatrix}
                \Matrix{G}_t & \Matrix{O} \\
                \Matrix{O}   & \Matrix{O}
            \end{pmatrix}
        +\begin{pmatrix}
             \Matrix{G}_r \Matrix{H}_r & \Matrix{O} \\
             \Matrix{O}                & \Matrix{O}
         \end{pmatrix}                                                               \\
        \implies
        \begin{pmatrix}
            \Matrix{G}_r \Matrix{H}_r & \Matrix{O} \\
            \Matrix{O}                & \Matrix{O}
        \end{pmatrix}
         & =\Matrix{\Gamma}'\Matrix{A\Gamma \Gamma}'\Matrix{B}\Matrix{\Gamma}                                 \\
         & =\Matrix{\Gamma}'\Matrix{AB \Gamma}=\Matrix{O}.
    \end{align*}
    Therefore, $ \Matrix{AB}=\Matrix{O} $ since $ \Matrix{\Gamma} $ is orthogonal and invertible.
\end{Lemma}
\begin{Theorem}{Cochran}{}
    Let $ \Vector{X}\sim \MN{\Vector{0},\Matrix{I}_n} $,
    $ \Matrix{A}_1,\ldots,\Matrix{A}_m $ be symmetric $ n\times n $
    matrices with $ \rank{\Matrix{A}_i}=r_i $, and
    $ \sum_{i=1}^{m}\Matrix{A}_i=\Matrix{I}_n $.
    $ \Vector{X}'\Matrix{A}_i \Vector{X}\sim \chi^2(r_i) $
    are independent if and only if
    $ \sum_{i=1}^{m}r_i=n $.
    \tcblower{}
    \textbf{Proof}:
    $ (\impliedby) $ ``Sufficiency'' Assume
    $ \sum_{i=1}^{m}r_i=n $. For each $ i=1,\ldots,m $,
    set
    \[ \Matrix{B}_i=\Matrix{I}-\Matrix{A}_i, \]
    with $ \rank{\Matrix{B}_i}=s_i $. We claim that $ s_i=n-r_i $.
    \begin{align*}
        s_i
         & =\rank{\Matrix{I}-\Matrix{A}_i}      \\
         & =\rank*{\sum_{j\ne i}\Matrix{A}_j}   \\
         & \le \sum_{j\ne i}\rank{\Matrix{A}_j} \\
         & =n-r_i.
    \end{align*}
    By definition, $ \Matrix{I}=\Matrix{A}_i+\Matrix{B}_i\implies \rank{\Matrix{I}}=n $. So,
    \begin{align*}
        \rank{\Matrix{I}}
         & =n                                           \\
         & =\rank{\Matrix{A}_i+\Matrix{B}_i}            \\
         & \le \rank{\Matrix{A}_i}+\rank{\Matrix{B}_i}.
    \end{align*}
    Therefore, $ \rank{\Matrix{B}_i}\ge n-r_i\implies s_i=n-r_i $ for all $ i $.
    Hence,
    \[ \abs{\lambda \Matrix{I}-\Matrix{B}_i}=0 \]
    have $ r_i $ roots being $ 0 $. Noting that
    \begin{align*}
        \abs{\lambda \Matrix{I}-\Matrix{B}_i}
         & =\abs{(\lambda-1)\Matrix{I}-\Matrix{A}_i}     \\
         & =\abs{\tilde{\lambda}\Matrix{I}-\Matrix{A}_i}
    \end{align*}
    have $ r_i $ roots being $ 1 $. Since $ \rank{\Matrix{A}_i}=r_i $,
    it follows that all other roots of $ \Matrix{A}_i $ are $ 0 $.
    Hence,
    \[ \Matrix{A}_i=\Matrix{A}_i \Matrix{\Sigma}=\Matrix{A}_i \Matrix{I} \]
    is idempotent by Lemma 4.1. Write
    \[ \Matrix{I}=\Matrix{A}_1+(\Matrix{A}_2+\cdots+\Matrix{A}_m). \]
    Since $ \Matrix{I}^2=\Matrix{I} $, $ \Matrix{A}_1^2=\Matrix{A}_1 $,
    $ \Matrix{A}_2+\cdots+\Matrix{A}_m $ is positive semidefinite,
    it follows from Lemma 6.1 that
    \[ \Matrix{A}_1(\Matrix{A}_2+\cdots+\Matrix{A}_m)=\Matrix{O}. \]
    This implies that
    \[ \Matrix{I}^2=\Matrix{A}_1+(\Matrix{A}_2+\cdots \Matrix{A}_m), \]
    which implies that $ \Matrix{A}_2+\cdots+\Matrix{A}_m $ is idempotent.
    Applying Lemma 6.1 to
    \[ \Matrix{A}_2+\cdots+\Matrix{A}_m=\Matrix{A}_2+(\Matrix{A}_3+\cdots+\Matrix{A}_m) \]
    it follows that
    \[ \Matrix{A}_2(\Matrix{A}_3+\cdots+\Matrix{A}_m)=\Matrix{O}. \]
    By induction, we get
    \[ \Matrix{A}_{m-1}\Matrix{A}_m=\Matrix{O}. \]
    By re-labeling, we get
    \[ \Matrix{A}_i \Matrix{A}_j=0\;\forall i\ne j. \]
    Since
    \[ \Matrix{A}_i \Matrix{\Sigma}\Matrix{A}_j=\Matrix{A}_i \Matrix{A}_j=\Matrix{O}, \]
    it follows from Theorem 5.2 that
    \[ \Vector{X}' \Matrix{A}_i \Vector{X} \text{ and }\Vector{X}'\Matrix{A}_j \Vector{X} \]
    are independent. The fact that
    \[ \Vector{X}' \Matrix{A}_i \Vector{X} \sim \chi^2(r_i) \]
    follows from Theorem 4.1.
\end{Theorem}