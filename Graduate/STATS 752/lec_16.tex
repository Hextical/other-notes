\makeheading{Lecture 16}{\printdate{2023-03-09}}%chktex 8
\section{Lecture 16: Two-Way Nested Classification (Continued)}
\begin{itemize}
    \item Model 1: $ Y_{ijk}=\mu+\varepsilon_{ijk} $.
    \item Model 2: $ Y_{ijk}=\mu+\alpha_i+\varepsilon_{ijk} $.
    \item Model 3: $ Y_{ijk}=\mu+\alpha_i+\beta_{ij}+\varepsilon_{ijk} $.
\end{itemize}
\subsection*{ANOVA}
\begin{align*}
    \SST
     & =\sum_{i=1}^{a}\sum_{j=1}^{b_i}\sum_{k=1}^{n_{ij}}(Y_{ijk}-\bar{Y}_{...})^2                           \\
     & =\Vector{Y}'\Vector{Y}-n\bar{Y}_{...}^2.                                                              \\
    \SSR
     & =\sum_{i=1}^{a}\sum_{j=1}^{b_i}\sum_{k=1}^{n_{ij}}(\hat{Y}_{ijk}-\bar{Y}_{...})^2                     \\
     & =\Vector{\beta}_0'\Matrix{X}'\Vector{Y}-n\bar{Y}_{...}^2.                                             \\
     & =\textcolor{blue}{\text{SS}(\alpha,\beta:\alpha\mid \mu)}.                                            \\
    \SSE
     & =\SST-\SSR                                                                                            \\
     & =\sum_{i=1}^{a}\sum_{j=1}^{b_i}\biggl(\sum_{k=1}^{n_{ij}}Y_{ijk}^2-\tfrac{1}{n_{ij}}Y_{ij.}^2\biggr).
\end{align*}
\subsection*{ANOVA Table I}
Testing the overall effectiveness of Model 3.
\[ \begin{array}{lllll}
        \toprule
        \text{Source of Variation}                          & \text{Degrees of Freedom} & \text{Sum of Squares} & \text{Mean Square} & F         \\
        \midrule
        \text{Fitting }\alpha,\beta:\alpha\text{ after }\mu & b-1                       & \SSR                  & \MSR               & \MSR/\MSE \\
        \text{Error}                                        & n-b                       & \SSE                  & \MSE                           \\
        \midrule
        \text{Total}                                        & n-1                       & \SST                                                   \\
        \bottomrule
    \end{array} \]
Tests on the significant impact on the variation in $ Y $.
\subsection*{ANOVA Table II}
\[ \begin{array}{lllll}
        \toprule
        \text{Source of Variation}                          & \text{Degrees of Freedom} & \text{Sum of Squares}                  & \text{Mean Square}                      & F                               \\
        \midrule
        \text{Fitting }\alpha\text{ after }\mu              & a-1                       & \text{SS}(\alpha\mid \mu)              & \text{MSR}(\alpha\mid \mu)              & F(\alpha\mid \mu)               \\
        \text{Fitting }\beta:\alpha\text{ after }\mu,\alpha & b-a                       & \text{SS}(\beta:\alpha\mid \mu,\alpha) & \text{MSR}(\beta:\alpha\mid \mu,\alpha) & F(\beta: \alpha\mid \mu,\alpha) \\
        \text{Error}                                        & n-b                       & \SSE                                   & \MSE                                                                      \\
        \midrule
        \text{Total}                                        & n-1                       & \SST                                                                                                               \\
        \bottomrule
    \end{array} \]
\begin{itemize}
    \item $ \text{SS}(\alpha\mid \mu)= $ SSR of Model 2; that is, we are checking the impact of
          $ \alpha $ on the variance.
    \item $ \text{SS}(\beta:\alpha\mid \mu,\alpha)=\text{SS}(\alpha,\beta:\alpha\mid \mu)-\text{SS}(\alpha\mid \mu) $; that is,
          we are checking the impact of $ \beta $ after $ \alpha $ on the model.
\end{itemize}
\begin{align*}
    F(\alpha\mid \mu)
     & =\frac{\MSR(\alpha\mid \mu)}{\MSE} \sim F(\alpha-1,n-b). \\
    F(\beta:\alpha\mid \mu,\alpha)
     & =\frac{\MSR(\beta:\alpha)}{\MSE}\sim F(b-a,n-b).
\end{align*}
\begin{Example}{}{}
    Refer to the English and Geology data from Example 15.1.
    \begin{itemize}
        \item $ \SST=110 $
        \item $ \SSR=84 $.
        \item $ \SSE=26 $.
        \item $ \displaystyle\text{SS}(\alpha\mid \mu)=\sum_{i=1}^{a}\frac{Y_{i..}^2}{n_{i.}}-\frac{Y_{...}^2}{n}=24 $.
        \item $ \text{SS}(\beta:\alpha\mid \mu,\alpha)=84-24=60 $.
    \end{itemize}
    ANOVA Table I\@:
    \[ \begin{array}{lllll}
            \toprule
            \text{Source of Variation}                          & \text{Degrees of Freedom} & \text{Sum of Squares} & \text{Mean Square} & F             \\
            \midrule
            \text{Fitting }\alpha,\beta:\alpha\text{ after }\mu & 4                         & 84                    & 21                 & 21(7)/26=5.66 \\
            \text{Error}                                        & 7                         & 26                    & 26/7                               \\
            \midrule
            \text{Total}                                        & 11                        & 110                                                        \\
            \bottomrule
        \end{array} \]
    ANOVA Table II\@:
    \[ \begin{array}{lllll}
            \toprule
            \text{Source of Variation}                          & \text{Degrees of Freedom} & \text{Sum of Squares} & \text{Mean Square} & F   \\
            \midrule
            \text{Fitting }\alpha\text{ after }\mu              & 1                         & 24                    & 24                 & 6.5 \\
            \text{Fitting }\beta:\alpha\text{ after }\mu,\alpha & 3                         & 60                    & 20                 & 5.4 \\
            \text{Error}                                        & 7                         & 26                    & 26/7                     \\
            \midrule
            \text{Total}                                        & 11                        & 110                                              \\
            \bottomrule
        \end{array} \]
    \begin{itemize}
        \item Testing $ \alpha,\beta:\alpha\text{ after }\mu $: $ 5.66>F_{0.05}(4,7)=4.12 $, reject the fact
              that we do not need $ \alpha $ and $ \beta $, so Model 3 is adequate. Model 3
              accounts for significantly more variation than Model 1.
        \item Testing $ \alpha\text{ after }\mu $: $ 6.5>F_{0.05}(1,7)=5.59 $, we need $ \alpha $.
              Model 2 accounts for significantly more variation than Model 1.
        \item Testing $ \beta:\alpha\text{ after }\mu,\alpha $: $ 5.4>F_{0.05}(3,7)=4.35 $, we need $ \beta $.
              Model 3 accounts for significantly more variation than Model 2.
    \end{itemize}
\end{Example}
\subsection*{Estimable Combinations}
$ \Vector{b}'\Matrix{F}\Matrix{X}'\Matrix{X}=\Vector{b}' $.
Write $ \Vector{b}'\Matrix{F}\Matrix{X}'=\Vector{c}' $.
Also, $ \Vector{b}'\Vector{\beta}=\Vector{c}'\Matrix{X}\Vector{\beta} $. Idea:
\[ \Vector{c}'=\begin{pmatrix}
        c_{111} & \cdots c_{11n_{11}} & \cdots & c_{ij1} & \cdots & c_{ijn_{ij}} & \cdots
    \end{pmatrix}. \]
\[ \Vector{c}'\Matrix{X}=\begin{pmatrix}
        w_{11} & \cdots & w_{1b_1} & w_{21} & \cdots & w_{2b_2} & \cdots & w_{a1} & \cdots & w_{ab_a}
    \end{pmatrix}. \]
\begin{align*}
    \Vector{b}'\Vector{\beta}
     & =\begin{pmatrix}
            w_{11} & \cdots & w_{1b_1} & \cdots
        \end{pmatrix}\Vector{\beta}                            \\
     & =\sum_{i=1}^{a}\sum_{j=1}^{b_i}w_{ij}(\mu+\alpha_i+\beta_{ij}).
\end{align*}
Conclusion:
\begin{enumerate}[(1)]
    \item $ \mu $, $ \mu+\alpha_i $, $ \alpha_i $ are not estimable.
    \item $ \mu+\alpha_i+\beta_{ij} $ is estimable.
    \item $ \beta_{ij}-\beta_{i\ell} $ is estimable.
    \item $ \mu+\alpha_i+\sum_{j=1}^{b_i}w_{ij}\beta_{ij} $ is estimable if $ \sum_{j=1}^{b_i}w_{ij}=1 $.
    \item $ \alpha_i-\alpha_\ell+\sum_{j=1}^{n_i}w_{ij}\beta_{ij}-\sum_{\ell=1}^{\beta_\ell}w_{i\ell}\beta_{i\ell} $
          is estimable if $ \sum_{j=1}^{b_i}w_{ij}=\sum_{\ell=1}^{b_\ell}w_{i\ell}=1 $.
    \item $ \alpha_i-\alpha_\ell $ are not estimable.
\end{enumerate}
\subsection*{Hypothesis Testing}
\begin{itemize}
    \item $ \HN $: $ \beta_{i1}=\cdots=\beta_{ib_i} $, $ i=1,\ldots,a $;
    \item $ \HA $: at least one equality fails.
\end{itemize}
Define $ (b_1-1)+(b_2-1)+\cdots+(b_a-1)=n' $ and
$ m=1+a+\sum_{i=1}^{a}b_i $. Hence, $ \Matrix{B}'\in\R^{n'\times m} $, where $ \rank{\Matrix{B}}=b-a $. Therefore,
\[ \Matrix{B}'=\begin{pmatrix}
        \Matrix{O}_{n'\times (a+1)} & \diag{\Matrix{Z}_{b_1},\ldots,\Matrix{Z}_{b_a}}
    \end{pmatrix}, \]
where $ \Matrix{Z}_{b_i}=\begin{pmatrix}
        \Matrix{1}_{(b_i-1)\times 1} & \diag{\underbrace{-1,\ldots,-1}_{b_i-1}}
    \end{pmatrix}\in\R^{(b_i-1)\times b_i} $ for $ i=1,\ldots,a $.
\[ Q=(\Matrix{B}'\Vector{\beta}_0)'(\Matrix{B}'\Matrix{F}\Matrix{B})(\Matrix{B}'\Vector{\beta}_0)=\text{SS}(\beta:\alpha\mid \mu,\alpha).  \]
\[ F=\frac{Q/(n-a)}{\SSE/(n-b)} \sim F(b-a,n-b). \]
\begin{Remark}{}{}
    If $ n_{ij}=r $ for all $ i,j $, $ b_i=c $ for all $ i $, then the model
    is called a \textbf{balanced} model.
\end{Remark}
From Example 15.1, we have
\[ \Vector{\beta}=\begin{pmatrix}
        \mu        \\
        \alpha_1   \\
        \alpha_2   \\
        \beta_{11} \\
        \beta_{12} \\
        \beta_{21} \\
        \beta_{22} \\
        \beta_{23}
    \end{pmatrix}, \]
\[ \Vector{b}'=\begin{pmatrix}
        a_1 & a_2 & a_3 & b_{11} & b_{12} & b_{21} & b_{22} & b_{23}
    \end{pmatrix}. \]
Therefore, $ \Vector{b}'\Matrix{F}\Matrix{X}'\Matrix{X}=\Vector{b}' $, where
\begin{align*}
    a_1 & = b_{11} + b_{12} + b_{21} + b_{22} + b_{23}, \\
    a_2 & =b_{11} +b_{12},                              \\
    a_3 & =b_{21}+b_{22}+b_{23.}
\end{align*}
Therefore,
\begin{align*}
    \Vector{b}'\Vector{\beta}
     & =a_1\mu+a_2\alpha_1+a_3\alpha_2+b_{11}\beta_{11}+b_{12}\beta_{12}+b_{21}\beta_{21}
    +b_{22}\beta_{22}+\beta_{23}\beta_{23}                                                                                                                                   \\
     & =b_{11}(\mu+\alpha_1+\beta_{11})+b_{12}(\mu+\alpha_1+\beta_{12})+b_{21}(\mu+\alpha_2+\beta_{21})+\beta_{22}(\mu+\alpha_2+\beta_{22})+b_{23}(\mu+\alpha_2+\beta_{23}).
\end{align*}