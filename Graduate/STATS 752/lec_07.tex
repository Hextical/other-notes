\makeheading{Lecture 7}{\printdate{2023-01-30}}%chktex 8
\section{Lecture 7: Full Rank Regression}
Model:
\[ Y=\beta_0+\beta_1x_1+\cdots+\beta_k x_k+\varepsilon, \]
where $ x_i $ are \textbf{predictors}, $ Y $ is the \textbf{response},
and $ \varepsilon $ is noise. If we have $ i=1,\ldots,n $ observations, then the model becomes:
\[ Y_i=\beta_0+\beta_1x_{i1}+\cdots+\beta_k x_{ik}+\varepsilon_i=\Matrix{X}\Vector{\beta}+\Vector{\varepsilon}, \]
where
\[ \Vector{Y}=\begin{pmatrix}
        Y_1    \\
        \vdots \\
        Y_n
    \end{pmatrix},\quad
    \Matrix{X}=\begin{pmatrix}
        1      & x_{11} & \cdots & x_{1k} \\
        1      & x_{21} & \cdots & x_{2k} \\
        \vdots & \vdots & \ddots & \vdots \\
        1      & x_{n1} & \cdots & x_{nk}
    \end{pmatrix},\quad
    \Vector{\beta}=\begin{pmatrix}
        \beta_0 \\
        \vdots  \\
        \beta_k
    \end{pmatrix},\quad
    \Vector{\varepsilon}=
    \begin{pmatrix}
        \varepsilon_1 \\
        \vdots        \\
        \varepsilon_n
    \end{pmatrix} \]
Assumptions:
\begin{enumerate}[(1)]
    \item $ \E{\varepsilon_i}=0 $.
    \item $ \Var{\varepsilon_i}=\sigma^2 $.
    \item $ \Cov{\varepsilon_i,\varepsilon_j}=0 $ for $ i\ne j $.
\end{enumerate}
Full rank assumption:
\begin{enumerate}[(1)]
    \item $ k<n $;
    \item $ \rank{\Matrix{X}}=k+1 $.
\end{enumerate}
\subsection*{Method 1: Least Squares Method}
\[ L=\sum_{i=1}^{n}\varepsilon_i^2=\Vector{\varepsilon}'\Vector{\varepsilon}=
    (\Vector{Y}-\Matrix{X}\Vector{\beta})'(\Vector{Y}-\Matrix{X}\Vector{\beta}). \]
If we minimize with respect to $ \Vector{\beta} $, we get
\[\pdv{L}{\Vector{\beta}} =\begin{pmatrix}
        \pdv{L}{\beta_0} \\
        \vdots           \\
        \pdv{L}{\beta_k}
    \end{pmatrix}         =-2 \Matrix{X}\Vector{Y}+2 \Matrix{X}'\Matrix{X}\Vector{\beta} =0
    \implies\hat{\Vector{\beta}} =(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Vector{Y}.\]
\begin{Theorem}{}{}
    $ \hat{\Vector{\beta}} $ is an unbiased estimator of $ \Vector{\beta} $.
    \tcblower{}
    \textbf{Proof}:
    \begin{align*}
        \E{\hat{\Vector{\beta}}}
         & =\E*{(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Vector{Y}}                                    \\
         & =(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\E{\Vector{Y}}                                     \\
         & =(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\E*{\Matrix{X}\Vector{\beta}+\Vector{\varepsilon}} \\
         & =(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Matrix{X}\Vector{\beta}+\Vector{0}                \\
         & =\Vector{\beta}.
    \end{align*}
\end{Theorem}
\begin{Theorem}{}{}
    If $ \Cov{\Vector{Y},\Vector{Y}}=\sigma^2 \Matrix{I} $, then
    $ \Cov{\hat{\Vector{\beta}},\hat{\Vector{\beta}}}=\sigma^2(\Matrix{X}'\Matrix{X})^{-1} $.
    \tcblower{}
    \textbf{Proof}:
    \begin{align*}
        \Cov{\hat{\Vector{\beta}},\hat{\Vector{\beta}}}
         & =\E*{\bigl(\hat{\Vector{\beta}}-\E{\hat{\Vector{\beta}}}\bigr)\bigl(\hat{\Vector{\beta}}-\E{\hat{\Vector{\beta}}}\bigr)'}                                                                                                                   \\
         & =\E*{\bigl((\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}\Vector{Y}-(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\E{\Vector{Y}}\bigr)\bigl((\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}\Vector{Y}-(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\E{\Vector{Y}}\bigr)'} \\
         & =\E*{(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\bigl(\Vector{Y}-\E{\Vector{Y}}\bigr)(\Vector{Y}-\E{\Vector{Y}})'\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}}                                                                                        \\
         & =(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\E*{(\Vector{Y}-\E{\Vector{Y}})(\Vector{Y}-\E{\Vector{Y}})'}\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}                                                                                                  \\
         & =\sigma^2 (\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}.                                                                                                                                                    \\
         & =\sigma^2  (\Matrix{X}'\Matrix{X})^{-1}.
    \end{align*}
\end{Theorem}
\subsection*{Estimation of $ \sigma^2 $}
\begin{itemize}
    \item Residual:
          \begin{align*}
              (\Vector{Y}-\hat{\Vector{Y}})
               & =\hat{\Vector{\varepsilon}}                                                \\
               & =(\Vector{Y}-\Matrix{X}\hat{\Vector{\beta}})                               \\
               & =(\Vector{Y}-\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Vector{Y})  \\
               & =(\Matrix{I}-\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}')\Vector{Y}.
          \end{align*}
    \item Let $ \Matrix{H}=\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}' $ be the \textbf{hat matrix}.
    \item Since $ \Matrix{H} $ is idempotent, we may write
          \begin{align*}
              \text{SSE}
               & =\hat{\Vector{\varepsilon}}'\hat{\Vector{\varepsilon}}       \\
               & =\norm{\Vector{Y}-\hat{\Vector{Y}}}^2                        \\
               & =(\Vector{Y}-\hat{\Vector{Y}})'(\Vector{Y}-\hat{\Vector{Y}}) \\
               & =\Vector{Y}'(\Matrix{I}-\Matrix{H})\Vector{Y}.
          \end{align*}
\end{itemize}
\begin{Theorem}{}{}
    \[ S^2=\frac{\text{SSE}}{n-(k+1)} \]
    is an unbiased estimator of $ \sigma^2 $.
    \tcblower{}
    \textbf{Proof}:
    \begin{align*}
        \E{S^2}
         & =\frac{1}{n-(k+1)}\E{\Vector{Y}'(\Matrix{I}-\Matrix{H})\Vector{Y}}                                                         \\
         & =\frac{1}{n-(k+1)}\bigl[\tr{(\Matrix{I}-\Matrix{H})\Matrix{\Sigma}}+\Vector{\mu}'(\Matrix{I}-\Matrix{H})\Vector{\mu}\bigr] \\
         & =\frac{\sigma^2}{n-(k+1)}\tr{\Matrix{I}-\Matrix{H}}                                                                        \\
         & =\frac{\sigma^2}{n-(k+1)}\bigl(\tr{\Matrix{I}}-\tr{\Matrix{H}}\bigr)                                                       \\
         & =\frac{\sigma^2}{n-(k+1)}\bigl(\tr{\Matrix{I}}-\rank{\Matrix{H}}\bigr)                                                     \\
         & =\frac{\sigma^2}{n-(k+1)}\bigl(n-(k+1)\bigr)                                                                               \\
         & =\sigma^2.
    \end{align*}
    Note that $ \Vector{\mu}=\Matrix{X}\Vector{\beta}\implies \Vector{\mu}'=\Vector{\beta}'\Matrix{X}' $, so
    \begin{align*}
        \Matrix{X}'(\Matrix{I}-\Matrix{H})\Matrix{X}
         & =\Matrix{X}'\Matrix{X}-\Matrix{X}'\Matrix{H}\Matrix{X}                                        \\
         & =\Matrix{X}'\Matrix{X}-\Matrix{X}'\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Matrix{X} \\
         & =\Matrix{O}.
    \end{align*}
\end{Theorem}
\subsection*{Maximum Likelihood Estimators for $ \Vector{\beta} $ and $ \sigma^2 $}
\begin{Theorem}{}{}
    If $ \Vector{Y}\sim \MN{\Matrix{X}\Vector{\beta},\sigma^2 \Matrix{I}_n} $, where $ \Matrix{X} $
    is $ n\times(k+1) $ of rank $ k+1<n $, then the maximum likelihood estimators of $ \Vector{\beta} $ and $ \sigma^2 $ are
    \[ \hat{\Vector{\beta}}=(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Vector{Y},\qquad \hat{\sigma}^2=\frac{\text{SSE}}{n}=\frac{(\Vector{Y}-\Matrix{X}\hat{\Vector{\beta}})'(\Vector{Y}-\Matrix{X}\hat{\Vector{\beta}})}{n}. \]
    \tcblower{}
    \textbf{Proof}: The likelihood function is given by the multivariate normal density
    \begin{align*}
        L(\Vector{\beta},\sigma^2)
         & =f(\Vector{Y};\Vector{\beta},\sigma^2)                                                                                                                                        \\
         & =\frac{1}{(2\pi)^{n/2}\abs{\sigma^2 \Matrix{I}}^{1/2}}\exp*{-\frac{(\Vector{Y}-\Matrix{X}\Vector{\beta})'(\sigma^2 \Matrix{I})^{-1}(\Vector{Y}-\Matrix{X}\Vector{\beta})}{2}} \\
         & =(2\pi\sigma^2)^{-n/2}\exp*{-\frac{(\Vector{Y}-\Matrix{X}\Vector{\beta})'(\Vector{Y}-\Matrix{X}\Vector{\beta})}{2\sigma^2}}.
    \end{align*}
    The log-likelihood function is
    \begin{align*}
        \ell(\Vector{\beta},\sigma^2)
         & =\ln*{L(\Vector{\beta},\sigma^2)}                                                                                                                                                    \\
         & =-\frac{n}{2}\ln{2\pi}-\frac{n}{2}\ln{\sigma^2}-\frac{1}{2\sigma^2}(\Vector{Y}-\Matrix{X}\Vector{\beta})'(\Vector{Y}-\Matrix{X}\Vector{\beta})                                       \\
         & =-\frac{n}{2}\ln{2\pi}-\frac{n}{2}\ln{\sigma^2}-\frac{1}{2\sigma^2}(\Vector{Y}'\Vector{Y}-2 \Vector{Y}'\Matrix{X}\Vector{\beta}+\Vector{\beta}'\Matrix{X}'\Matrix{X}\Vector{\beta}).
    \end{align*}
    Taking the derivative with respect to $ \Vector{\beta} $ yields
    \begin{align*}
        \Vector{0}                          & =\pdv{\ell(\Vector{\beta},\sigma^2)}{\Vector{\beta}}=-\frac{1}{2\sigma^2}(-2 \Matrix{X}'\Vector{Y}+2 \Matrix{X}'\Matrix{X}\Vector{\beta}) \\
        \Vector{0}                          & =2\Matrix{X}'\Vector{Y}-2 \Matrix{X}'\Matrix{X}\Vector{\beta}                                                                             \\
        \Matrix{X}'\Matrix{X}\Vector{\beta} & =\Matrix{X}'\Vector{Y}                                                                                                                    \\
        \hat{\Vector{\beta}}                & =(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Vector{Y}.
    \end{align*}
    Taking the derivative with respect to $ \sigma^2 $ yields
    \begin{align*}
        0                              & =\pdv{\ell(\Vector{\beta},\sigma^2)}{\sigma^2}=-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}(\Vector{Y}-\Matrix{X}\Vector{\beta})'(\Vector{Y}-\Matrix{X}\Vector{\beta}) \\
        \frac{n(2\sigma^4)}{2\sigma^2} & =(\Vector{Y}-\Matrix{X}\Vector{\beta})'(\Vector{Y}-\Matrix{X}\Vector{\beta})                                                                                       \\
        \hat{\sigma}^2                 & =\frac{(\Vector{Y}-\Matrix{X}\hat{\Vector{\beta}})'(\Vector{Y}-\Matrix{X}\hat{\Vector{\beta}})}{n}.
    \end{align*}
\end{Theorem}
\subsection*{Properties of $ \hat{\Vector{\beta}} $ and $ \hat{\sigma}^2 $}
\begin{Theorem}{}{}
    If $ \Vector{Y}\sim \MN{\Matrix{X}\Vector{\beta},\sigma^2 \Matrix{I}_n} $, where $ \Matrix{X} $
    is $ n\times(k+1) $ of rank $ k+1<n $, and $ \Vector{\beta}=(\beta_0,\beta_1,\ldots,\beta_k)' $, then
    the maximum likelihood estimators of $ \hat{\Vector{\beta}} $ and $ \hat{\sigma}^2 $ given in Theorem 7.1 have the following distributional properties:
    \begin{enumerate}[(1)]
        \item $ \hat{\Vector{\beta}}\sim \MN{\Vector{\beta},\sigma^2(\Matrix{X}'\Matrix{X})^{-1}} $.
        \item $ \dfrac{n\hat{\sigma}^2}{\sigma^2}\sim \chi^2\bigl(n-(k+1)\bigr) $.
        \item $ \hat{\Vector{\beta}} $ and $ \hat{\sigma}^2 $ are independent.
    \end{enumerate}
    \tcblower{}
    \textbf{Proof}:
    \begin{enumerate}[(1)]
        \item Note that $ \Vector{Y}\sim \MN{\Vector{\mu},\Matrix{\Sigma}}\implies \Matrix{A}\Vector{Y}\sim \MN{\Matrix{A}\Vector{\mu},\Matrix{A}\Matrix{\Sigma}\Matrix{A}'} $.
              Let $ \Matrix{A}=(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}' $, $ \Vector{\mu}=\Matrix{X}\Vector{\beta} $, and $ \Matrix{\Sigma}=\sigma^2 \Matrix{I}_n $. Now,
              \begin{align*}
                  \Matrix{A}\Vector{Y}=\hat{\Vector{\beta}} & \sim \MN*{(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Matrix{X}\Vector{\beta},(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\sigma^2 \Matrix{I}_n\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}} \\
                                                            & \sim\MN{\Vector{\beta},\sigma^2(\Matrix{X}'\Matrix{X})^{-1}}.
              \end{align*}
        \item Note that
              \begin{align*}
                  \frac{n\hat{\sigma}^2}{\sigma^2}
                   & =\frac{\text{SSE}}{\sigma^2}                                   \\
                   & =\frac{\Vector{Y}'(\Matrix{I}-\Matrix{H})\Vector{Y}}{\sigma^2} \\
                   & =\Vector{W}'(\Matrix{I}-\Matrix{H})\Vector{W},
              \end{align*}
              where $ \Vector{W}=\frac{\Vector{Y}}{\sigma}\sim \MN*{\frac{\Matrix{X}\Vector{\beta}}{\sigma},\Matrix{I}} $.
              It follows from Theorem 4.1 that
              \[ \frac{n\hat{\sigma}^2}{\sigma^2}\sim \chi^2(r,\lambda), \]
              with $ r=\rank{\Matrix{I}-\Matrix{H}}=\tr{\Matrix{I}-\Matrix{H}}=n-(k+1) $ and
              \begin{align*}
                  \lambda
                   & =\frac{1}{2}\Vector{\mu}'\Matrix{A}\Vector{\mu}                                                                                  \\
                   & =\frac{1}{2}\biggl(\frac{\Matrix{X}\Vector{\beta}}{\sigma}\biggr)'(\Matrix{I}-\Matrix{H})\frac{\Matrix{X}\Vector{\beta}}{\sigma} \\
                   & =\frac{1}{2\sigma^2}\Vector{\beta}'\bigl[\Matrix{X}'(\Matrix{I}-\Matrix{H})\Matrix{X}\bigr]\Vector{\beta}                        \\
                   & =0,
              \end{align*}
              where $ \Vector{\mu}=\E{\Vector{W}} $ and $ \Matrix{A}=\Matrix{I}-\Matrix{H} $.
        \item Note that $ \hat{\Vector{\beta}}=(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Vector{Y} $ and
              \[ \hat{\sigma}^2=\frac{\text{SSE}}{n}=\frac{1}{n}\Vector{Y}'(\Matrix{I}-\Matrix{H})\Vector{Y}=\Vector{Y}'\biggl(\frac{\Matrix{I}-\Matrix{H}}{n}\biggr)\Vector{Y}. \]
              Let $ \Vector{Y}=\Vector{X} $, $ \Matrix{B}=(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X} $, and
              $ \Matrix{A}=\frac{\Matrix{I}-\Matrix{H}}{n} $. Relabelling,
              \[ \hat{\Vector{\beta}}=\Matrix{B}\Vector{X}, \]
              \[ \hat{\sigma}^2=\Vector{X}'\Matrix{A}\Vector{X}. \]
              Now,
              \begin{align*}
                  (\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}\biggl(\frac{\Matrix{I}-\Matrix{H}}{n}\biggr)
                   & =\frac{1}{n}\Bigl[(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'-(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'\Bigr] \\
                   & =0.
              \end{align*}
              The result follows from Theorem 5.1.
    \end{enumerate}
\end{Theorem}