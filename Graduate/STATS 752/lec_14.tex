\makeheading{Lecture 14}{\printdate{2023-03-02}}%chktex 8
\section{Lecture 14: General Linear Hypothesis Testing}
Let $ \Matrix{B}\in\R^{(k+1)\times s} $
with $ \rank{\Matrix{B}}=s $ and $ \Vector{m}\in\R^s $.
We want to test $ \HN $: $ \Matrix{B}'\Vector{\beta}=\Vector{m} $
versus $ \HA $: $ \Matrix{B}'\Vector{\beta}\ne \Vector{m} $. Write
\[ \Matrix{B}=\begin{pmatrix}
        b_{11}     & \cdots & b_{1s}     \\
        b_{21}     & \cdots & b_{2s}     \\
        \vdots     & \ddots & \vdots     \\
        b_{(k+1)1} & \cdots & b_{(k+1)s}
    \end{pmatrix}=\begin{pmatrix}
        \Vector{b}_1 & \Vector{b}_2 & \cdots & \Vector{b}_s
    \end{pmatrix}. \]
Hence,
\[ \Matrix{B}'\Vector{\beta}=
    \begin{pmatrix}
        \Vector{b}_1' \Vector{\beta} & \cdots & \Vector{b}_s' \Vector{\beta}
    \end{pmatrix}'. \]
In order for $ \Matrix{B}'\Vector{\beta} $ to be estimable, we need
$ \Vector{b}_i'\Vector{\beta} $ to be estimable for $ i=1,\ldots,s $.
This requires
\[ \Vector{b}_i'\Matrix{F}\Matrix{X}'\Matrix{X}=\Vector{b}_i'
    \iff \Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}=\Matrix{B}'. \]
\begin{Theorem}{}{}
    Let $ \Vector{\beta}_0 $ be a solution to the normal equation
    \[ \Matrix{X}'\Matrix{X}\Vector{\beta}=\Matrix{X}'\Vector{Y}. \]
    Since $ \Matrix{X}\Matrix{F}\Matrix{X}'\Matrix{X}=\Matrix{X} $,
    it follows that $ \Matrix{X}\Vector{\beta} $ is estimable and
    $ \Matrix{X}\Vector{\beta}_0 $ is an estimator of $ \Matrix{X}\Vector{\beta} $.

    Assume that $ \Matrix{B}'\Vector{\beta} $ is estimable.
    \begin{enumerate}[(1)]
        \item $ \E{\Matrix{B}'\Vector{\beta}_0}=\Matrix{B}\Vector{\beta} $.
        \item $ \Var{\Matrix{B}'\Vector{\beta}_0}=\sigma^2 \Matrix{B}'\Matrix{F}\Matrix{B} $.
    \end{enumerate}
    \tcblower{}
    \textbf{Proof}:
    Since $ \Matrix{B}'\Vector{\beta} $ is estimable, $ \Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}=\Matrix{B}' $.
    Furthermore,
    \[ \Matrix{B}'\Vector{\beta}_0=\Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Vector{\beta}_0. \]
    \begin{enumerate}[(1)]
        \item For the expectation,
              \begin{align*}
                  \E{\Matrix{B}'\Vector{\beta}_0}
                   & =\E{\Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Vector{\beta}_0} \\
                   & =\Matrix{B}'\Matrix{F}\E{\Matrix{X}'\Matrix{X}\Vector{\beta}_0} \\
                   & =\Matrix{B}'\Matrix{F}\E{\Matrix{X}'\Vector{Y}}                 \\
                   & =\Matrix{B}'\Matrix{F}\Matrix{X}'\E{\Vector{Y}}                 \\
                   & =\Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Vector{\beta}       \\
                   & =\Matrix{B}'\Vector{\beta}.
              \end{align*}
        \item For the variance,
              \begin{align*}
                  \Var{\Matrix{B}'\Vector{\beta}_0}
                   & =\Var{\Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Vector{\beta}_0}                         \\
                   & =\Var{\Matrix{B}'\Matrix{F}\Matrix{X}'\Vector{Y}}                                         \\
                   & =(\Matrix{B}'\Matrix{F}\Matrix{X}')\sigma^2 \Matrix{I}(\Matrix{B}'\Matrix{F}\Matrix{X}')' \\
                   & =\sigma^2 \Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Matrix{F}'\Matrix{B}                 \\
                   & =\sigma^2 \Matrix{B}'\Matrix{F}\Matrix{B}.
              \end{align*}
    \end{enumerate}
\end{Theorem}
\begin{Theorem}{}{}
    If $ \Matrix{B}'\Vector{\beta} $ is estimable, then
    $ \rank{\Matrix{B}'\Matrix{F}\Matrix{B}}=s $.
    \tcblower{}
    \textbf{Proof}: $ \Matrix{B}'\Vector{\beta} $
    is estimable implies that there exists $ \Matrix{C}\in\R^{(k+1)\times s} $
    matrix such that $ \Matrix{B}'=\Matrix{C}'\Matrix{X}'\Matrix{X} $ (extension of Theorem 13.3).
    We know that $ \rank{\Matrix{B}'}=s=\rank{\Matrix{C}'\Matrix{X}'\Matrix{X}}\le \rank{\Matrix{C}'}\le s $.
    Therefore, $ \rank{\Matrix{C}}=\rank{\Matrix{C}'}=s $. Also,
    $ \rank{\Matrix{B}'}\le \rank{\Matrix{C}'\Matrix{X}}\le \rank{\Matrix{C}'}=s $,
    which implies that $ \rank{\Matrix{C}'\Matrix{X}'}=\rank{\Matrix{X}\Matrix{C}}=s $.
    Now,
    \begin{align*}
        \Matrix{B}'\Matrix{F}\Matrix{B}
         & =\Matrix{C}'\Matrix{X}'\Matrix{X}\Matrix{F}\Matrix{X}'\Matrix{X}\Matrix{C} \\
         & =\Matrix{C}'\Matrix{X}'\Matrix{X}\Matrix{C}                                \\
         & =(\Matrix{X}\Matrix{C})'(\Matrix{X}\Matrix{C}),
    \end{align*}
    and by Theorem 1.3, we have that
    \begin{align*}
        \rank{\Matrix{B}'\Matrix{F}\Matrix{B}}
         & =\rank[\big]{(\Matrix{X}\Matrix{C})'(\Matrix{X}\Matrix{C})} \\
         & =\rank{\Matrix{X}\Matrix{C}}                                \\
         & =s.
    \end{align*}
\end{Theorem}
\begin{Theorem}{}{}
    Set $ Q=(\Matrix{B}'\Vector{\beta}_0-\Vector{m})'(\Matrix{B}'\Matrix{F}\Matrix{B})^{-1}(\Matrix{B}'\Vector{\beta}_0-\Vector{m}) $.
    Then, the following hold:
    \begin{enumerate}[(1)]
        \item $ Q/\sigma^2 \sim \chi^2(s,\lambda) $, where
              \[ \lambda=\frac{1}{2\sigma^2}(\Matrix{B}'\Vector{\beta}-\Vector{m})'(\Matrix{B}'\Matrix{F}\Matrix{B})^{-1}(\Matrix{B}'\Vector{\beta}-\Vector{m}). \]
        \item $ Q $ and $ \SSE $ are independent.
    \end{enumerate}
    \tcblower{}
    \textbf{Proof}:
    \begin{enumerate}[(1)]
        \item The main idea is to use Theorem 4.1.
              Note that $ \Vector{Y}\sim \N{\Matrix{X}\Vector{\beta},\sigma^2 \Matrix{I}} $.
              \begin{align*}
                  \Matrix{B}'\Vector{\beta}_0-\Vector{m}
                   & =\Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Vector{\beta}_0-\Vector{m}                        \\
                   & =\Matrix{B}'\Matrix{F}\Matrix{X}'\Vector{Y}-\Vector{m}                                        \\
                   & \sim\MN*{\Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Vector{\beta}-\Vector{m},\Matrix{\Sigma}} \\
                   & =\MN{\underbrace{\Matrix{B}'\Vector{\beta}-\Vector{m}}_{\Vector{\mu}},\Matrix{\Sigma}},
              \end{align*}
              where
              \begin{align*}
                  \Matrix{\Sigma}
                   & =\sigma^2 \Matrix{B}'\Matrix{F}\Matrix{X}\Matrix{X}'(\Matrix{B}'\Matrix{F}\Matrix{X}')' \\
                   & =\sigma^2 \Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Matrix{B}'\Matrix{F}'\Matrix{B}    \\
                   & =\sigma^2 \Matrix{B}'\Matrix{F}\Matrix{B}.
              \end{align*}
              Therefore,
              \[ \Matrix{A}\Matrix{\Sigma}=(\Matrix{B}'\Matrix{F}\Matrix{B})^{-1}\sigma^2(\Matrix{B}'\Matrix{F}\Matrix{B})=\Matrix{I}, \]
              which is idempotent. It follows from Theorem 4.1 that $ Q \sim \chi^2(s,\lambda) $,
              where $ \lambda $ is defined above.
        \item $ Q=(\Matrix{B}'\Vector{\beta}_0-\Vector{m})'(\Matrix{B}'\Matrix{F}\Matrix{B})^{-1}(\Matrix{B}'\Vector{\beta}_0-\Vector{m}) $.
              Idea: rewrite $ Q $ into a quadratic form and use Theorem 5.1.
              \[ Q=(\Matrix{B}'\Matrix{F}\Matrix{X}'\Vector{Y}-\Vector{m})'(\Matrix{B}'\Matrix{F}\Matrix{B})^{-1}(\Matrix{B}'\Matrix{F}\Matrix{X}'\Vector{Y}-\Vector{m}) \]

              Recall that $ \SSE=\Vector{Y}'(\Matrix{I}-\Matrix{X}\Matrix{F}\Matrix{X}')\Vector{Y} $,
              and
              \begin{align*}
                  \Matrix{B}'\Vector{\beta}_0
                   & =\Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Vector{\beta}_0 \\
                   & =\Matrix{B}'\Matrix{F}\Matrix{X}'\Vector{Y}                 \\
                   & =(\Matrix{B}'\Matrix{F}\Matrix{X}')\Vector{Y}.
              \end{align*}
              Hence,
              \begin{align*}
                  (\Matrix{B}'\Matrix{F}\Matrix{X}')\sigma^2 \Matrix{I}(\Matrix{I}-\Matrix{X}\Matrix{F}\Matrix{X}')
                   & =\sigma^2[\Matrix{B}'\Matrix{F}\Matrix{X}'-\Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Matrix{F}\Matrix{X}'] \\
                   & =\sigma^2[\Matrix{B}'\Matrix{F}\Matrix{X}'-\Matrix{B}'\Matrix{F}\Matrix{X}']                                \\
                   & =\Matrix{O}.
              \end{align*}
              Therefore, by Theorem 5.1, we obtain the result.

              \underline{Alternative Proof Idea}: Prove that $ \Matrix{B}'\Matrix{F}\Matrix{X}'\Vector{Y} $ is independent of $ \SSE $.
              \begin{align*}
                  \Matrix{B}'\Matrix{F}\Matrix{X}'(\Matrix{I}-\Matrix{X}\Matrix{F}\Matrix{X}')
                   & =\Matrix{B}'\Matrix{F}\Matrix{X}'-\Matrix{B}'\Matrix{F}\Matrix{X}'\Matrix{X}\Matrix{F}\Matrix{X}' \\
                   & =\Matrix{B}'\Matrix{F}\Matrix{X}.
              \end{align*}
    \end{enumerate}
\end{Theorem}
\subsection*{General One-Way Classification Model}
\[ Y_{ij}=\mu+\alpha_i+\varepsilon_{ij}, \]
where $ i=1,\ldots,a $ are the factors, $ j=1,\ldots,n_i $ are the levels,
and $ n=n_1+\cdots+n_a $ are the total number of observations.
\begin{itemize}
    \item $ \mu $ is the global average;
    \item $ \alpha_i $ is additional impact of group $ i $ ($ \alpha_i>0 $ higher than global average);
    \item $ \varepsilon_{ij} $ is the error of group $ i $ on level $ j $.
\end{itemize}
In matrix-vector form, we may write the model as follows.
\[ \begin{pmatrix}
        Y_{11}   \\
        \vdots   \\
        Y_{1n_1} \\
        \vdots   \\
        Y_{a1}   \\
        \vdots   \\
        Y_{an_a}
    \end{pmatrix}=\Matrix{X}\begin{pmatrix}
        \mu      \\
        \alpha_1 \\
        \vdots   \\
        \alpha_a
    \end{pmatrix}+\Vector{\varepsilon}, \]
where
\[ \Matrix{X}=\begin{pmatrix}
        n_1\begin{Bmatrix}
               1      & 1      & 0      & \cdots & 0      \\
               \vdots & \vdots & \vdots & \ddots & \vdots \\
               1      & 1      & 0      & \cdots & 0      \\
           \end{Bmatrix} \\
        n_2\begin{Bmatrix}
               1      & 1      & 0      & \cdots & 0      \\
               \vdots & \vdots & \vdots & \ddots & \vdots \\
               1      & 1      & 0      & \cdots & 0      \\
           \end{Bmatrix} \\
        \vdots                                        \\
        n_a\begin{Bmatrix}
               1      & 0      & 0      & \cdots & 1      \\
               \vdots & \vdots & \vdots & \ddots & \vdots \\
               1      & 0      & 0      & \cdots & 1      \\
           \end{Bmatrix}
    \end{pmatrix}\implies
    \Matrix{X}'=\begin{pmatrix}
        1      & \cdots & 1      & 1      & \cdots & 1      & \cdots & 1      & \cdots & 1      \\
        1      & \cdots & 1      & 0      & \cdots & 0      & \cdots & 0      & \cdots & 0      \\
        0      & \cdots & 0      & 1      & \cdots & 1      & \cdots & 0      & \cdots & 0      \\
        \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0      & \cdots & 0      & 0      & \cdots & 0      & \cdots & 1      & \cdots & 1
    \end{pmatrix} \]
and $ \rank{\Matrix{X}}=a $. Note that
\[ \Matrix{X}'\Matrix{X}=\begin{pmatrix}
        n      & n_1    & n_2    & \cdots & n_a    \\
        n_1    & n_1    & 0      & \cdots & 0      \\
        n_2    & 0      & n_2    & \cdots & 0      \\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        n_a    & 0      & \cdots & 0      & n_a
    \end{pmatrix}\in\R^{(a+1)\times (a+1)} \]
A $ g $-inverse of $ \Matrix{X}'\Matrix{X} $ is
\[ \Matrix{F}=\begin{pmatrix}
        0      & \cdots & 0      & 0      \\
        \vdots & 1/n_1  & \ddots & \vdots \\
        0      & \ddots & \ddots & 0      \\
        0      & \cdots & 0      & 1/n_a
    \end{pmatrix} \]
\[ \Matrix{X}'\Vector{Y}=\begin{pmatrix}
        \sum_{i=1}^{a}\sum_{j=1}^{n_i}Y_{ij} \\
        \sum_{j=1}^{n_1}Y_{1j}               \\
        \vdots                               \\
        \sum_{j=1}^{n_a}Y_{aj}
    \end{pmatrix}=\begin{pmatrix}
        Y_{..} \\
        Y_{1.} \\
        \vdots \\
        Y_{a.}
    \end{pmatrix}, \]
where $ Y_{..}=\sum_{i=1}^{n}\sum_{j=1}^{n_i}Y_{ij} $
and $ Y_{i.}=\sum_{j=1}^{n_i}=Y_{ij} $. Also,
\begin{align*}
    \Matrix{F}\Matrix{X}'
     & =\begin{pmatrix}
            0      & \cdots & 0      & 0      \\
            \vdots & 1/n_1  & \ddots & \vdots \\
            0      & \ddots & \ddots & 0      \\
            0      & \cdots & 0      & 1/n_a
        \end{pmatrix}\begin{pmatrix}
                         1      & \cdots & 1      & 1      & \cdots & 1      & \cdots & 1      & \cdots & 1      \\
                         1      & \cdots & 1      & 0      & \cdots & 0      & \cdots & 0      & \cdots & 0      \\
                         0      & \cdots & 0      & 1      & \cdots & 1      & \cdots & 0      & \cdots & 0      \\
                         \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
                         0      & \cdots & 0      & 0      & \cdots & 0      & \cdots & 1      & \cdots & 1
                     \end{pmatrix} \\
     & =\begin{pmatrix}
            0      & \cdots & 0      & 0      & \cdots & 0      & \cdots & 0      & \cdots & 0      \\
            1/n_1  & \cdots & 1/n_1  & 0      & \cdots & 0      & \cdots & 0      & \cdots & 0      \\
            0      & \cdots & 0      & 1/n_2  & \cdots & 1/n_2  & \cdots & 0      & \cdots & 0      \\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
            0      & \cdots & 0      & 0      & \cdots & 0      & \cdots & 1/n_a  & \cdots & 1/n_a
        \end{pmatrix}
\end{align*}
Therefore, a solution to the normal equation
\[ \Matrix{X}'\Matrix{X}\Vector{\beta}=\Matrix{X}'\Vector{Y} \]
is
\[ \Vector{\beta}_0=\Matrix{F}\Matrix{X}'\Vector{Y}=\begin{pmatrix}
        0                   \\
        \frac{1}{n_1}Y_{1.} \\
        \vdots              \\
        \frac{1}{n_a}Y_{a.}
    \end{pmatrix}=\begin{pmatrix}
        0            \\
        \bar{Y}_{1.} \\
        \vdots       \\
        \bar{Y}_{a.}
    \end{pmatrix}, \]
where $ \bar{Y}_{i.}=\frac{1}{n_i}Y_{i.} $.
\begin{itemize}
    \item Sum of Squares Total:
          \begin{align*}
              \SST
               & =\sum_{i=1}^{a}\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y}_{..})^2      \\
               & =\sum_{i=1}^{a}\sum_{j=1}^{n_i}Y_{ij}^2-\frac{Y_{..}^2}{n}.
          \end{align*}
    \item Sum of Squares Regression: Define $ \hat{\Vector{Y}}=\Matrix{X}\Vector{\beta}_0=
              \Matrix{X}\Matrix{F}\Matrix{X}'\Vector{Y} $. Hence,
          \begin{align*}
              \SSR
               & =\sum_{i=1}^{a}\sum_{j=1}^{n_i}(\hat{Y}_{ij}-\bar{Y}_{..})^2                                                              \\
               & =\hat{\Vector{Y}}'\hat{\Vector{Y}}-2\biggl(\sum_{i=1}^{a}\sum_{j=1}^{n_i}\hat{Y}_{ij}\biggr)\bar{Y}_{..}+n\bar{Y}_{..}^2.
          \end{align*}
          Furthermore,
          \begin{align*}
              \sum_{i=1}^{a}\sum_{j=1}^{n_i}\hat{Y}_{ij}
               & =\Vector{j}'\hat{\Vector{Y}}                             \\
               & =\Vector{j}'\Matrix{X}\Vector{\beta}_0                   \\
               & =\begin{pmatrix}
                      n & n_1 & \cdots & n_a
                  \end{pmatrix}\Vector{\beta}_0                           \\
               & =n_1 \bar{Y}_{1.}+n_2\bar{Y}_{2.}+\cdots+n_a\bar{Y}_{a.} \\
               & =Y_{..}                                                  \\
               & =n\bar{Y}_{..}
          \end{align*}
          Furthermore, $ \SSR=\hat{\Vector{Y}}'\hat{\Vector{Y}}-n\bar{Y}_{..}^2 $, and
          \[ \hat{\Vector{Y}}'=\Matrix{X}\Vector{\beta}_0=
              \Matrix{X}\begin{pmatrix}
                  0            \\
                  \bar{Y}_{1.} \\
                  \vdots       \\
                  \bar{Y}_{a.}
              \end{pmatrix}=\begin{pmatrix}
                  n_1\begin{Bmatrix}
                         \bar{Y}_{1.} \\
                         \vdots       \\
                         \bar{Y}_{1.} \\
                     \end{Bmatrix} \\
                  \vdots            \\
                  n_a\begin{Bmatrix}
                         \bar{Y}_{a.} \\
                         \vdots       \\
                         \bar{Y_{a.}}
                     \end{Bmatrix}
              \end{pmatrix} \]
          Therefore,
          \begin{align*}
              \SSR
               & =n_1\bar{Y}_{1.}^2+\cdots+n_a\bar{Y}_{a.}^2-n\bar{Y}_{..}^2   \\
               & =\sum_{i=1}^{a}\frac{Y_{i.}^2}{n_i}-\frac{\bar{Y}_{..}^2}{n}.
          \end{align*}
    \item Sum of Squares Error:
          \begin{align*}
              \SSE
               & =\SST-\SSR                                                                \\
               & =\sum_{i=1}^{a}\sum_{j=1}^{n_i}Y_{ij}^2-\sum_{i=1}^{a}\frac{Y_{i.}}{n_i}.
          \end{align*}
\end{itemize}
By Theorem 13.1 and Theorem 13.2, we have the following distributions.
\begin{itemize}
    \item $ \SSE/\sigma^2 \sim \chi^2(n-a) $.
    \item $ \SSR/\sigma^2 \sim \chi^2(a-1,\lambda) $, where
          \[ \lambda=\frac{1}{2\sigma^2}(\Matrix{X}\Vector{\beta})(\Matrix{X}\Matrix{F}\Matrix{X}'-\tfrac{1}{n}\Matrix{J})\Matrix{X}\Vector{\beta}. \]
\end{itemize}