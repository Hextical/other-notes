\makeheading{Lecture 5}{\printdate{2023-01-23}}%chktex 8
\section{Lecture 5: Criteria for Independence}
\begin{Lemma}{}{}
    Let $ \Matrix{A} $ be a symmetric positive semidefinite
    $ n\times n $ matrix with rank $ r $. Then, there exists an $ n\times r $
    matrix $ \Matrix{D} $ with rank $ r $ such that
    \[ \Matrix{A}=\Matrix{DD}' \]
    \tcblower{}
    \textbf{Proof}: If $ \Matrix{A} $ is symmetric, then by the spectral theorem
    \[ \Matrix{A}=\Matrix{Q}'\diag{\lambda_1,\ldots,\lambda_n}\Matrix{Q}, \]
    where $ \Matrix{Q} $ is orthogonal where $ r $ of $ \lambda_i $ are non-zero since
    $ \rank{\Matrix{A}}=r $. Without loss of generality, we can assume that
    $ \lambda_1,\ldots,\lambda_r>0 $, $ \lambda_j=0 $ for $ j>r $.
    Define
    \[ \Matrix{D}=\Matrix{Q}'\begin{bmatrix}
            \Matrix{\Lambda}^{1/2} \\
            \Matrix{O}
        \end{bmatrix}_{n\times r}, \]
    where
    \[ \Matrix{\Lambda}^{1/2}=\diag{\sqrt{\lambda_1},\ldots,\sqrt{\lambda_r}}. \]
    Hence,
    \[ \Matrix{D}\Matrix{D}'=\Matrix{Q}'\begin{bmatrix}
            \Matrix{\Lambda}^{1/2} \\
            \Matrix{O}
        \end{bmatrix}\begin{bmatrix}
            \Matrix{\Lambda}^{1/2} & \Matrix{O}
        \end{bmatrix}\Matrix{Q}=
        \Matrix{Q}'\begin{bmatrix}
            \Matrix{\Lambda} & \Matrix{O} \\
            \Matrix{O}       & \Matrix{O}
        \end{bmatrix}\Matrix{Q}=\Matrix{A}. \]
\end{Lemma}
\begin{Theorem}{}{}
    Let $ \Vector{X}\sim \MN{\Vector{\mu},\Matrix{\Sigma}} $,
    $ \Matrix{A}\in\R^{n\times n} $ be symmetric and $ \Matrix{B}\in\R^{k\times n} $.
    $ \Vector{X}'\Matrix{A}\Vector{X} $ and $ \Matrix{B}\Vector{X} $
    are independent if and only if $ \Matrix{B\Sigma A}=\Matrix{O} $.
    \tcblower{}
    \textbf{Proof}: We assume that $ \Matrix{A} $ is positive semidefinite.

    \underline{Step 1}: Let $ r=\rank{\Matrix{A}} $, $ \Matrix{A}=\Matrix{DD}' $
    from Lemma 5.1. We know that $ \Matrix{D} $ is $ n\times r $ with
    $ \rank{\Matrix{D}}=r $, and since
    $ \rank{\Matrix{DD}'}=\rank{\Matrix{D}}=r $, then $ \Matrix{D}'\Matrix{D} $
    is invertible. We will show that
    \[ \Matrix{B\Sigma A}=\Matrix{O}\iff \Matrix{B\Sigma D}=\Matrix{O}. \]
    $ (\implies) $ Note that
    \begin{align*}
        \Matrix{B\Sigma A}=\Matrix{B\Sigma DD}'                             & =\Matrix{O}                     \\
        \implies \Matrix{B\Sigma DD'}\Matrix{D}                             & =\Matrix{O}                     \\
        \implies \Matrix{B\Sigma DD'}\Matrix{D}(\Matrix{D}'\Matrix{D})^{-1} & =\Matrix{B\Sigma D}=\Matrix{O}.
    \end{align*}
    On the other hand, if $ \Matrix{B\Sigma D}=\Matrix{O} $, then
    \[ \Matrix{B\Sigma DD}'=\Matrix{B\Sigma A}=\Matrix{O}. \]
    \underline{Step 2} (Sufficiency): Assume that $ \Matrix{B\Sigma A}=\Matrix{O} $,
    then $ \Matrix{B\Sigma D}=\Matrix{O} $. By direct calculation,
    \[ \Cov{\Matrix{B}\Vector{X},\Vector{X}'\Matrix{D}}=\Matrix{B}\Cov{\Vector{X},\Vector{X}}\Matrix{D}=\Matrix{B\Sigma D}=\Matrix{O}. \]
    Since $ \Matrix{B}\Vector{X} $ and $ \Vector{X}'\Matrix{D} $ are multivariate normal,
    it follows that $ \Matrix{B}\Vector{X} $ and $ \Vector{X}'\Matrix{D} $ are independent.
    Noting that
    \begin{align*}
        \Vector{X}'\Matrix{A}\Vector{X}
         & =\Vector{X}'\Matrix{DD}'\Vector{X}                &  & \text{Lemma 5.1} \\
         & =(\Vector{X}'\Matrix{D})(\Vector{X}'\Matrix{D})',
    \end{align*}
    which is a function of $ \Vector{X}'\Matrix{D} $. We know that
    if $ X $ and $ Y $ are independent, then for any measurable function
    $ f(X) $ and $ g(Y) $ are independent. Hence,
    $ \Matrix{B}\Vector{X} $ and $ \Vector{X}'\Matrix{A}\Vector{X} $ are independent.

    \underline{Step 3} (Necessity): Assume that $ \Matrix{B}\Vector{X} $
    and $ \Vector{X}'\Matrix{A}\Vector{X} $ are independent. By direct calculation,
    \begin{align*}
        \Cov{\Matrix{B}\Vector{X},\Vector{X}'\Matrix{A}\Vector{X}}
         & =\Matrix{B}\Cov{\Vector{X},\Vector{X}' \Matrix{A}\Vector{X}}                                                                                                              \\
         & =\Matrix{B}\E[\big]{(\Vector{X}-\Vector{\mu})(\Vector{X}'\Matrix{A}\Vector{X}-\E{\Vector{X}'\Matrix{A}\Vector{X}})}                                                       \\
         & =\Matrix{B}\E[\big]{(\Vector{X}-\Vector{\mu})(\Vector{X}'\Matrix{A}\Vector{X}-\Vector{\mu}'\Matrix{A}\Vector{\mu}-\tr{\Matrix{A\Sigma}})}                                 \\
         & =\Matrix{B}\E[\big]{(\Vector{X}-\Vector{\mu})(\Vector{X}'\Matrix{A}\Vector{X}-\Vector{\mu}'\Matrix{A}\Vector{\mu})}
        +\Matrix{B}\underbrace{\E[\big]{(\Vector{X}-\Vector{\mu})}}_{0}\tr{\Matrix{A\Sigma}}                                                                                         \\
         & =\Matrix{B}\E[\big]{(\Vector{X}-\Vector{\mu})(\Vector{X}'\Matrix{A}\Vector{X}-\Vector{\mu}\Matrix{A}\Vector{\mu})}                                                        \\
         & =\Matrix{B}\E[\Big]{(\Vector{X}-\Vector{\mu})\bigl[(\Vector{X}-\Vector{\mu})'\Matrix{A}(\Vector{X}-\Vector{\mu})+2(\Vector{X}-\Vector{\mu})'\Matrix{A}\Vector{\mu}\bigr]} \\
         & =\Matrix{B}\E[\Big]{(\Vector{X}-\Vector{\mu})(\Vector{X}-\Vector{\mu})'\Matrix{A}(\Vector{X}-\Vector{\mu})}
        +2 \Matrix{B}\E[\Big]{(\Vector{X}-\Vector{\mu})'\Matrix{A}\Vector{\mu}}                                                                                                      \\
         & =\Matrix{B}\E[\Big]{(\Vector{X}-\Vector{\mu})(\Vector{X}-\Vector{\mu})'\Matrix{A}(\Vector{X}-\Vector{\mu})}
        +2 \Matrix{B\Sigma A}\Vector{\mu}.
    \end{align*}
    To show that the first term is zero, using the spectral theorem re-write $ \Matrix{A} $,
    define $ \Vector{Y}=\Vector{X}-\Vector{\mu} $, and use the fact that the third moments
    of multivariate normal are $ 0 $ (exercise).
    Hence,
    \[ \Matrix{B\Sigma A}\Vector{\mu}=\Matrix{O}. \]
    Since $ \Vector{\mu} $ is arbitrary, it follows that
    \[ \Matrix{B\Sigma A}=\Matrix{O}. \]
\end{Theorem}
\begin{Theorem}{}{}
    Let $ \Vector{X}\sim \MN{\Vector{\mu},\Matrix{\Sigma}} $,
    $ \Matrix{A},\Matrix{B}\in\R^{n\times n} $ be symmetric matrices.
    $ \Vector{X}'\Matrix{A}\Vector{X} $ and $ \Vector{X}'\Matrix{B}\Vector{X} $
    are independent if and only if $ \Matrix{A\Sigma B}=\Matrix{O} $.
    \tcblower{}
    \textbf{Proof}: Let $ \rank{\Matrix{A}}=r $, $ \rank{\Matrix{B}}=s $. By
    the spectral theorem, there are orthogonal matrices $ \Matrix{Q}_1 $ and $ \Matrix{Q}_2 $
    such that
    \begin{align*}
        \Matrix{A} & =\Matrix{Q}_1'\diag{\lambda_1,\ldots,\lambda_n}\Matrix{Q}_1,                 \\
        \Matrix{B} & =\Matrix{Q}_2'\diag{\tilde{\lambda}_1,\ldots,\tilde{\lambda}_n}\Matrix{Q}_2.
    \end{align*}
    Without loss of generality, we assume that
    \begin{itemize}
        \item $ \lambda_1,\ldots,\lambda_r\ne 0 $, $ \lambda_j=0 $ for $ j>r $,
        \item $ \tilde{\lambda}_1,\ldots,\tilde{\lambda}_s\ne 0 $, $ \tilde{\lambda}_i=0 $ for $ i>s $.
    \end{itemize}
    Set
    \[ \Matrix{D}_r=\diag{\lambda_1,\ldots,\lambda_r},\qquad \tilde{\Matrix{D}}_s=\diag{\tilde{\lambda}_1,\ldots,\tilde{\lambda}_s}. \]
    Hence, $ \Matrix{Q}_1'=\begin{pmatrix}
            \Matrix{Q}_{11}' &
            \Matrix{Q}_{12}'
        \end{pmatrix} $ with $ \Matrix{Q}_{11}' $ being $ n\times r $
    and $ \rank{\Matrix{Q}_{11}'}=r $. Then,
    \begin{align*}
        \Matrix{A}
         & =\begin{pmatrix}
                \Matrix{Q}_{11}' & \Matrix{Q}_{12}'
            \end{pmatrix}\begin{pmatrix}
                             \Matrix{D}_r & \Matrix{O} \\
                             \Matrix{O}   & \Matrix{O}
                         \end{pmatrix}\begin{pmatrix}
                                          \Matrix{Q}_{11} \\
                                          \Matrix{Q}_{12}
                                      \end{pmatrix}     \\
         & =\Matrix{Q}_{11}'\Matrix{D}_r \Matrix{Q}_{12}.
    \end{align*}
    Define $ \Matrix{Q}_2'=\begin{pmatrix}
            \tilde{\Matrix{Q}}_{11}' & \tilde{\Matrix{Q}}_{12}'
        \end{pmatrix} $ to similarly get
    \[ \Matrix{B}=\tilde{\Matrix{Q}}_{11}'\tilde{\Matrix{D}}_s \tilde{\Matrix{Q}}_{12}. \]
    $ (\impliedby) $ ``Sufficiency:'' Assume that $ \Matrix{A\Sigma B}=\Matrix{O} $, so
    \begin{align*}
        \Matrix{A\Sigma B}=\Matrix{Q}_{11}'\Matrix{D}_r \Matrix{Q}_{11} \Matrix{\Sigma}\tilde{\Matrix{Q}}_{11}'\tilde{\Matrix{D}}_s\tilde{\Matrix{Q}}_{11}                      & =\Matrix{O} \\
        \implies \Matrix{Q}_{11}\Matrix{Q}_{11}'\Matrix{Q}_{11}'\Matrix{D}_r \Matrix{Q}_{11} \Matrix{\Sigma}\tilde{\Matrix{Q}}_{11}'\tilde{\Matrix{D}}_s\tilde{\Matrix{Q}}_{11} & =\Matrix{O} \\
        \implies \Matrix{D}_r \Matrix{Q}_{11}\Matrix{\Sigma}\tilde{\Matrix{Q}}_{11}'\tilde{\Matrix{D}}_s                                                                        & =\Matrix{O} \\
        \implies \Matrix{Q}_{11}\Matrix{\Sigma}\tilde{\Matrix{Q}}_{11}'                                                                                                         & =\Matrix{O}
    \end{align*}
    Noting that
    \[ \Cov{\Matrix{Q}_{11}\Vector{X},\Vector{X}'\tilde{\Matrix{Q}}_{11}'}=\Matrix{Q}_{11}\Matrix{\Sigma}\tilde{\Matrix{Q}}_{11}'=\Matrix{O}. \]
    Therefore, $ \Matrix{Q}_{11}\Vector{X} $ and $ \Vector{X}'\tilde{\Matrix{Q}}_{11}' $ are independent. Hence,
    \begin{align*}
        \Vector{X}'\Matrix{A}\Vector{X}
         & =\Vector{X}'\Matrix{Q}_{11}\Matrix{D}_r \Matrix{Q}_{11}'\Vector{X}      \\
         & =(\Matrix{Q}_{11}'\Vector{X})' \Matrix{D}_r \Matrix{Q}_{11}' \Vector{X}
    \end{align*}
    is a function of $ \Matrix{Q}_{11}'\Vector{X} $, and
    similarly $ \Vector{X}'\Matrix{B}\Vector{X} $ is a function of
    $ \Vector{X}'\Matrix{Q}_{11}' $. Therefore,
    $ \Vector{X}'\Matrix{A}\Vector{X} $ is independent of $ \Vector{X}'\Matrix{B}\Vector{X} $.

    $ (\implies) $ ``Necessity:'' Assume that
    $ \Vector{X}'\Matrix{A}\Vector{X} $ and
    $ \Vector{X}'\Matrix{B}\Vector{X} $ are independent.
    By Theorem 3.1, we have
    \begin{align*}
        \Var{\Vector{X}'\Matrix{A}\Vector{X}}
         & =2\tr{(\Matrix{A\Sigma})^2}+4 \Vector{\mu}'\Matrix{A\Sigma A}\Vector{\mu}. \\
        \Var{\Vector{X}'\Matrix{B}\Vector{X}}
         & =2\tr{(\Matrix{B\Sigma})^2}+4 \Vector{\mu}'\Matrix{B\Sigma B}\Vector{\mu}.
    \end{align*}
    Since $ (\Matrix{A}+\Matrix{B}) $ is symmetric,
    \begin{align*}
        \Var{\Vector{X}'(\Matrix{A}+\Matrix{B})\Vector{X}}
         & =\Var{\Vector{X}'\Matrix{A}\Vector{X}+\Vector{X}'\Matrix{B}\Vector{X}}                                  \\
         & =\Var{\Vector{X}'\Matrix{A}\Vector{X}}+\Var{\Vector{X}'\Matrix{B}\Vector{X}} &  & \text{by assumption}.
    \end{align*}
    Hence,
    \begin{align*}
        2\tr{((\Matrix{A}+\Matrix{B})\Matrix{\Sigma})^2}+4 \Vector{\mu}'(\Matrix{A}+\Matrix{B})\Matrix{\Sigma}(\Matrix{A}+\Matrix{B})\Vector{\mu}
         & =2\tr{(\Matrix{A\Sigma})^2+(\Matrix{B\Sigma})^2}+4 \Vector{\mu}'(\Matrix{A\Sigma A}+\Matrix{B\Sigma B})\Vector{\mu}.
    \end{align*}
    Therefore,
    \begin{align*}
        2\tr{(\Matrix{A\Sigma B\Sigma})+\tr{\Matrix{B\Sigma A\Sigma}}}
        +4 \Vector{\mu}'(\Matrix{A\Sigma B}+\Matrix{B\Sigma A})\Vector{\mu}=0.
    \end{align*}
    By cyclic property of trace, we obtain
    \[ \tr{\Matrix{A\Sigma B\Sigma}}=\tr{\Matrix{\Sigma B\Sigma A}}=\tr{\Matrix{B\Sigma A\Sigma}}. \]
    On the other hand,
    \[ \Vector{\mu}'\Matrix{A\Sigma B}\Vector{\mu}+4 \Vector{\mu}'\Matrix{A\Sigma B}\Vector{\mu}=0. \]
    Choose $ \Vector{\mu}=\Vector{0} $, we get
    \[ \tr{\Matrix{A\Sigma B\Sigma}}=0. \]
    Thus, $ \Vector{\mu}'\Matrix{A\Sigma B}\Vector{\mu}=0 $ for all $ \Vector{\mu} $, which implies
    that
    \[ \Matrix{A\Sigma B}=\Matrix{O}. \]
\end{Theorem}
\begin{Example}{}{}
    Let $ \Vector{X}'=(X_1,X_2)' \sim \MN{\Vector{\mu},\Matrix{I}_2} $. Show
    \[ (X_1-X_2)^2\text{ is independent of }(X_1+X_2)^2. \]
    \tcblower{}
    \textbf{Solution}:
    \[ (X_1-X_2)^2=X_1^2-2X_1X_2+X_2^2=X_1^2-X_1X-2-X_2X_1+X_2^2=
        \begin{pmatrix}
            X_1 & X_2
        \end{pmatrix}\begin{pmatrix}
            1  & -1 \\
            -1 & 1
        \end{pmatrix}\begin{pmatrix}
            X_1 \\
            X_2
        \end{pmatrix}. \]
    \[ (X_1+X_2)^2=X_1^2+X_1X_2+X_2X_1+X_2^2=
        \begin{pmatrix}
            X_1 &
            X_2
        \end{pmatrix}
        \begin{pmatrix}
            1 & 1 \\
            1 & 1
        \end{pmatrix}\begin{pmatrix}
            X_1 \\
            X_2
        \end{pmatrix}. \]
    Now,
    \[ \Matrix{A\Sigma B}=\begin{pmatrix}
            1  & -1 \\
            -1 & 1
        \end{pmatrix}\begin{pmatrix}
            1 & 1 \\
            1 & 1
        \end{pmatrix}=\Matrix{O}, \]
    as required. Therefore, $ (X_1-X_2)^2 $ is independent of $ (X_1+X_2)^2 $ by Theorem 5.2.
\end{Example}
\begin{Example}{}{}
    Let $ \Vector{X}'=(X_1,X_2)' \sim \MN{\Vector{\mu},\Matrix{\Sigma}} $. Define
    \[ \Matrix{B}=\begin{pmatrix}
            1 & 2 \\
            2 & 1
        \end{pmatrix}. \]
    Find
    \[ \Matrix{A}=\begin{pmatrix}
            a & b \\
            b & c
        \end{pmatrix} \]
    such that $ \Vector{X}'\Matrix{A}\Vector{X} $ is independent of
    $ \Vector{X}'\Matrix{B}\Vector{X} $, where
    \[ \Matrix{\Sigma}=\begin{pmatrix}
            1    & -1/2 \\
            -1/2 & 1
        \end{pmatrix}. \]
    \tcblower{}
    \textbf{Solution}:
    \[ \Vector{X}'\Matrix{B}\Vector{X}=X_1^2+4X_1X_2+X_2^2=(X_1+X_2)^2+2X_1X_2. \]
    \begin{align*}
        \Matrix{A\Sigma B}
         & =\begin{pmatrix}
                a & b \\
                b & c
            \end{pmatrix} \begin{pmatrix}
                              1    & -1/2 \\
                              -1/2 & 1
                          \end{pmatrix}\begin{pmatrix}
                                           1 & 2 \\
                                           2 & 1
                                       \end{pmatrix} \\
         & =\begin{pmatrix}
                a & b \\
                b & c
            \end{pmatrix}\begin{pmatrix}
                             0   & 1.5 \\
                             1.5 & 0
                         \end{pmatrix}               \\
         & =\begin{pmatrix}
                1.5b & 1.5a \\
                1.5c & 1.5b
            \end{pmatrix}                            \\
         & =1.5 \begin{pmatrix}
                    b & a \\
                    c & b
                \end{pmatrix}                        \\
         & =\Matrix{O}
    \end{align*}
    implies that $ a=b=c=0 $, so $ \Matrix{A}=\Matrix{O} $. Therefore, there
    is no quadratic form.
\end{Example}