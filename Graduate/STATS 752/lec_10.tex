\makeheading{Lecture 10}{\printdate{2023-02-09}}%chktex 8
\section{Lecture 10: Determination of Predictors and Generalized Inverse}
\begin{Theorem}{}{}
    Let $ \Vector{Y}\sim \MN{\Matrix{X}\Vector{\beta},\sigma^2 \Matrix{I}} $,
    $ \Matrix{H}=\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}' $,
    $ \Matrix{H}_1=\Matrix{X}_I(\Matrix{X}_I'\Matrix{X}_I)^{-1}\Matrix{X}_I' $.
    Then,
    \begin{enumerate}[(1)]
        \item $ \Vector{Y}'(\Matrix{I}-\Matrix{H})\Vector{Y}/\sigma^2=
                  \SSE/\sigma^2 \sim \chi^2(n-k-1) $.
        \item $ \Vector{Y}'(\Matrix{H}-\Matrix{H}_1)\Vector{Y}/\sigma^2=\frac{\text{SS}(\Vector{\beta}_{II}\mid \Vector{\beta}_I)}{\sigma^2}
                  \sim \chi^2(k-\ell,\tilde{\lambda}) $, where
              \[ \tilde{\lambda}=\Vector{\beta}_{II}'\Matrix{X}_{II}'(\Matrix{I}-\Matrix{H}_1)\Matrix{X}_{II}\Vector{\beta}_{II}/2\sigma^2. \]
        \item $ \Vector{Y}'(\Matrix{I}-\Matrix{H})\Vector{Y} $ and $ \Vector{Y}'(\Matrix{H}-\Matrix{H}_1)\Vector{Y} $
              are independent.
    \end{enumerate}
    \tcblower{}
    \textbf{Proof}:
    \begin{enumerate}[(1)]
        \item Earlier proof.
        \item \begin{align*}
                  \Vector{Y}'(\Matrix{H}-\Matrix{H}_1)\Vector{Y}/\sigma^2
                   & =\frac{\text{SS}(\text{full}\mid \text{reduced})}{\sigma^2}            \\
                   & =\frac{\text{SS}(\Vector{\beta}_{II}\mid \Vector{\beta}_I)}{\sigma^2}.
              \end{align*}
              By Theorem 9.1, $ \Matrix{H}-\Matrix{H}_1 $ is idempotent, so
              \[ \Vector{Y}'(\Matrix{H}-\Matrix{H}_1)\Vector{Y} \sim \chi^2(r,\tilde{\lambda}) \]
              $ r=\rank{\Matrix{H}-\Matrix{H}_1} $, and
              \[ \tilde{\lambda}=\frac{1}{2\sigma^2}(\Matrix{X}\Vector{\beta})'(\Matrix{H}-\Matrix{H}_1)\Matrix{X}\Vector{\beta}. \]
              By direct calculation, we have
              \begin{align*}
                  r
                   & =\rank{\Matrix{H}-\Matrix{H}_1}                                                                                                                                                    \\
                   & =\tr{\Matrix{H}-\Matrix{H}_1}                                                                                          &  & \text{since $ \Matrix{H}-\Matrix{H}_1 $ is idempotent} \\
                   & =\tr{\Matrix{H}}-\tr{\Matrix{H}_1}                                                                                                                                                 \\
                   & =\tr{\Matrix{X}(\Matrix{X}'\Matrix{X})^{-1}\Matrix{X}'}-\tr{\Matrix{X}_I(\Matrix{X}_I'\Matrix{X}_I)^{-1}\Matrix{X}_I'}                                                             \\
                   & =\tr{\Matrix{I}_{k+1}}-\tr{\Matrix{I}_{\ell+1}}                                                                        &  & \text{cyclic property}                                 \\
                   & =k-\ell.
              \end{align*}
              Noting that $ \Matrix{H}\Matrix{X}=\Matrix{X} $,  $ \Matrix{H}_1 \Matrix{X}_I=\Matrix{X}_I $, and
              \[ \Matrix{X}\Vector{\beta}=\Matrix{X}_I \Vector{\beta}_I+\Matrix{X}_{II}\Vector{\beta}_{II} \]
              it follows that
              \begin{align*}
                  2\sigma^2\tilde{\lambda}
                   & =(\Matrix{X}\Vector{\beta})'(\Matrix{H}-\Matrix{H}_1)\Matrix{X}\Vector{\beta}                                                                                                                 \\
                   & =(\Matrix{X}\Vector{\beta})'\Matrix{H}\Matrix{X}\Vector{\beta}-(\Matrix{X}\Vector{\beta})'\Matrix{H}_1 \Matrix{X}\Vector{\beta}                                                               \\
                   & =(\Matrix{X}\Vector{\beta})'(\Matrix{X}\Vector{\beta})-(\Matrix{X}\Vector{\beta})'(\Matrix{H}_1\Matrix{X}_I \Vector{\beta}_I+\Matrix{H}_1\Matrix{X}_{II}\Vector{\beta}_{II})                  \\
                   & =(\Matrix{X}\Vector{\beta})'\bigl[\Matrix{X}\Vector{\beta}-\Matrix{H}_1 \Matrix{X}_I \Vector{\beta}_I-\Matrix{H}_1 \Matrix{X}_{II}\Vector{\beta}_{II}\bigr]                                   \\
                   & =(\Matrix{X}\Vector{\beta})'\bigl[\Matrix{X}_I \Vector{\beta}_I+\Matrix{X}_{II}\Vector{\beta}_{II}-\Matrix{X}_I \Vector{\beta}_{I}-\Matrix{H}_1 \Matrix{X}_{II}\Vector{\beta}_{II}\bigr]      \\
                   & =(\Matrix{X}\Vector{\beta})'(\Matrix{X}_{II}\Vector{\beta}_{II}-\Matrix{H}_1 \Matrix{X}_{II}\Vector{\beta}_{II})                                                                              \\
                   & =(\Matrix{X}\Vector{\beta})'(\Matrix{I}-\Matrix{H}_1)\Matrix{X}_{II}\Vector{\beta}_{II}                                                                                                       \\
                   & =(\Matrix{X}_I \Vector{\beta}_I+\Matrix{X}_{II}\Vector{\beta}_{II})'(\Matrix{I}-\Matrix{H}_1)\Matrix{X}_{II}\Vector{\beta}_{II}                                                               \\
                   & =(\Matrix{X}_{II}\Vector{\beta}_{II})'(\Matrix{I}-\Matrix{H}_1)\Vector{X}_{II}\Vector{\beta}_{II}+(\Matrix{X}_I \Vector{\beta}_I)(\Matrix{I}-\Matrix{H}_1)\Matrix{X}_{II}\Vector{\beta}_{II}.
              \end{align*}
              It remains to show that $ (\Matrix{X}_I \Vector{\beta}_I)(\Matrix{I}-\Matrix{H}_1)\Matrix{X}_{II}\Vector{\beta}_{II}=\Matrix{O} $.
              \begin{align*}
                  (\Matrix{X}_I \Vector{\beta}_I)'(\Matrix{I}-\Matrix{H}_1)\Matrix{X}_{II}\Vector{\beta}_{II}
                   & =(\Matrix{X}_I \Vector{\beta}_I)'\Matrix{X}_{II}\Vector{\beta}_{II}-\underbrace{(\Matrix{X}_I \Vector{\beta}_I)'\Matrix{H}_1}_{\text{see below}} \Matrix{X}_{II}\Vector{\beta}_{II} \\
                   & =(\Matrix{X}_I \Vector{\beta}_I)'\Matrix{X}_{II}\Vector{\beta}_{II}-(\Matrix{X}_I \Vector{\beta}_I)'\Matrix{X}_{II}\Vector{\beta}_{II}                                              \\
                   & =\Matrix{O}
              \end{align*}
              since $ \Set[\Big]{\bigl[(\Matrix{X}_I \Vector{\beta}_I)'\Matrix{H}_1\bigr]'}'=\Set{\Matrix{H}_1 \Matrix{X}_I \Vector{\beta}_I}'=(\Matrix{X}_I \Vector{\beta}_I)' $.
        \item A2Q5\@.
    \end{enumerate}
\end{Theorem}
\subsection*{ANOVA for Model Selection}
$ \HN $: $ \Vector{\beta}_{II}=\Vector{0} $ versus $ \HA $: $ \Vector{\beta}_{II}\ne \Vector{0} $.
\[ \begin{array}{lllll}
        \toprule
        \text{Source}                                            & \text{df} & \text{SS}                                 & \text{MS}                                          & \text{Statistics} \\
        \midrule
        \text{Due to }\Vector{\beta}                             & k         & \text{SSR}(\text{full})                   & \text{SSR}(\text{full})/k                          &                   \\
        \text{Due to }\Vector{\beta}_{I}                         & \ell      & \text{SSR}(\text{reduced})                & \text{SSR}(\text{reduced})/\ell                    & F                 \\
        \text{Due to }\Vector{\beta}_{II}\mid \Vector{\beta}_{I} & k-\ell    & \text{SSR}(\text{full}\mid\text{reduced}) & \text{SSR}(\text{full}\mid\text{reduced})/(k-\ell) &                   \\
        \text{Error}                                             & n-(k+1)   & \SSE                                      & \MSE                                               &                   \\
        \text{Total}                                             & n-1       & \SST                                                                                                               \\
        \bottomrule
    \end{array} \]
where
\[ F=\frac{\text{SSR}(\text{full}\mid\text{reduced})/(k-\ell)}{\SSE/(n-k-1)} \]
If we reject $ \HN $, then the full model is better than the reduced model.
Reject $ \HN $ when $ F>F_{\alpha}(k-\ell,n-k-1) $.
\subsection*{Regression for Models without Full Rank}
\begin{Definition}{Generalized Inverse}{}
    Let $ \Matrix{A}\in\R^{n\times k} $. The
    \textbf{generalized inverse} ($ g $-inverse) of $ \Matrix{A} $
    is \emph{any} $ \Matrix{G}\in\R^{k\times n} $ satisfying
    \[ \Matrix{AGA}=\Matrix{A}. \]
    We say $ \Matrix{G} $ is a $ g $-inverse of $ \Matrix{A} $.
\end{Definition}
\begin{Example}{}{}
    Let
    $ \Matrix{A}=\begin{pmatrix}
            1 \\
            2 \\
            3 \\
            4
        \end{pmatrix} $.
    Find a $ g $-inverse of $\Matrix{A}$.
    \tcblower{}
    \textbf{Solution}:
    \[
        \begin{pmatrix}
            1 \\
            2 \\
            3 \\
            4
        \end{pmatrix}
        \begin{pmatrix}
            a & b & c & d
        \end{pmatrix}
        \begin{pmatrix}
            1 \\
            2 \\
            3 \\
            4
        \end{pmatrix}
        =\begin{pmatrix}
            1 \\
            2 \\
            3 \\
            4
        \end{pmatrix}. \]
    If we pick $ a=1 $, $ b=c=d=0 $, then $ \Matrix{G}=\begin{pmatrix}
            1 & 0 & 0 & 0
        \end{pmatrix} $ is a $ g $-inverse of $ \Matrix{A} $. Also, $ \tilde{\Matrix{G}}=\begin{pmatrix}
            1/2 & 1/4 & 0 & 0
        \end{pmatrix} $ is another $ g $-inverse of $ \Matrix{A} $. Hence, we can see that
    $ g $-inverses are not unique.
\end{Example}
\begin{Remark}{Basic Facts}{}
    \begin{enumerate}[(1)]
        \item If $ \Matrix{A} $ is invertible, then the $ g $-inverse of $ \Matrix{A} $
              is unique and given by $ \Matrix{A}^{-1} $.
              \begin{itemize}
                  \item \textbf{Proof}: Let $ \Matrix{G} $ be any $ g $-inverse of $ \Matrix{A} $, then $ \Matrix{A}\Matrix{G}\Matrix{A}=\Matrix{A} $.
                        \[ \Matrix{A}^{-1}\Matrix{AGA}\Matrix{A}^{-1}=\Matrix{A}^{-1}\Matrix{A}\Matrix{A}^{-1}\implies \Matrix{G}=\Matrix{A}^{-1}. \]
                        Clearly, $ \Matrix{A}\Matrix{A}^{-1}\Matrix{A}=\Matrix{A} $.
              \end{itemize}
        \item If $ \Matrix{G} $ is a $ g $-inverse of $ \Matrix{A} $, then for any $ \Matrix{C}\in\R^{k\times n} $,
              $ \Matrix{G}_1=\Matrix{G}+\Matrix{C}-\Matrix{GACAG} $
              is also a $ g $-inverse of $ \Matrix{A} $.
              \begin{itemize}
                  \item \textbf{Proof}: Note that \begin{align*}
                            \Matrix{A}\Matrix{G}_1 \Matrix{A}
                             & =\Matrix{AGA}+\Matrix{ACA}-\Matrix{AGACAGA} \\
                             & =\Matrix{AGA}+\Matrix{ACA}-\Matrix{ACA}     \\
                             & =\Matrix{AGA}                               \\
                             & =\Matrix{A}.
                        \end{align*}
              \end{itemize}
    \end{enumerate}
\end{Remark}
\begin{Lemma}{}{}
    Every matrix $ \Matrix{A} $ has at least one $ g $-inverse.
    \tcblower{}
    \textbf{Proof}: Let $ \Matrix{A}\in\R^{n\times k} $ with $ \rank{\Matrix{A}}=r<\min{n,k} $. Then,
    \[ \Matrix{A}=\begin{pmatrix}
            \Matrix{A}_{11} & \Matrix{A}_{12} \\
            \Matrix{A}_{21} & \Matrix{A}_{22}
        \end{pmatrix}, \]
    where $ \Matrix{A}_{11}\in\R^{r\times r} $ with $ \rank{\Matrix{A}_{11}}=r $.

    \underline{Claim}:
    \[ \Matrix{G}=\begin{pmatrix}
            \Matrix{A}_{11}^{-1} & \Matrix{O} \\
            \Matrix{O}           & \Matrix{O}
        \end{pmatrix}_{k\times n} \]
    is a $ g $-inverse of $ \Matrix{A} $.
    \begin{align*}
        \Matrix{AGA}
         & =
        \begin{pmatrix}
            \Matrix{A}_{11} & \Matrix{A}_{12} \\
            \Matrix{A}_{21} & \Matrix{A}_{22}
        \end{pmatrix}\begin{pmatrix}
                         \Matrix{A}_{11}^{-1} & \Matrix{O} \\
                         \Matrix{O}           & \Matrix{O}
                     \end{pmatrix}\begin{pmatrix}
                                      \Matrix{A}_{11} & \Matrix{A}_{12} \\
                                      \Matrix{A}_{21} & \Matrix{A}_{22}
                                  \end{pmatrix}              \\
         & =\begin{pmatrix}
                \Matrix{I}_r                   & \Matrix{O} \\
                \Matrix{A}_{21}\Matrix{A}_{11} & \Matrix{O}
            \end{pmatrix}\begin{pmatrix}
                             \Matrix{A}_{11} & \Matrix{A}_{12} \\
                             \Matrix{A}_{21} & \Matrix{A}_{22}
                         \end{pmatrix}                       \\
         & =\begin{pmatrix}
                \Matrix{A}_{11} & \Matrix{A}_{12}                                    \\
                \Matrix{A}_{21} & \Matrix{A}_{21}\Matrix{A}_{11}^{-1}\Matrix{A}_{12}
            \end{pmatrix}.
    \end{align*}
    Since $ \rank{\Matrix{A}}=r=\rank{\Matrix{A}_{11}} $, it follows that $ \Matrix{A}_{21} $ and $ \Matrix{A}_{22} $
    are linear combinations of $\Matrix{A}_{11}$ and $ \Matrix{A}_{12} $. Thus, one can find a $ \Matrix{B}\in\R^{(n-r)\times r} $
    such that
    \[ \begin{pmatrix}
            \Matrix{A}_{21} & \Matrix{A}_{22}
        \end{pmatrix}=\Matrix{B}\begin{pmatrix}
            \Matrix{A}_{11} & \Matrix{A}_{12}
        \end{pmatrix}. \]
    Hence,
    \[ \Matrix{A}_{21}\Matrix{A}_{11}^{-1}\Matrix{A}_{12}=\Matrix{B}\Matrix{A}_{11}\Matrix{A}_{11}^{-1}\Matrix{A}_{12}=\Matrix{B}\Matrix{A}_{12}=\Matrix{A}_{22}. \]
    Therefore, $ \Matrix{AGA}=\Matrix{A} $.
\end{Lemma}