\makeheading{Lecture 14}{\printdate{2022-11-09}}%chktex 8
Suppose $ X $ and $ Y $ are jointly continuous with joint pdf
$ f_{X,Y}\colon\mathbf{R}^2\to\interval[open right]{0}{\infty} $.
Suppose $ U=g_1(X,Y) $, $ V=g_2(X,Y) $. Any region
$ S\subseteq \mathbf{R}^2 $ for which
$ \Prob[\big]{(X,Y)\in S} $ must have $ \operatorname{Area}(S)>0 $,
which fails in the example $ (X,1-X) $ or $ (X,g(X)) $ generally.

Let $ A=\Set[\big]{(x,y)\given f_{X,Y}(x,y)>0} $.
Suppose $ g_1 $ and $ g_2 $ satisfy the property that
$ \forall S\subseteq A $, if $ \operatorname{Area}(S)>0 $, then
\[ \Set[\Big]{\bigl(g_1(x,y),g_2(x,y)\bigr)\given (x,y)\in S} \]
has positive area. If there exists differentiable functions
$ h_1,h_2\colon\mathbf{R}^2\to\mathbf{R} $ such that
\begin{align*}
    h_1\bigl(g_1(x,y),g_2(x,y)\bigr) & =x, \\
    h_2\bigl(g_1(x,y),g_2(x,y)\bigr) & =y,
\end{align*}
then $ U,V $ have joint pdf
\[ f_{U,V}(u,v)=f_{X,Y}\bigl(h_1(u,v),h_2(u,v)\bigr)\mathbf{J} \]
where
\[ \mathbf{J}=\abs*{\frac{\partial(x,y)}{\partial(u,v)}}=
    \begin{vmatrix}
        \pdv{x}{u} & \pdv{x}{v} \\
        \pdv{y}{u} & \pdv{y}{v}
    \end{vmatrix}. \]
\begin{Example}{Distribution of the Product of Beta Variables}{}
    Suppose $ X \sim \BetaDist{2,3} $ and
    $ Y \sim \BetaDist{5,9} $ are independent. Let $ U=XY $ and $ V=X $.
    Hence,
    \[ g_1(x,y)=xy,\quad g_2(x,y)=x, \]
    and
    \[ h_1(u,v)=v,\quad h_2(x,y)=\frac{u}{v}. \]
    Since the range of $ (X,Y) $ is $ [0,1]^2 $, the range of
    $ (U,V) $ is also $ [0,1]^2 $. Furthermore,
    $ 0\le u\le v\le 1 $. By independence,
    \begin{align*}
        f_{X,Y}(x,y)
         & =\frac{\Gamma(5)}{\Gamma(2)\Gamma(3)}x^{2-1}(1-x)^{3-1}
        \frac{\Gamma(14)}{\Gamma(5)\Gamma(9)}y^{5-1}(1-y)^{9-1}               \\
         & =\frac{\Gamma(14)}{\Gamma(2)\Gamma(3)\Gamma(9)}x(1-x)^2y^4(1-y)^8.
    \end{align*}
    For $ 0\le u\le v\le 1 $,
    \[
        f_{U,V}(u,v)
        =\underbrace{\frac{\Gamma(14)}{\Gamma(2)\Gamma(3)\Gamma(9)}}_{C}
        v(1-v)^2 \biggl(\frac{u}{v}\biggr)^{\!4}\biggl(1-\frac{u}{v}\biggr)^{8}\mathbf{J},
    \]
    where
    \[ \mathbf{J}=\begin{vmatrix}
            0   & 1      \\
            1/v & -u/v^2
        \end{vmatrix}=\abs*{-\frac{1}{v}}=\frac{1}{v}. \]
    The marginal pdf of $ U $ is
    \[
        f_U(u)
        =\int_{u}^{1}Cv(1-v)^2 \biggl(\frac{u}{v}\biggr)^{\!4}\biggl(1-\frac{u}{v}\biggr)^{8}\frac{1}{v}\odif{v}.\]
    See textbook [Casella Example 4.3.3] for calculation, not done in class.

    Start a Polya's urn with 2 red, 3 blue, and 9 green:
    \[ X=\lim \frac{\text{red}}{\text{red}+\text{blue}},\quad Y=\lim \frac{\text{red}+\text{blue}}{\text{all}}. \]
    Hence,
    $ XY \sim\BetaDist{2,12} $.
\end{Example}
\begin{Example}{}{}
    Suppose $ X \sim \N{1,4} $ and $ Y \sim \N{2,1} $ are independent.
    \[ f_{X,Y}(x,y)=
        \frac{1}{2\pi\sqrt{4\cdot 1}}\exp*{-\frac{(x-1)^2}{2\cdot 4}}
        \exp*{-\frac{(y-2)^2}{2\cdot 1}}. \]
    Let $ U=X+Y $ and $ V=X-Y $. Are $ U $ and $ V $ independent?
    $ (U,V) $ can have any values in $ \mathbf{R}^2 $.
    \[ g_1(x,y)=xy,\quad g_2(x,y)=x-y, \]
    and
    \[ h_1(u,v)=\frac{u+v}{2},\quad h_2(u,v)=\frac{u-v}{2}. \]
    The Jacobian is
    \[ \mathbf{J}=\begin{vmatrix}
            1/2 & 1/2  \\
            1/2 & -1/2
        \end{vmatrix}=\abs*{-\frac{1}{4}-\frac{1}{4}}=\frac{1}{2}. \]
    Thus,
    \begin{align*}
        f_{U,V}(u,v)
         & =\frac{1}{4\pi}\exp*{-\frac{(\frac{u+v}{2}-1)^2}{8}-
        \frac{(\frac{u-v}{2}-2)^2}{2}}\frac{1}{2}                                                              \\
         & =\frac{1}{8\pi}\exp*{-\frac{(u+v-2)^2}{32}-\frac{(u-v-4)^2}{8}}                                     \\
         & =\frac{1}{8\pi}\exp*{-\frac{(u+v)^2}{32}-\frac{(u-v)^2}{8}+
        \frac{4(u+v)}{32}+\frac{8(u-v)}{8}-\frac{4}{32}-\frac{16}{8}}                                          \\
         & =\frac{1}{8\pi}\exp*{-\frac{(u+v)^2}{32}-\frac{4(u-v)^2}{32}+
        \frac{4(u+v)}{32}+\frac{32(u-v)}{32}-\frac{4}{32}-\frac{64}{32}}                                       \\
         & =\frac{1}{8\pi}\exp*{-\frac{2uv}{32}+\frac{8uv}{32}+u\text{ terms}+v\text{ terms}+\text{constant}}.
    \end{align*}
    We have $ uv $ terms, so $ U $ and $ V $ are not independent.
\end{Example}
\begin{Definition}{Convolution}{}
    If $ X $ and $ Y $ are jointly continuous, $ U=X+Y $, then
    the \textbf{convolution} is defined by
    \[ f_U(u)=\int_{-\infty}^{\infty}f_{X,Y}(t,u-t)\odif{t} \]
\end{Definition}
\begin{Example}{}{}
    Suppose $ X $ and $ Y $ are independent $ \text{Uniform}[0,1] $
    where $ U=X+Y $.
    \tcblower{}
    \begin{align*}
        f_U(u)
         & =\int_{-\infty}^{\infty}f_{X,Y}(t,u-t)\odif{t}   \\
         & =\begin{cases}
                \int_{u-1}^{1}f_{X,Y}(t,u-t)\odif{t} & 0<u<1    \\
                \int_{0}^{u}1\odif{t}                & 1\le u<2
            \end{cases} \\
         & =\begin{cases}
                u   & 0<u<1             \\
                2-u & 1\le u< 2         \\
                0   & \text{otherwise}.
            \end{cases}
    \end{align*}
\end{Example}
\makeheading{Lecture 15}{\printdate{2022-11-11}}%chktex 8
\begin{itemize}
    \item A \textbf{sample} from a probability distribution is a sequence, independent,
          identically distributed (iid) variables with that distribution.
    \item A \textbf{sample with replacement} from a finite population (meaning a finite set $ S $)
          is a sequence of iid random variables chosen from the uniform distribution on $ S $.
    \item A \textbf{sample without replacement} from a finite population $ S $ is a sequence of
          random variables each chosen from the uniform distribution on $ S $, but
          conditioned to all having distinct values.
\end{itemize}
For the rest of this lecture, we assume all samples are iid.
\begin{Definition}{Statistic}{}
    Given a sample $ X_1,X_2,\ldots,X_n $, a \textbf{statistic}
    of the sample is a real- or vector-valued function
    $ T(X_1,X_2,\ldots,X_n) $.
    \tcblower{}
    In our probabilistic model, a statistic is another random variable.
\end{Definition}
\begin{Example}{}{}
    Some examples of statistics include:
    \begin{itemize}
        \item Order Statistics: highest value, $ 2\textsuperscript{nd} $ highest;
        \item Percentiles: $ 90\textsuperscript{th} $ percentile, median, $ 1\textsuperscript{st} $ quantile.
    \end{itemize}
\end{Example}
\begin{Definition}{Sample Mean (Average), Sample Variance, Sample Standard Deviation}{}
    Given a sample $ X_1,\ldots,X_n $, the \textbf{sample mean} or \textbf{average} of the sample is
    \[ \bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i. \]
    The \textbf{sample variance} is
    \[ S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2. \]
    The \textbf{sample standard deviation} is $ S=\sqrt{S^2} $.
\end{Definition}
\begin{Theorem}{}{}
    \[ \sum_{i=1}^{n}(X_i-a)^2=\argmin_{a\in\mathbf{R}}\sum_{i=1}^{n}(X_i-a)^2. \]
    \tcblower{}
    \textbf{Proof}: Fix $ a\in\mathbf{R} $,
    \begin{align*}
        \sum_{i=1}^{n}(X_i-a)^2
         & =\sum_{i=1}^{n}((X_i-\bar{X})+(\bar{X}-a))^2                                                                        \\
         & =\sum_{i=1}^{n}(X_i-\bar{X})^2+2\underbrace{\sum_{i=1}^{n}(X_i-\bar{X})(\bar{X}-a)}_{0}+\sum_{i=1}^{n}(\bar{X}-a)^2 \\
         & =\sum_{i=1}^{n}(X_i-\bar{X})^2+\sum_{i=1}^{n}(\bar{X}-a)^2                                                          \\
         & >\sum_{i=1}^{n}(X_i-\bar{X})^2,
    \end{align*}
    unless $ a=\bar{X} $, in which case they are equal. The middle term is $ 0 $ since
    \begin{align*}
        \sum_{i=1}^{n}(X_i-\bar{X})(\bar{X}-a)
         & =(\bar{X}-a)\sum_{i=1}^{n}(X_i-\bar{X}) \\
         & =(\bar{X}-a)(0)                         \\
         & =0.
    \end{align*}
\end{Theorem}
\begin{Theorem}{}{}
    \[ (n-1)S^2=\sum_{i=1}^{n}X_i^2 - n\bar{X}^2. \]
    \tcblower{}
    \textbf{Proof}: By the previous argument with $ a=0 $,
    \begin{align*}
        \sum_{i=1}^{n}X_i^2
         & =\sum_{i=1}^{n}(X_i-\bar{X})^2+\sum_{i=1}^{n}\bar{X}^2 \\
         & =(n-1)S^2+n\bar{X}^2.
    \end{align*}
\end{Theorem}
\begin{Theorem}{}{}
    Suppose $ X_1,X_2,\ldots,X_n $ is an iid sample from a probability distribution with mean
    $ \mu\in\mathbf{R} $ and variance $ \sigma^2<\infty $. Then,
    \begin{enumerate}[(i)]
        \item $ \E{\bar{X}}=\mu $;
        \item $ \Var{\bar{X}}=\sigma^2/n $;
        \item $ \E{S^2}=\sigma^2 $.
    \end{enumerate}
    \tcblower{}
    \textbf{Proof}:
    \begin{enumerate}[(i)]
        \item Easy: $ \E{\bar{X}}=\E*{\frac{1}{n}\sum_{i=1}^{n}X_i}=\frac{1}{n}n\E{X_1}=\mu $.
        \item Still easy:
              \begin{align*}
                  \Var{\bar{X}}
                   & =\Var*{\frac{1}{n}\sum_{i=1}^{n}X_i}                                            \\
                   & =\frac{1}{n^2}\Var*{\sum_{i=1}^{n}X_i}                                          \\
                   & =\frac{1}{n^2}\sum_{i=1}^{n}\Var*{X_i} &  & \text{$X_i\indep X_j$ for $i\ne j$} \\
                   & =\frac{1}{n^2}n\Var{X_1}               &  & \text{$X_i$ iid}                    \\
                   & =\frac{\sigma^2}{n}.
              \end{align*}
        \item Still easy, but long
              \begin{align*}
                  \E{S^2}
                   & =\E*{\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2}                                                                                                        \\
                   & =\frac{1}{n-1}\sum_{i=1}^{n}\E*{(X_i-\bar{X})^2}                                                                                                        \\
                   & =\frac{n}{n-1}\E{(X_1-\bar{X})^2}                                                                                                                       \\
                   & =\frac{n}{n-1}\E*{\biggl(X_1-\sum_{i=1}^{n}\frac{X_i}{n}\biggr)^2}                                                                                      \\
                   & =\frac{n}{n-1}\Set*{\E{X_1^2}-2\E*{X_1 \sum_{i=1}^{n}\frac{X_i}{n}}+\E{\bar{X}^2}}                                                                      \\
                   & =\frac{n}{n-1}\Set*{(\sigma^2+\mu^2)+\biggl(\frac{\sigma^2}{n}+\mu^2\biggr)-2\biggl(\E*{X_1 \frac{X_1}{n}}+\E*{X_1 \sum_{i=2}^{n}\frac{X_i}{n}}\biggr)} \\
                   & =\frac{n}{n-1}\Set*{(\sigma^2+\mu^2)+\biggl(\frac{\sigma^2}{n}+\mu^2\biggr)-2\biggl(\frac{1}{n}\E{X_1^2}+\frac{1}{n}\E*{X_1 X_2}\biggr)}                \\
                   & =\frac{n}{n-1}\Set*{(\sigma^2+\mu^2)+\biggl(\frac{\sigma^2}{n}+\mu^2\biggr)-2\biggl(\frac{1}{n}(\sigma^2+\mu^2)+\frac{n-1}{n}\E{X_1}\E{X_2}\biggr)}     \\
                   & =\frac{n}{n-1}\Set*{(\sigma^2+\mu^2)+\biggl(\frac{\sigma^2}{n}+\mu^2\biggr)-2\biggl(\frac{1}{n}(\sigma^2+\mu^2)+\frac{n-1}{n}\mu^2\biggr)}              \\
                   & =\frac{n}{n-1}\Set*{\sigma^2+\mu^2+\frac{\sigma^2}{n}+\mu^2-\frac{2}{n}\sigma^2-\frac{2}{n}\mu^2-2\frac{n-1}{n}\mu^2}                                   \\
                   & =\frac{n}{n-1}\Set*{\frac{(n-1)\sigma^2}{n}}                                                                                                            \\
                   & =\sigma^2.
              \end{align*}
    \end{enumerate}
\end{Theorem}
\begin{Definition}{Exponential Family}{}
    A family of pdfs (continuous) or pmfs (discrete) form an \textbf{exponential family} if it has the form
    \[ f(x\mid \Vector{\theta})=h(x)c(\Vector{\theta})\exp*{\sum_{i=1}^{k}w_i(\Vector{\theta})t_i(x)}, \]
    where $ h(x)\ge 0 $, $ c(\Vector{\theta})\ge 0 $. All real-valued functions $ h $ and $ t_1,\ldots,t_k $
    cannot depend on $ \theta $; $ c $ and $ w_1,\ldots,w_k $ cannot depend on $ x $.
\end{Definition}
\begin{Example}{}{ex_bin}
    For $ n $ fixed, the family of Binomial distributions $ \BIN{n,p} $ for $ 0<p<1 $ form an exponential family.
    \tcblower{}
    \textbf{Solution}: First,
    \[ f(j\mid p)=\binom{n}{j}p^j(1-p)^{n-j}=\binom{n}{j}\biggl(\frac{p}{1-p}\biggr)^{\!j}(1-p)^n, \]
    where we define
    \begin{align*}
        h(j) & =\begin{cases}
                    \binom{n}{j}, & 0\le j\le n,      \\
                    0,            & \text{otherwise},
                \end{cases} \\
        c(p) & =\begin{cases}
                    (1-p)^n, & 0<p<1             \\
                    0,       & \text{otherwise}.
                \end{cases}
    \end{align*}
    We want
    \[ \biggl(\frac{p}{1-p}\biggr)^{\!j}=\exp*{w_1(p)t_1(j)}, \]
    so if we set $ t_1(j)=j $, we get
    \[ \exp*{w_1(p)j}=\biggl(\frac{p}{1-p}\biggr)^{\!j}\implies w_1(p)j=j\ln*{\frac{p}{1-p}}\implies w_1(p)=\ln*{\frac{p}{1-p}}. \]
    Therefore,
    \[ f(j\mid p)=h(j)c(p)\exp*{w_1(p)t_1(j)}=\binom{n}{j}(1-p)^n\exp*{\ln*{\frac{p}{1-p}}j}. \]
\end{Example}
\begin{Example}{}{}
    The normal distribution $ \N{\mu,\sigma^2} $ form an exponential family for $ \mu\in\mathbf{R} $, $ 0<\sigma^2<\infty $.
    \tcblower{}
    \textbf{Solution}: First,
    \begin{align*}
        f(x\mid \mu,\sigma^2)
         & =\frac{1}{\sqrt{2\pi\sigma^2}}\exp*{-\frac{(x-\mu)^2}{2\sigma^2}}                                                  \\
         & =\frac{1}{\sqrt{2\pi\sigma^2}}\exp*{-\frac{x^2}{2\sigma^2}-\frac{\mu^2}{2\sigma^2}+\frac{\mu x}{\sigma^2}}         \\
         & =\frac{1}{\sqrt{2\pi\sigma^2}}\exp*{-\frac{\mu^2}{2\sigma^2}}\exp*{-\frac{x^2}{2\sigma^2}+\frac{\mu x}{\sigma^2}}.
    \end{align*}
    Define the following functions:
    \begin{align*}
        h(x)              & =1,                                                                             \\
        c(\mu,\sigma^2)   & =\frac{1}{\sqrt{2\pi\sigma^2}}\exp*{-\frac{\mu^2}{2\sigma^2}},                  \\
        w_1(\mu,\sigma^2) & =-\frac{1}{2\sigma^2},                                         &  & t_1(x)=x^2, \\
        w_2(\mu,\sigma^2) & =\frac{\mu}{\sigma^2},                                         &  & t_2(x)=x.
    \end{align*}
    We try to fit this representation with as few $ w_i $'s and $ t_i $'s as possible. If the number of terms in the sum $ k $
    (number of $ w_i $'s and $ t_i $'s) equals the number of parameters for the family of distributions, then this is a
    \textbf{full exponential family}.

    If $ k $ is greater than the number of parameters, then this is a \textbf{curved exponential family}.
\end{Example}
\begin{Theorem}{}{}
    If $ X $ is a random variable whose distribution comes from an exponential family,
    \[ f(x\mid \Vector{\theta})=h(x)c(\Vector{\theta})\exp*{\sum_{i=1}^{k}w_i(\Vector{\theta})t_i(x)}, \]
    then for any parameter $ \theta $,
    \begin{enumerate}[(i)]
        \item $ \displaystyle \E*{\sum_{i=1}^{k}\pdv{w_i(\Vector{\theta})}{\theta_j}t_i(X)}=-\pdv*{\ln{c(\Vector{\theta})}}{\theta_j} $;
        \item $ \displaystyle \Var*{\sum_{i=1}^{k}\pdv{w_i(\Vector{\theta})}{\theta_j}t_i(X)}=-\pdv*[order=2]{\ln{c(\Vector{\theta})}}{\theta_j}-\E*{\sum_{i=1}^{k}\pdv[order=2]{w_i(\Vector{\theta})}{\theta_j}t_i(X)} $.
    \end{enumerate}
\end{Theorem}
\begin{Example}{}{}
    Let $ X \sim \BIN{n,p} $. From~\Cref{ex:ex_bin}, we know that
    \begin{align*}
        h(j)   & =\begin{cases}
                      \binom{n}{j}, & 0\le j\le n,      \\
                      0,            & \text{otherwise},
                  \end{cases} \\
        c(p)   & =\begin{cases}
                      (1-p)^n, & 0<p<1             \\
                      0,       & \text{otherwise},
                  \end{cases}      \\
        t_1(j) & =j,                                \\
        w_1(p) & =\ln*{\frac{p}{1-p}}.
    \end{align*}
    To use $ \E*{\sum_{i=1}^{k}\pdv{w_i(p)}{p}t_i(X)}=-\pdv*{\ln{c(p)}}{p} $, we compute
    \begin{align*}
        \pdv{w_i(p)}{p}     & =\pdv*{\ln*{\frac{p}{1-p}}}{p}=\frac{1}{p/(1-p)}\frac{(1-p)\cdot 1-p(-1)}{(1-p)^2}=\frac{1}{p(1-p)}, \\
        -\pdv{\ln{c(p)}}{p} & =-\pdv*{\ln{(1-p)^n}}{p}=-n\pdv*{\ln{1-p}}{p}=-n\frac{1}{1-p}(-1)=\frac{n}{1-p}.
    \end{align*}
    Hence,
    \[ \E*{\frac{1}{p(1-p)}X}=\frac{n}{1-p}\implies \E{X}=np. \]
\end{Example}
\begin{Theorem}{}{}
    Suppose $ X_1,\ldots,X_n $ are iid samples from a distribution that comes from an exponential family,
    \[ f(x\mid \Vector{\theta})=h(x)c(\Vector{\theta})\exp*{\sum_{i=1}^{k}w_i(\Vector{\theta})t_i(x)}. \]
    Define statistics $ T_1,T_2,\ldots,T_k $ by
    \[ T_i(X_1,\ldots,X_n)=\sum_{j=1}^{n}t_i(X_j),\; 1\le i\le k. \]
    If the set
    \[ \Set[\big]{(w_1(\Vector{\theta}),w_2(\Vector{\theta}),\ldots,w_k(\Vector{\theta}))\given \Vector{\theta}\text{ is an allowed value for the parameter}} \]
    contains an open subset of $ \mathbf{R}^k $ (usually true for full exponential families), then
    the distribution of the vector $ (T_1,\ldots,T_k)=\Vector{T} $ is itself an exponential family of the form
    \[ f_{\Vector{T}}(u_1,\ldots,u_k\mid \Vector{\theta})=H(u_1,\ldots,u_k)c(\Vector{\theta})^n\exp*{\sum_{i=1}^{k}w_i(\Vector{\theta})u_i}. \]
\end{Theorem}