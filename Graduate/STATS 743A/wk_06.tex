\makeheading{Lecture 11}{\printdate{2022-10-26}}%chktex 8
\begin{Definition}{}{}
    The \textbf{joint probability mass function}
    (joint pmf) of a sequence $ X_1,\ldots,X_n $
    of discrete random variables is a function
    $ p\colon\mathbf{R}^n\to[0,1] $
    with
    \[ p(a_1,\ldots,a_n)=\Prob[\big]{\Set{X_1=a_1}\cap\cdots\cap \Set{X_n=a_n}}. \]
\end{Definition}
\begin{Example}{}{}
    Suppose we are rolling two 4-sided die independently. The joint pmf is
    \[ p(a,b)=\begin{cases}
            \frac{1}{16}, & a,b\in\Set{1,2,3,4}, \\
            \text{0},     & \text{otherwise}.
        \end{cases} \]
\end{Example}
\begin{Example}{}{}
    Suppose we roll a die and flip a coin. Let $ X $ be a die roll and
    \[ Y=\begin{cases}
            X,   & \text{if H}, \\
            5-X, & \text{if T}.
        \end{cases} \]
    \[ \begin{array}{cc|c|c|c|c}
            \multicolumn{2}{c}{} & \multicolumn{4}{c}{a}                         \\
                                 &                       & 1   & 2   & 3   & 4   \\
            \cline{2-6}
            \multirow{4}{*}{$b$} & 1                     & 1/8 & 0   & 0   & 1/8 \\
            \cline{2-6}
                                 & 2                     & 0   & 1/8 & 1/8 & 0   \\
            \cline{2-6}
                                 & 3                     & 0   & 1/8 & 1/8 & 0   \\
            \cline{2-6}
                                 & 4                     & 1/8 & 0   & 0   & 1/8
        \end{array} \]
    Note that
    \[ \Prob{\Set{Y=3}}=\frac{1}{8}+\frac{1}{8}=\frac{1}{4}. \]
\end{Example}
\begin{Remark}{}{}
    If $ p $ is the joint pmf of $ (X,Y) $, then
    \begin{align*}
        \Prob{\Set{X=k}} & =\sum_{j}\underbrace{p(k,j)}_{\Prob{\Set{X=k,Y=j}}}. \\
        \Prob{\Set{Y=k}} & =\sum_{j}p(j,k).
    \end{align*}
    In this context of starting with a joint distribution,
    distribution of components are called ``marginal distributions.''
\end{Remark}
\begin{Definition}{}{}
    If $ p $ is the joint pmf of
    $ X_1,\ldots,X_n $, then the marginal distribution
    of $ X_k $ for any $ k\in\Set{1,2,\ldots,n} $ is
    \[ \Prob{\Set{X_k=a}}=\sum_{b_1,\ldots,b_{k-1},b_{k+1},\ldots,b_n}
        p(b_1,\ldots,b_{k-1},a,b_{k+1},\ldots,b_n). \]
\end{Definition}
\begin{Theorem}{}{}
    $ X_1,\ldots,X_n $ (discrete) are jointly independent
    if and only if their joint pmf is the product of their individual pmfs;
    that is,
    \[ p_{X_1,\ldots,X_n}(b_1,\ldots,b_n)=
        p_{X_1}(b_1)\cdots p_{X_n}(b_n). \]
\end{Theorem}
\begin{Example}{}{}
    Let $ X $ and $ Y $ be independent with pmfs
    \begin{align*}
        p_X(-1) & =\frac{1}{2}, \\
        p_X(0)  & =\frac{1}{4}, \\
        p_X(1)  & =\frac{1}{4}, \\
        p_Y(0)  & =\frac{1}{3}, \\
        p_Y(1)  & =\frac{2}{3}.
    \end{align*}
    They have joint pmf
    \[ \begin{array}{cc|c|c|c}
            \multicolumn{2}{c}{} & \multicolumn{3}{c}{X}                     \\
                                 &                       & -1  & 0    & 1    \\
            \cline{2-5}
            \multirow{2}{*}{$Y$} & 0                     & 1/6 & 1/12 & 1/12 \\
            \cline{2-5}
                                 & 1                     & 1/3 & 1/6  & 1/6
        \end{array} \]
\end{Example}
\begin{Definition}{}{}
    If $ X_1,\ldots,X_n $ are continuous random variables and
    $ f\colon\mathbf{R}^n\to\interval[open right]{0}{\infty} $ ($ A\subseteq\mathbf{R}^n $)
    that satisfies
    \[ \int\cdots\int\limits_{A}f(x_1,\ldots,x_n)\odif{x_1}\cdots\odif{x_n} \]
    then $ f $ is a joint pdf for these variables, and they are said to be
    jointly continuous.
\end{Definition}
\begin{Example}{}{}
    Suppose we have two continuous random variables $ X $ and $ Y $.
    \[ \Prob{\Set{(X,Y)\in A}}=\iint\limits_{A}f(x,y)\odif{x}\odif{y}. \]
    If $ A  $ is a rectangle, then $ A=[a,b]\times[c,d] $,
    which implies
    \[ \Prob{\Set{a\le X\le b, c\le Y\le d}}
        =\int_{c}^{d}\int_{a}^{b}f(x,y)\odif{x}\odif{y}. \]
\end{Example}
\begin{Theorem}{}{}
    $ X_1,\ldots,X_n $ (continuous) are jointly independent
    if and only if they are jointly continuous with joint pdf
    \[ f_{X_1,\ldots,X_n}(a_1,\ldots,a_n)=f_{X_1}(a_1)\cdots f_{X_n}(a_n). \]
\end{Theorem}
\begin{Example}{}{}
    \[ f(x,y)=\begin{cases}
            2x^2, & x\in[0,1],\; \abs{y}\le x, \\
            0,    & \text{otherwise}.
        \end{cases} \]
    \underline{Verifying we have a probability density function}:
    \begin{align*}
        \int_{0}^{1}\int_{-x}^{x}2x^2\odif{y}\odif{x}
         & =\int_{0}^{1}\biggl[2x^2y\biggr]_{y=-x}^{y=x}\odif{x} \\
         & =\int_{0}^{1}2x^2(x-(-x))\odif{x}                     \\
         & =\int_{0}^{1}4x^3\odif{x}                             \\
         & =\biggl[x^4\biggr]_{x=0}^{x=1}                        \\
         & =1.
    \end{align*}
    \underline{Calculating Probabilities}:
    To calculate $ \Prob{\Set{Y\ge 1/2}} $,
    we could work out
    the system of inequalities: $ 0\le x\le 1 $, $ -x\le y\le x $, and $ 1/2\le y $ yields
    \[ 1/2\le y\le x\le 1. \]
    Or we can work it out graphically.
    \begin{align*}
        \Prob*{\Set*{Y\ge \frac{1}{2}}}
         & =\int_{1/2}^{1}\int_{1/2}^{x}2x^2\odif{y}\odif{x}        \\
         & =\int_{1/2}^{1}\biggl[2x^2y\biggr]_{y=1/2}^{y=x}\odif{x} \\
         & =\int_{1/2}^{1}(2x^3-x^2)\odif{x}                        \\
         & =\biggl[\frac{x^4}{2}-\frac{x^3}{3}\biggr]_{x=1/2}^{x=1} \\
         & =\frac{1}{2}-\frac{1}{32}-\frac{1}{3}+\frac{1}{24}.
    \end{align*}
\end{Example}
\begin{Definition}{}{}
    The marginal density of $ X $ is
    \[ f_X(t)=\int_{-\infty}^{\infty}f_{X,Y}(t,u)\odif{u}. \]
\end{Definition}
\begin{Definition}{Expectation (Continuous)}{}
    \[ \E[\big]{g(X,Y)}
        =\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)f_{X,Y}(x,y)\odif{x}\odif{y}. \]
    \tcblower{}
    For example, to calculate $ \E{XY} $ we use $ g(x,y)=xy $.
\end{Definition}
\begin{Example}{Polya Urn}{}
    \[ \Prob{\Set{3\textsuperscript{rd}\text{ pick R}}\given \Set{\text{BB}}}
        =\frac{1}{4}. \]
    If $ Y $ is the limiting percentage of blue, then
    \begin{align*}
        \Prob*{\Set*{Y\le \frac{1}{2}}\given \Set{\text{BB}}}
         & =\Prob*{X_1,X_2,Y\le \frac{1}{2}}                                          \\
         & =\int_{0}^{1/2}\int_{0}^{1/2}\int_{0}^{1/2}1\odif{x_1}\odif{x_2}\odif{x_3} \\
         & =\frac{1}{2}\cdot \frac{1}{2}\cdot \frac{1}{2}                             \\
         & =\frac{1}{8}.
    \end{align*}
    \[ \Prob{\Set{Y\le t}}=t^3. \]
    \[ f_Y(t)=\begin{cases}
            3t^2, & t\in[0,1]         \\
            0,    & \text{otherwise},
        \end{cases} \]
    which is a $ \BetaDist{3,1} $ distribution.
    \[ \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1},\; x\in[0,1]. \]
\end{Example}
\makeheading{Lecture 12}{\printdate{2022-10-28}}%chktex 8
Discussion on gamma function when $ \alpha=0 $.
\begin{Example}{}{}
    Suppose $ X \sim \GAM{\alpha,\lambda} $. Find
    $ M_X(t) $.
    \tcblower{}
    \textbf{Solution}:
    \begin{align*}
        M_X(t)
         & =\E{e^{tX}}                                                                                                                                                                               \\
         & =\int_{0}^{\infty}e^{tx}\frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\odif{x}                                                                                           \\
         & =\frac{\lambda^\alpha}{\Gamma(\alpha)}\int_{0}^{\infty}x^{\alpha-1}e^{-\lambda x(1-t/\lambda)}\odif{x}                                                                                    \\
         & =\frac{\lambda^\alpha}{\Gamma(\alpha)}\int_{0}^{\infty}\frac{u^{\alpha -1}}{(\lambda-t)^{\alpha-1}}e^{-u}\frac{1}{\lambda-t}\odif{u} &  & u=x(\lambda-t)\iff \odif{u}=(\lambda-t)\odif{x} \\
         & =\frac{\lambda^\alpha}{\Gamma(\alpha)(\lambda-t)^{\alpha}}\int_{0}^{\infty}u^{\alpha-1}e^{-u}\odif{u}                                                                                     \\
         & =\frac{\lambda^\alpha}{\Gamma(\alpha)(\lambda-t)^{\alpha}}\Gamma(\alpha)                                                                                                                  \\
         & =\biggl(\frac{\lambda}{\lambda-t}\biggr)^{\! \alpha}.
    \end{align*}
\end{Example}
\begin{Example}{}{}
    Suppose $ X_1 \sim \GAM{1/2,2} $ and $ X_2 \sim \GAM{3,2} $ are independent.
    Find $ M_Y(t) $ where $ Y=X_1+X_2 $.
    \tcblower{}
    \textbf{Solution}: Since $ X_1 $ and $ X_2 $ are independent,
    \begin{align*}
        M_Y(t)
         & =M_{X_1}(t)M_{X_2}(t)                                             \\
         & =\biggl(\frac{2}{2-t}\biggr)^{1/2}\biggl(\frac{2}{2-t}\biggr)^{3} \\
         & =\biggl(\frac{2}{2-t}\biggr)^{7/2}.
    \end{align*}
    Therefore, $ Y \sim \GAM{3.5,2} $.
\end{Example}
\begin{Example}{}{}
    The pdf for $ \GAM{1,\lambda} $ is
    \[ f_X(t)=\frac{\lambda^1}{\Gamma(1)}t^0 e^{-\lambda t}=\lambda e^{-\lambda t}, \]
    which is $ \EXP{1} $.
\end{Example}
\begin{Remark}{}{}
    \[ \BIN*{n,\frac{\lambda}{n}}\xrightarrow{n\to\infty} \POI{\lambda}. \]
\end{Remark}
\begin{Example}{}{}
    Suppose Chocolat gets 1 customer every 10 minutes, on average (discrete time).
    \begin{enumerate}[(i)]
        \item Model level 1:
              \begin{itemize}
                  \item Every minute there is an independent $ 1/10 $ chance for a customer to enter
                        ($0$ chance for multiple customers in the same minute).
                  \item Let $ T_1 $ be the waiting time for the first customer in minutes,
                        \[ T_1=\text{waiting time for the first customer in minutes}\sim \GEO*{\frac{1}{10}}, \]
                        and $ \E{T_1}=10 $.
                        \[ N_{60}=\text{number of customers in the first hour}\sim \BIN*{60,\frac{1}{10}}. \]
              \end{itemize}
        \item Model level 2:
              \begin{itemize}
                  \item Every second there is a $ 1/600 $ chance for a customer to enter, independently.
                        \[ T_1=\text{waiting time in minutes}=\frac{\tilde{T}_1}{60},\; \text{where }\tilde{T}_1 \sim \GEO*{\frac{1}{600}}, \]
                        and $ \E{T_1}=600/60=10 $.
                        \[ N_{60}=\text{number of customers in the first hour}\sim \BIN*{3600,\frac{1}{600}}. \]
              \end{itemize}
    \end{enumerate}
    As we approach continuity,
    \[ N_{60}\tod\POI*{\frac{60}{10}},\;
        T_1\tod\EXP*{\frac{1}{10}}. \]
    For $ t\ge 0 $,
    \[ N(t)=\text{number of arrivals in the first $t$ minutes}\sim \POI*{\frac{1}{10}t}. \]
\end{Example}
\begin{Definition}{}{}
    A \textbf{Poisson process} ($ N(t) $ for $ t\ge 0 $) with rate $ \lambda $ is a stochastic process
    with the properties:
    \begin{enumerate}[(1)]
        \item For $ 0\le t_1<t_2 $,
              \[ (N(t_2)-N(t_1))\sim \POI{\lambda(t_2-t_1)}. \]
        \item For $ 0\le t_1<t_2<\cdots<t_n $, the variables
              \[ (N(t_2)-N(t_1)),(N(t_3)-N(t_2)),\ldots (N(t_n)-N(t_{n-1})) \]
              are jointly independent.
    \end{enumerate}
\end{Definition}
\begin{Definition}{}{}
    \[ T_n=\inf{\Set{t\ge 0\given N(t)\ge n}} \]
    is the arrival time of the $ n\textsuperscript{th} $ customer.
\end{Definition}
\begin{Theorem}{Interarrival Times}{}
    $ \Delta_1=T_1 $, and $ \Delta_n=T_n-T_{n-1} $ for $ n\ge 2 $ are known as
    \textbf{interarrival times}. Then,
    $ \Delta_1,\ldots,\Delta_n \iid \EXP{\lambda} $ variables.
\end{Theorem}
\begin{Corollary}{}{}
    For $ 0\le n_1<n_2<\cdots<n_k $,
    \[ T_{n_2}-T_{n_1},T_{n_3}-T_{n_2},\ldots,T_{n_k}-T_{n_{k-1}} \]
    are jointly independent with respective probability
    distributions
    \[ (T_{n_{j+1}}-T_{n_j})\sim \GAM{n_{j+1}-n_j,\lambda}. \]
\end{Corollary}
\begin{Example}{}{}
    $ T_3 \sim \GAM{3,\lambda} $ and
    $ T_5-T_3 \sim \GAM{2,\lambda} $ are independent.
\end{Example}
\begin{Example}{}{}
    Suppose $ X \sim \GAM{\alpha,1} $ and
    $ Y \sim \GAM{\beta,1} $
    are independent (rate doesn't matter, set it equal to $1$
    for simplicity).
    \begin{Example}{}{}
        $ \alpha=3 $, $ \beta=5 $, $ X=T_3 $, $ Y=T_8-T_3 $.
        What is the distribution of $ T_3/T_8 $?
        If it took two hours for 8 people to arrive, what is the
        conditional distribution of how long it took for three people to arrive?
    \end{Example}
    That is, find the distribution of
    \[ Z=\frac{X}{X+Y},\; 0\le Z\le 1. \]
    For $ t\in[0,1] $,
    \[ \frac{x}{x+y}\le t\implies x\le \frac{ty}{1-t}. \]
    Thus, noting that $ X $ and $ Y $ are independent,
    \begin{align*}
        \Prob{\Set{Z\le t}}
         & =\int_{0}^{\infty}\int_{0}^{ty/(1-t)}
        \frac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{-x}\frac{1}{\Gamma(\beta)} y^{\beta-1}e^{-y}\odif{x}\odif{y} \\
         & =\frac{1}{\Gamma(\alpha)\Gamma(\beta)}\int_{0}^{\infty}\int_{0}^{ty/(1-t)}
        x^{\alpha-1}y^{\beta-1}e^{-(x+y)}\odif{x}\odif{y}.
    \end{align*}
    Multivariable substitution:
    \[ u=\frac{x}{x+y},\; v=x+y\implies x=uv,\; y=v-uv=v(1-u). \]
    \[ J=\pdv{(x,y)}{(u{,}v)}=\begin{vmatrix}
            \pdv{x}{u} & \pdv{x}{v} \\
            \pdv{y}{u} & \pdv{y}{v}
        \end{vmatrix}=\begin{vmatrix}
            v  & u   \\
            -v & 1-u
        \end{vmatrix}=\abs{(v)(1-u)-(u)(-v)}=v. \]
    Note that $ u\le t $ and $ 0\le v<\infty $, which implies
    \begin{align*}
         & =\frac{1}{\Gamma(\alpha)\Gamma(\beta)}\int_{0}^{\infty}\int_{0}^{1}
        (uv)^{\alpha-1}\bigl(v(1-u)\bigr)^{\beta-1}e^{-v}v\odif{u}\odif{v}                                                                       \\
         & =\frac{1}{\Gamma(\alpha)\Gamma(\beta)}
        \int_{0}^{\infty}\underbrace{v^{\alpha-1}v^{\beta-1}}_{v^{\alpha+\beta-1}} e^{-v}\odif{v}\int_{0}^{1}u^{\alpha-1}(1-u)^{\beta-1}\odif{u} \\
         & =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\int_{0}^{1}u^{\alpha-1}(1-u)^{\beta-1}\odif{u}.
    \end{align*}
\end{Example}
\begin{Definition}{Beta Distribution}{}
    We say $ X \sim \BetaDist{\alpha,\beta} $ with shape parameters $ 0<\alpha\in\mathbf{R} $ and $ 0<\beta\in\mathbf{R} $ if it has pdf
    \[ f_X(t\mid \alpha,\beta)=\frac{1}{B(\alpha,\beta)}t^{\alpha-1}(1-t)^{\beta-1},\; t\in[0,1] \]
    where $ B(\alpha,\beta) $ denotes the beta function,
    \[ B(\alpha,\beta)=\int_{0}^{1}x^{\alpha-1}(1-x)^{\beta-1}\odif{x}=
        \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}. \]
\end{Definition}
\begin{Theorem}{}{}
    If $ X \sim \GAM{\alpha,1} $ and $ Y \sim \GAM{\beta,1} $, then
    \[ Z=\frac{X}{X+Y}\sim \BetaDist{\alpha,\beta} \]
    and is independent of
    \[ X+Y \sim \GAM{\alpha+\beta,1}. \]
\end{Theorem}