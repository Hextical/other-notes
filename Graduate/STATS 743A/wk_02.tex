\makeheading{Lecture 3}{\printdate{2022-09-14}}%chktex 8
\begin{Example}{}{}
    Suppose we have 4 shirts, 3 pairs of blue jeans, 2 pairs of shorts,
    and 2 pairs of shoes. How many outfits can we make?
    \tcblower{}
    \textbf{Solution}: $ 4\times (3+2)\times 2 $.
\end{Example}
\begin{Example}{}{marbles}
    Suppose we have a bag of 7 marbles numbered $ 1,2,\ldots,7 $.
    We pick one marble uniformly (equal probability) at random, then put it back in the bag.
    Repeat this process three more times. We care about the order.
    \begin{enumerate}[i.]
        \item How many outcomes are in this experiment?
        \item What is $ \Prob[\big]{(2,4,2,7)} $?
        \item What is $ \Prob[\big]{\Set{\text{all $4$ picks are even numbers}}} $.
    \end{enumerate}
    \tcblower{}
    \textbf{Solution}:
    \begin{enumerate}[i.]
        \item This is known as \textbf{sampling with replacement}.
              In our example, $ \abs{\Omega}=7^4 $. We can represent
              our sample space as the set of ordered quadruples.
              \begin{align*}
                  \Omega
                   & =\Set[\big]{(a,b,c,d)\given a,b,c,d\in\Set{1,2,3,4,5,6,7}} \\
                   & =\Set{1,2,3,4,5,6,7}^4.
              \end{align*}
              The set of ordered quadruples (or 4-tuples) of
              numbers $ 1 $ to $ 7 $.
        \item $ 1/7^4 $.
        \item $ (3/7)^4=3^4/7^4 $.
    \end{enumerate}
\end{Example}
\begin{Definition}{}{}
    The \textbf{Cartesian product} of two sets $ A $ and $ B $
    is
    \[ A\times B=\Set[\big]{(a,b)\given a\in A,b\in B}. \]
    \tcblower{}
    For example,
    $ \Set{x,y}\times \Set{1,2,3}=\Set[\big]{(x,1),(x,2),(x,3),(y,1),(y,2),(y,3)} $.
    That is,
    \[ A^n=\underbrace{A\times \cdots \times A}_{\text{$n$ times}}. \]
\end{Definition}
\begin{Proposition}{}{}
    If $ \Omega $ is countable and $ \mathcal{F}=2^{\Omega} $, then
    \[ \forall A\subseteq \Omega,\; \Prob{A}=\sum_{i\in A}\Prob{\Set{i}}. \]
    \tcblower{}
    \textbf{Proof}: Follows from countable additivity.
\end{Proposition}
\begin{Proposition}{}{}
    If $ \Omega $ is finite and all outcomes are equally likely
    (i.e., $ \forall x,y\in \Omega $, $ \Prob{\Set{x}}=\Prob{\Set{y}} $), then
    \[ \forall A\subseteq \Omega,\; \Prob{A}=\frac{\abs{A}}{\abs{\Omega}}. \]
\end{Proposition}
\section*{Sampling without Replacement (Ordered)}
\begin{Example}{}{}
    Suppose we do the same experiment as~\Cref{ex:marbles},
    but we don't pick marbles back after picking them.
    Then, $ \abs{\Omega}=7\times 6\times 5\times 4=\frac{7!}{3!} $, and
    \[ \Omega=\Set[\big]{(a,b,c,d)\in\Set{1,2,\ldots,7}^4\given
            a\ne b\ne c\ne d\ne a\ne c,b\ne d}. \]
    These 4-tuples without repeats are called 4-arrangements.
\end{Example}
\section*{Sampling without Replacement (Unordered)}
\begin{Example}{}{}
    We reach in and grab 4 marbles all at once.
    \[ \Omega=\Set[\big]{A\subseteq \Set{1,2,\ldots,7}\given \abs{A}=4}. \]
    Hence,
    \[ \abs{\Omega}=\frac{7\times 6\times 5\times 4}{4!}=\binom{7}{4}. \]
    These are called 4-combinations. Every 4-combination can be matched up
    with $ 4! $ 4-arrangements. So,
    \[ \abs[\big]{\Set{\text{4-arrangements}}}=4!\times \abs[\big]{\Set{\text{4-combinations}}}, \]
    and we can re-arrange the equation above to get the number of 4-combinations.
\end{Example}
\begin{Example}{}{}
    Suppose we have a standard deck of cards (52 cards where there
    are 13 ranks and 4 suits).
    \begin{enumerate}[i.]
        \item Number of events that we get a full house (3 cards of
              one rank, and 2 cards of another rank)?
        \item Number of events that we get two pairs (2 cards of one rank,
              2 cards of another rank, and one last card of a different rank).
    \end{enumerate}
    \tcblower{}
    \textbf{Solution}:
    \begin{enumerate}[i.]
        \item Number of events:
              \[ \binom{13}{1}\binom{4}{3}\binom{12}{1}\binom{4}{2}=13\times 4\times 12\times 6. \]
        \item Number of events:
              \[ \binom{13}{2}\binom{4}{2}\binom{4}{2}\times 44. \]
    \end{enumerate}
\end{Example}
\section*{Conditional Probability}
\underline{Idea}: Revising your estimate based on
partial information.
\begin{Example}{}{}
    \begin{itemize}
        \item $ 38.0 $M Canadians.
        \item $ 4.23 $M positive COVID-19 tests in Canada
              (pretend all distinct people).
    \end{itemize}
    \[ \Prob{\Set{\text{positive}}}=\frac{4.23\times 10^6}{3.8\times 10^6}\approx 11.1\% \]
    Now, suppose we have further data for Quebec.
    \begin{itemize}
        \item $ 8.49 $M people in Quebec.
        \item $ 1.19 $M positive tests in Quebec.
    \end{itemize}
    \[ \Prob{\Set{\text{positive}}\given \Set{\text{QC}}}=\frac{1.19}{8.49}\approx 14.0\%. \]
\end{Example}
\begin{Definition}{}{}
    If $ A $ and $ B $ are events, and $ \Prob{B}>0 $, then
    the \textbf{conditional probability of $A$ given $B$} is
    \[ \Prob{A\given B}=\frac{\Prob{A\cap B}}{\Prob{B}}. \]
\end{Definition}
\begin{Definition}{}{}
    Events $ A $ and $ B $ are \textbf{independent} if
    \[ \Prob{A\cap B}=\Prob{A}\Prob{B}. \]
    Equivalently, $ A $ and $ B $ are \textbf{independent} if either
    $ \Prob{B}=0 $ or $ \Prob{A\given B}=\Prob{A} $.
\end{Definition}
\begin{Example}{}{}
    Roll a fair 6-sided die. Let
    $ A=\Set{1,2} $, $ B=\Set{1,3,5} $, $ C=\Set{2,4,6} $.
    \begin{align*}
        \Prob{A}       & =\frac{2}{6}=\frac{1}{3}.                               \\
        \Prob{B}       & =\frac{3}{6}=\frac{1}{2}.                               \\
        \Prob{A\cap B}
                       & =\Prob{\Set{1}}                                         \\
                       & =\frac{1}{6}                                            \\
                       & =\Prob{A}\Prob{B}                                       \\
                       & =\frac{1}{3}\times \frac{1}{2}.                         \\
        \Prob{C}       & =\frac{3}{6}=\frac{1}{2}.                               \\
        \Prob{B\cap C} & =\Prob{\varnothing}=0\ne \frac{1}{2}\times \frac{1}{2}.
    \end{align*}
    Therefore, $ B $ and $ C $ are \underline{not} independent, but they are
    \underline{disjoint events}.
    In probability theory, \emph{disjoint events} are also called \textbf{mutually exclusive
        events}.
\end{Example}
\begin{Definition}{}{}
    If $ A $ and $ B $ are \textbf{disjoint}, then
    \begin{itemize}
        \item $ \Prob{A\cup B}=\Prob{A}+\Prob{B} $;
        \item $ \Prob{A\cap B}=0 $.
    \end{itemize}
    If $ A $ and $ B $ are \textbf{independent}, then
    \begin{itemize}
        \item $ \Prob{A\cap B}=\Prob{A}\Prob{B} $;
        \item \begin{align*}
                  \Prob{A\cup B}
                   & =\Prob{A}+\Prob{B}-\Prob{A}\Prob{B} \\
                   & =(\Prob{A}-1)(1-\Prob{B})+1         \\
                   & =1-(1-\Prob{A})(1-\Prob{B})         \\
                   & =1-\Prob{A^c}\Prob{B^c}             \\
                   & =1-\Prob{(A\cup B)^c}               \\
                   & =1-\Prob{A^c\cap B^c}.
              \end{align*}
              This proves that $ A^c $ is independent of $ B^c $.
    \end{itemize}
\end{Definition}
\begin{Example}{}{}
    Suppose we have a standard deck of cards. What is the probability
    that we have four aces if we select four cards?
    \[ \frac{4}{52}\times \frac{3}{1}\times \frac{2}{50}\times \frac{1}{49}=
        \frac{1}{\binom{52}{4}}. \]
    Hence,
    \begin{align*}
        \Prob{A_1\cap A_2\cap A_3\cap A_4}
         & =\Prob{A_1}\Prob{A_2\given A_1}\Prob{A_3\given A_1\cap A_2}
        \Prob{A_4\given A_1\cap A_2\cap A_3}.
    \end{align*}
\end{Example}
\makeheading{Lecture 4}{\printdate{2022-09-16}}%chktex 8
\begin{Definition}{}{}
    A \textbf{partition} of a set $ S $
    is a collection of subsets
    of $ A_1,\ldots,A_n\subseteq S $ with the properties
    \begin{enumerate}[(i)]
        \item $ A_1\cup A_2\cup \cdots \cup A_n = S $
        \item $ \forall 1\le i<j\le n $, $ A_i\cap A_j=\emptyset $.
    \end{enumerate}
\end{Definition}
\begin{Theorem}{Law of Total Probability}{}
    If $ A_1,\ldots,A_n $ is a partition of $ \Omega $ into events
    and $ B\in \mathcal{F} $, then
    \[ \Prob{B}=\Prob{A_1}\Prob{B\given A_1}+
        \Prob{A_2}\Prob{B\given A_2}+\cdots+\Prob{A_n}\Prob{B\given A_n}
        =\sum_{i=1}^{n}\Prob{A_i}\Prob{B\given A_i}. \]
\end{Theorem}
\begin{Example}{}{}
    \begin{itemize}
        \item 20\% of students in STATS 2D are first
              years, 45\% are second years, and
              35\% are third years.
        \item 25\% of first years are getting an A,
              along with 35\% of second years, and 50\% of third years.
    \end{itemize}
    What's the overall percentage who are getting an A\@?
    \[ A_n=\Set{n\textsuperscript{th}\text{ year students}}. \]
    $ \Set{A_1,A_2,A_3} $ is a partition of any class $ \Omega $.
    \[ B=\Set{\text{students getting an A}}. \]
    \[ \Prob{B}=20\%\cdot 25\%+45\%\cdot 35\%+35\%\cdot 50\%. \]
    Bayes Rule allows us to flip the direction of conditioning.
    \[ \Prob{B\given A}=\frac{\Prob{B\cap A}}{\Prob{A}}
        \implies \Prob{B\cap A}=\Prob{B\given A}\Prob{A}. \]
    \begin{align*}
        \Prob{\Set{\text{third year}}\given \Set{\text{getting an A}}}
         & =\Prob{A_3\given B}                                                              \\
         & =\frac{\Prob{A_3}\Prob{B\given A_3}}{\sum_{i=1}^{3}\Prob{A_i}\Prob{B\given A_i}} \\
         & =
    \end{align*}
\end{Example}
\begin{Example}{Monty Hall Problem}{}
    \begin{itemize}
        \item Let $ A_j=\Set{\text{car behind door $j$}} $ for $ j=1,2,3 $.
        \item Let $ G_2=\Set{\text{Monty reveals goat behind door $2$}} $.
        \item For simplicity, assume we choose door $1$ first.
    \end{itemize}
    \begin{align*}
        \Prob{A_1\given G_2}
         & =\frac{\Prob{A_1}\Prob{G_2\given A_1}}{
            \sum_{i=1}^{3}\Prob{A_j}\Prob{G_2\given A_j}.
        }                                          \\
         & =\frac{\frac{1}{3}\cdot \frac{1}{2}}{
            \frac{1}{3}\cdot \frac{1}{2}+\frac{1}{3}\cdot 0+\frac{1}{3}\cdot 1
        }                                          \\
         & =\frac{1/6}{1/6+1/3}                    \\
         & =\frac{1}{1+2}                          \\
         & =\frac{1}{3}.
    \end{align*}
\end{Example}
\begin{Definition}{}{}
    A \textbf{random variable} is a (measurable) function
    on a probability space. A real-valued radom variable is a function
    $ X\colon \Omega\to\mathbf{R} $.
\end{Definition}
\begin{Example}{}{}
    Suppose we flip a fair coin three times.
    (We care about the order because
    we want each event to be equally likely to occur.)
    There are $ 8 $ possible outcomes:
    \begin{align*}
        \Omega
         & =\Set{\text{H},\text{T}}^3              \\
         & =\Set{\text{HHH},\text{HHT},\text{HTH},
            \cdots,\text{TTT}}.
    \end{align*}
    If $ X $ is the number of heads tossed, then it is the function
    \begin{itemize}
        \item $ \text{HHH}\to 3 $;
        \item $ \text{HHT},\text{HTH},\text{THH}\to 2 $;
        \item $ \text{HTT},\text{THT},\text{TTH}\to 1 $;
        \item $ \text{TTT}\to 0 $.
    \end{itemize}
    Therefore, $ X(\text{HTT})=1 $.
\end{Example}
\begin{Definition}{}{}
    A random variable is \textbf{discrete} if it only has countably
    many possible values, meaning
    $ \text{range}(X) $ is countable.
\end{Definition}
\begin{Definition}{}{}
    If $ X $ is discrete, then it has a \textbf{probability
        function} (PMF)
    \[ P_X\colon \text{codomain}(X)\to[0,1]. \]
    \[ P_X(k)=\Prob{X=k}. \]
\end{Definition}
\begin{Example}{}{}
    In our coin tossing example,
    \begin{align*}
        P_X(0) & =\frac{1}{8}.                                  \\
        P_X(1)
               & =\Prob{X=1}                                    \\
               & =\Prob{\Set{\text{TTH},\text{THT},\text{HTT}}} \\
               & =\frac{3}{8}.                                  \\
        P_X(2) & =\frac{3}{8}.                                  \\
        P_X(3) & =\frac{1}{8}.                                  \\
        P_X(k) & =0\text{ for } k\notin \Set{0,1,2,3}.
    \end{align*}
\end{Example}
\begin{Definition}{}{}
    Given a real-valued random variable $ X\colon \Omega\to\mathbf{R} $,
    the \textbf{probability distribution of $ X $} is the probability
    measure
    \[ \mathcal{L}_X(A)=\Prob{\Set{X\in A}}=
        \Prob{\Set{x\in \Omega:X(w)\in A}} \]
    for any reasonably nice (Borel) subset $ A\subseteq\mathbf{R} $.
\end{Definition}
\begin{Example}{}{}
    In our coin tossing example,
    \[ \mathcal{L}_X(A)=\frac{1}{8}\Ind{0\in A}+
        \frac{3}{8}\Ind{1\in A}+\frac{3}{8}\Ind{2\in A}+\frac{1}{8}\Ind{3\in A}. \]
    \[ \mathcal{L}_X([1/2,2\cdot 1/2])=\frac{3}{8}+\frac{3}{8}=\frac{3}{4}. \]
\end{Example}
\begin{Definition}{}{}
    For any real-valued random variable
    $ X\colon \Omega\to\mathbf{R} $, the \textbf{cumulative distribution function}
    (CDF) of $ X $ is the function
    \[ F_X(t)=\Prob{\Set{X\le t}}=\Prob{\Set{w\in \Omega:X(w)\le t}}. \]
\end{Definition}
\begin{Example}{}{}
    In our coin tossing example,
    \begin{itemize}
        \item $ F_X(-1)=0 $.
        \item $ F_X(1)=\frac{1}{8}+\frac{3}{8}=\frac{1}{2} $.
        \item $ F_X(1.5)=\frac{1}{2} $.
    \end{itemize}
    \[ F_X(t)=
        \begin{cases}
            0   & t<0      \\
            1/8 & 0\le t<1 \\
            1/2 & 1\le t<2 \\
            7/8 & 2\le t<3 \\
            1   & t\ge 3
        \end{cases}. \]
\end{Example}
\begin{Theorem}{}{}
    Two real-valued random variables have the same distribution
    if and only if their CDFs are equal.
\end{Theorem}
\begin{Definition}{}{}
    A random variable has a Uniform distribution on $ [0,1] $
    if it has CDF
    \[ F_X(t)=\begin{cases}
            t & 0\le t\le 1 \\
            0 & t<0         \\
            1 & t>1
        \end{cases} \]
\end{Definition}
\begin{Theorem}{}{}
    $ F\colon\mathbf{R}\to[0,1] $ is a CDF for some random variable
    if and only if
    \begin{enumerate}[(i)]
        \item $ \lim\limits_{{t} \to {\infty}}F(t)=1 $;
        \item $ \lim\limits_{{t} \to {-\infty}}F(t)=0 $;
        \item $ F $ is non-decreasing; that is, $ F(s)\le F(t) $ for all $ -\infty<s\le t<\infty $.
    \end{enumerate}
\end{Theorem}
\begin{Example}{}{}
    Suppose we have a dart board with radius 1 ft.
    \[ \Omega=\Set[\big]{(x,y)\in\mathbf{R}^2:x^2+y^2\le 1}. \]
    \begin{align*}
        F_R(t)
         & =\Prob{\Set{R\le t}}                                                           \\
         & =\Prob{\Set{(x,y)\in \Omega:x^2+y^2\le t^2}}                                   \\
         & =\frac{\text{Area}(\text{radius $t$ circle})}{\text{Area}(\text{unit circle})} \\
         & =\frac{\pi t^2}{\pi\cdot 1^2}                                                  \\
         & =t^2.
    \end{align*}
    \[ F_R(t)=\begin{cases}
            0   & t<0         \\
            t^2 & 0\le t\le 1 \\
            1   & t>1
        \end{cases} \]
\end{Example}
\begin{Definition}{}{}
    A random variable $ X $ is continuous if its CDF $ F_X $
    is continuous. In that case, it has a probability density function (PDF)
    \[ f_X=\odv{F_X}{t}. \]
\end{Definition}