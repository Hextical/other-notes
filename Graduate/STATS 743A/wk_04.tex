\makeheading{Lecture 7}{\printdate{2022-10-05}}%chktex 8
\begin{Remark}{Algebraic Properties of Expectation and Variance}{}
    \begin{itemize}
        \item $ \E{aX+b}=a\E{X}+b $.
        \item $ \Var{aX+b}=a^2\Var{X} $.
        \item $ \E{X+Y}=\E{X}+\E{Y} $.
        \item If $ X $ and $ Y $ are independent, then
              $ \E{XY}=\E{X}\E{Y} $, and we say $ X $ and $ Y $ are
              \textbf{uncorrelated}.
        \item To calculate $ \Var{X+Y} $, we have
              \begin{align*}
                  \Var{X+Y}
                   & =\E{(X+Y)^2}-\E{X+Y}^2                               \\
                   & =\E{X^2}+2\E{XY}+\E{Y^2}-\E{X}^2-2\E{X}\E{Y}+\E{Y}^2 \\
                   & =\Var{X}+\Var{Y}+2\Cov{X,Y}.
              \end{align*}
              If $ X $ and $ Y $ are uncorrelated, then
              $ \Var{X+Y}=\Var{X}+\Var{Y} $.
    \end{itemize}
\end{Remark}
\begin{Proposition}{}{}
    \[ \Prob*{\Set*{\sum_{i=1}^{\infty}\abs{X_i}<\infty}}=1
        \implies \E*{\sum_{i=1}^{n}X_i}=\sum_{i=1}^{\infty}\E{X_i}. \]
\end{Proposition}
\begin{Example}{}{}
    If $ S\sim\BIN{n,p} $, then $ S=I_1+\cdots+I_n $,
    where $ I_1,\ldots,I_n\iid \BERN{p} $ trials; that is,
    \begin{align*}
        \Prob{\Set{I_j=0}}=1-p, \\
        \Prob{\Set{I_j=1}}=p.
    \end{align*}
    Hence,
    \[ \E{I_j}=(0)(1-p)+(1)(p)=p. \]
    Therefore,
    \[ \E{S}=\E*{\sum_{j=1}^{n}I_j}=\sum_{j=1}^{n}\E{I_j}=np. \]
\end{Example}
\begin{Example}{}{}
    Suppose I have 100 people at a party. They drop their coats in a pile (all have coats).
    When they leave, each take a uniform random coat.
    Let $ X $ denote the number of people who get back their own coat.
    \begin{enumerate}[(a)]
        \item $ \Prob{\Set{X=0}} $,
        \item $ \E{X} $,
        \item $ \Var{X} $.
    \end{enumerate}
    \tcblower{}
    \textbf{Solution}:
    Let $ X=I_1+\cdots+I_n $, where $ I_j\sim\BERN{1/100} $.
    Note that the $ I_j $'s are \underline{not} independent.
    \begin{enumerate}[(a)]
        \item Inclusion-exclusion.
        \item $ \E{X}=\sum_{i=1}^{100}\E{I_j}=(100)(1/100)=1 $.
        \item \begin{align*}
                  \E{X^2}
                   & =\E*{\biggl(\sum_{j=1}^{n}I_j\biggr)^{\!2}}                                                             \\
                   & =\E*{\sum_{j=1}^{n}I_j^2+2\sum_{1\le k<j\le n}I_j I_k}                                                  \\
                   & =\sum_{j=1}^{n}\E{I_j^2}+2\binom{100}{2}\E{I_1 I_2}                                                     \\
                   & =\sum_{j=1}^{n}\bigl[(0)^2(1-1/100)+(1)^2(1/100)\bigr]+2 \frac{100\cdot 99}{2}\frac{1}{100}\frac{1}{99} \\
                   & =2.
              \end{align*}
              Thus,
              \[ \Var{X}=2-1=1. \]
    \end{enumerate}
    Converges to $ \POI{1} $ as $ n\to\infty $.
\end{Example}
\begin{Example}{}{}
    Every box of Sugar Bombs cereal has a toy inside. There are 100 different
    toys and each box contains an i.i.d.\ uniform random toy.
    Let $ X $ be the number of boxes purchased in order to complete
    a set of at least one of each toy. Find $ \E{X} $.
    \tcblower{}
    \textbf{Solution}: Let $ Y_j $ be the number of additional trials
    to get $ (j+1)\textsuperscript{st} $ toy after first $ j $ toys.
    For each $ j $, $ Y_j \sim \GEO*{\frac{100-j}{100}} $. Hence,
    \[ X=Y_0+Y_1+\cdots+Y_{99}. \]
    Therefore,
    \begin{align*}
        \E{X}
         & =\sum_{j=0}^{99}\E{Y_j}                                          \\
         & =\sum_{j=0}^{99}\frac{100}{100-j}                                \\
         & =(100)\sum_{j=0}^{99}\frac{1}{100-j}                             \\
         & =100\biggl(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{100}\biggr) \\
         & \approx 100\ln{100}.
    \end{align*}
\end{Example}
\begin{Remark}{Measure-Theoretic Integration}{}
    Recall~\Cref{thm:mgf_0}. In general, for a random
    variable $ X\colon \Omega\to\mathbf{R} $ defined on a probability
    space $ (\Omega,\mathcal{F},\mathbb{P}) $,
    \[ \E{X}=\int_{\Omega}X(\omega)\odif{\mathbb{P}(\omega)}. \]
    Recall that for Riemann sums, we draw vertical bars under the function.
    However, for Lebesgue (measure) integral, we draw horizontal bars, which
    implies that we do not need a continuous function.

    \underline{Idea}:
    \begin{align*}
        \E{X}
         & =\int_{\Omega}X(\omega)\odif{\mathbb{P}(\omega)}         \\
         & =\lim\limits_{{n} \to {\infty}}\sum_{j=-\infty}^{\infty}
        \Prob*{\Set*{\frac{j}{n}<X\le \frac{j+1}{n}}}\cdot \frac{j}{n}.
    \end{align*}
\end{Remark}
\begin{Remark}{}{}
    \[ \lim\limits_{{x} \to {\infty}}\lim\limits_{{y} \to {\infty}}
        \biggl(\frac{1}{x}\biggr)^{1/y}=\lim\limits_{{x} \to {\infty}}1=1. \]
    \[ \lim\limits_{{y} \to {\infty}}\lim\limits_{{x} \to {\infty}}
        \biggl(\frac{1}{x}\biggr)^{1/y}=\lim\limits_{{y} \to {\infty}}0=0. \]
\end{Remark}
\begin{Theorem}{Lebesgue Dominated Convergence Theorem}{}
    Suppose $ X $ is a measurable function (random variable) on a measure
    probability space $ (\Omega,\mathcal{F},\Omega) $, and $ X_1,X_2,X_3,\ldots $ is a sequence
    of real-valued measurable functions on this space that converge
    pointwise to $ X $; that is,
    \[ \forall \omega\in\Omega,\; \lim\limits_{{n} \to {\infty}}
        X_n(\omega)=X(\omega). \]
    Suppose there is some non-negative measurable
    function $ Y $ such that for all $ n\ge 1 $ and for all
    $ \omega\in\Omega $,
    \[ \abs{X_n(\omega)}\le Y(\omega), \]
    and $ \int_{\Omega}^{}Y(\omega)\odif{\mathbb{P}(\omega)}<\infty $.
    Then, we conclude that
    \[ \lim\limits_{{n} \to {\infty}}\int_{\Omega}^{}\abs{X_n(\omega)-X(\omega)}\odif{\mathbb{P}(\omega)} =0. \]
    Moreover, $ \int_{\Omega}^{}X(\omega)\odif{\mathbb{P}(\omega)} $
    exists (is finite) and equals
    \[ \lim\limits_{{n} \to {\infty}}\int_{\Omega}^{}X(\omega)\odif{\mathbb{P}(\omega)}. \]
    This theorem also holds for infinite measure spaces.
\end{Theorem}

\makeheading{Lecture 8}{\printdate{2022-10-07}}%chktex 8
Cancelled.