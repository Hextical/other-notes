\makeheading{Lecture 19}{\printdate{2022-12-07}}%chktex 8
\begin{Definition}{Statistic}{}
    Recall, given a sample $ (X_1,X_2,\ldots,X_n)=\Vector{X} $,
    a \textbf{statistic} of the sample is some function
    $ T(\Vector{X})\in\mathbf{R}^d $.
\end{Definition}
\begin{Definition}{}{}
    We say $ T $ is a \textbf{sufficient statistic} for
    a parameter $ \theta $ of the distribution of the sample
    if any inference about $ \theta $ based on the sample should depend
    only on $ T(\Vector{X}) $.
\end{Definition}
\begin{Definition}{}{}
    $ T(\Vector{X}) $ is a \textbf{sufficient statistic} for $ \theta $
    if the conditional distribution of $ \Vector{X} $
    given $ T(\Vector{X}) $ does not depend on $ \theta $.
    \[ \theta\leftrightarrow T(\Vector{X})\leftrightarrow \Vector{X}. \]
    \tcblower{}
    \begin{itemize}
        \item In a Bayesian framework, we would say $ \theta $ and $ \Vector{X} $
              are conditionally independent given $ T(\Vector{X}) $.
        \item If $ T(\Vector{x})=T(\Vector{y}) $, then our inferences about
              $ \theta $ should be the same in the sample.
    \end{itemize}
\end{Definition}
\begin{Theorem}{}{}
    If $ p(\Vector{x}\mid \theta) $ is the joint pmf or pdf of the sample
    $ \Vector{X} $ and $ q(t\mid \theta) $  is the pmf or (joint)
    pdf of $ T(\Vector{X}) $, then $ T(\Vector{X}) $ is a sufficient statistic
    for $ \theta $ if and only if the ratio
    \[ \frac{p(\Vector{x}\mid \theta)}{q(T(\Vector{x})\mid \theta)}
        =\Prob{\Set{\Vector{X}=\Vector{x}}\given \Set{T(\Vector{X})=T(\Vector{x})}} \]
    does not depend on $ \theta $; that is,
    \[ \forall \Vector{x}\; \exists C\in\interval[open right]{0}{\infty}
        \text{ such that }\forall \theta,\; \frac{p(\Vector{x}\mid \theta)}{q(T(\Vector{x})\mid \theta)}=C. \]
\end{Theorem}
\begin{Example}{}{}
    $ X_1,\ldots,X_n \iid \BERN{\theta} $ for $ 0<\theta<1 $. Let
    $ T(\Vector{X})=\sum_{j=1}^{n}X_j $, so
    $ T(\Vector{X})\sim\BIN{n,\theta} $. Fix $ \Vector{x}\in\Set{0,1}^n $. Let
    $ t=\sum_{i=1}^{n}x_i=T(\Vector{x}) $. First,
    \[ p(\Vector{x}\mid \theta)=\prod_{i=1}^n
        \begin{cases}
            \theta,   & x_i=1, \\
            1-\theta, & x_i=0
        \end{cases}=
        \theta^{T(\Vector{x})}=(1-\theta)^{n-T(\Vector{x})}. \]
    \begin{Remark}{}{}
        \[ \ProbB{11010 \given \theta}=\theta\cdot \theta\cdot (1-\theta)\cdot \theta\cdot(1-\theta)=\theta^3(1-\theta)^2. \]
    \end{Remark}
    Second,
    \[ q(t\mid \theta)=\binom{n}{t}\theta^t(1-\theta)^{n-t}. \]
    \begin{Remark}{}{}
        \[ \ProbB{\text{three 1's and two 0's}\given \theta}=\ProbB{T(\Vector{X})=3\given \theta}. \]
    \end{Remark}
    Using these two facts,
    \[ \frac{p(\Vector{x}\mid \theta)}{q(T(\Vector{x})\mid \theta)}=\frac{1}{\binom{n}{T(\Vector{x})}}, \]
    which does not depend on $ \theta $, so $ T $ is sufficient for $ \theta $.
\end{Example}
\begin{Example}{}{}
    $ \Vector{X}=(X_1,\ldots,X_n)\iid \N{\mu,\sigma^2} $. Suppose $ \sigma^2 $
    is known and $ \mu $ is unknown.
    \[ T(\Vector{X})=\bar{X}=\frac{X_1+\cdots+X_n}{n} \sim \N*{\mu,\frac{\sigma^2}{n}}. \]
    Recall the following trick:
    \begin{align*}
        \sum_{i=1}^{n}(x_i-\mu)^2
         & =\sum_{i=1}^{n}\bigl((x_i-\bar{x})+(\bar{x}-\mu)\bigr)  \\
         & =n(\bar{x}-\mu)^2+\sum_{i=1}^{n}(x_i-\bar{x})+
        2(\bar{x}-\mu)\underbrace{\sum_{i=1}^{n}(x_i-\bar{x})}_{0} \\
         & =n(\bar{x}-\mu)^2+\sum_{i=1}^{n}(x_i-\bar{x}).
    \end{align*}
    First,
    \[ p(\Vector{x}\mid \mu,\sigma^2)
        =(2\pi\sigma^2)^{-n/2}\exp*{-\frac{\sum_{i=1}^{n}(x_i-\mu)^2}{2\sigma^2}}. \]
    Second,
    \[ q(t(\Vector{x})\mid \mu,\sigma^2)
        =\biggl(2\pi \frac{\sigma^2}{n}\biggr)^{-1/2}\exp*{-\frac{(\bar{x}-\mu)^2}{2(\sigma^2/n)}}. \]
    Hence,
    \begin{align*}
        \frac{p}{q}
         & =\sqrt{n}(2\pi\sigma^2)^{(1-n)/2}
        \exp*{-\frac{1}{2\sigma^2}
        \biggl(n(\bar{x}-\mu)^2+\sum_{i=1}^{n}(x_i-\bar{x})^2\biggr)+\frac{n(\bar{x}-\mu)^2}{2\sigma^2}} \\
         & =\sqrt{n}(2\pi\sigma^2)^{-(n-1)/2}\exp*{-\frac{\sum_{i=1}^{n}(x_i-\bar{x})^2}{2\sigma^2}},
    \end{align*}
    which does not depend on $ \mu $, so $ T $ is a sufficient statistic for $ \mu $.
\end{Example}
The vector of order statistics of a sample $ X_{(1)}\le X_{(2)}\le \cdots\le X_{(n)} $
is sufficient for everything.
\begin{Theorem}{Factorization Theorem (Halmos + Savage, 1949)/(Neyman 1935)}{}
    $ T $ is sufficient for $ \theta $ if and only if
    there exists functions $ g $ and $ h $ such that
    \[ \forall \Vector{x}\; \forall \theta,\; p(\Vector{x}\mid \theta)
        =g(T(\Vector{x}\mid \theta))h(\Vector{x}) \]
    \tcblower{}
    \textbf{Proof} (Discrete Setting):
    Assume $ \Vector{X} $ is a sample from a discrete distribution.

    $ (\implies) $ Assume $ T $ is sufficient. Choose $ g(t,\theta)=q(t\mid \theta) $
    and $ h(\Vector{x})=\ProbB{\Vector{X}=\Vector{x}\given T(\Vector{X})=T(\Vector{x})} $
    (sufficiency was used to define $ h(\Vector{x}) $).
    Hence,
    \begin{align*}
        g(T(\Vector{x}),\theta)h(\Vector{x})
         & =q(T(\Vector{x})\mid \theta)\frac{\ProbB{\Vector{X}=\Vector{x}\given \theta}}{\ProbB{T(\Vector{X})=T(\Vector{x})\given \theta}} \\
         & =q(T(\Vector{x})\mid \theta)\frac{p(\Vector{x}\mid \theta)}{q(T(\Vector{x})\mid \theta)}                                        \\
         & =p(\Vector{x}\mid \theta).
    \end{align*}
    $ (\impliedby) $ Assume $ p(\Vector{x}\mid \theta)=g(T(\Vector{x}),\theta)h(\Vector{x}) $ for some $ g $ and $ h $.
    Then,
    \[ q(t\mid \theta)=\sum_{\Vector{x}\colon T(\Vector{x})=t}p(\Vector{x}\mid \theta). \]
    Therefore,
    \begin{align*}
        \frac{p(\Vector{x}\mid \theta)}{q(T(\Vector{x})\mid \theta)}
         & =\frac{g(T(\Vector{x}),\theta)h(\Vector{x})}{\sum_{\Vector{y}\colon T(\Vector{y})=T(\Vector{x})}g(T(\Vector{y}),\theta)h(\Vector{y})} \\
         & =\frac{g(T(\Vector{x}),\theta)h(\Vector{x})}{\sum_{\Vector{y}\colon T(\Vector{y})=T(\Vector{x})}g(T(\Vector{x}),\theta)h(\Vector{y})} \\
         & =\frac{h(\Vector{x})}{\sum_{\Vector{y}\colon T(\Vector{y})=T(\Vector{x})}h(\Vector{y})},
    \end{align*}
    which does not depend on $ \theta $, so $ T $ is sufficient.
\end{Theorem}
\begin{Example}{}{}
    In our $ \N{\mu,\sigma^2} $ example with $ T(\Vector{X})=\bar{X} $, we have
    \[ h(\Vector{x})=\sqrt{n}(2\pi\sigma^2)^{-(n-1)/2}\exp*{-\frac{\sum_{i=1}^{n}(x_i-\bar{x})^2}{2\sigma^2}}, \]
    and
    \[ q(t,\mu)=\biggl(2\pi \frac{\sigma^2}{n}\biggr)^{-1/2}\exp*{-\frac{(t-\mu)^2}{2(\sigma^2/n)}}. \]
\end{Example}
\begin{Example}{}{}
    IID $ \text{Uniform}\Set{1,2,\ldots,\theta} $ for $ \theta\in\mathbf{N} $.
    \[ p(\Vector{x}\mid \theta)=
        \begin{dcases}
            \frac{1}{\theta^n}, & \operatorname*{max}_{1\le i\le n}x_i\le \theta, \\
            0,                  & \text{otherwise}.
        \end{dcases} \]
    So,
    $ T(\Vector{x})=\operatorname*{max}_{1\le i\le n}x_i $.
    Take
    \[ g(t,\theta)=
        \begin{dcases}
            \frac{1}{\theta^n}, & t_i\le \theta,    \\
            0,                  & \text{otherwise},
        \end{dcases} \]
    and
    \[ h(\Vector{x})=\begin{cases}
            1, & x_i\in\mathbf{N}\; \forall i, \\
            0, & \text{otherwise}.
        \end{cases} \]


\end{Example}