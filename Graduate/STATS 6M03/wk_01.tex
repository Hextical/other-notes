\makeheading{Lecture 1}{\printdate{2022-09-07}}%chktex 8
\section{Introduction to Multivariate Distributions}
\subsection*{Notation \& Terminology}
\begin{itemize}
      \item We observe $ n $ realizations $ \Vector{x}_1,\ldots,\Vector{x}_n $ of $ p $-dimensional
            random variables $ \RVector{X}_1,\ldots,\RVector{X}_n $, where $ \RVector{X}_i=(\RV{X}_{i1},\ldots,\RV{X}_{ip})' $
            for $ i=1,\ldots,n $.
      \item In matrix form,
            \[ \RMatrix{X}=(\RVector{X}_1,\ldots,\RVector{X}_n)'=
                  \begin{pmatrix}
                        \RVector{X}_1' \\
                        \vdots         \\
                        \RVector{X}_n'
                  \end{pmatrix}=\begin{pmatrix}
                        \RV{X}_{11} & \cdots & \RV{X}_{1p} \\
                        \vdots      & \ddots & \vdots      \\
                        \RV{X}_{n1} & \cdots & \RV{X}_{np}
                  \end{pmatrix}. \]
      \item $ \RVector{X}_i $ is called a \textbf{random vector}.
      \item $ \RMatrix{X} $ is called an $ n\times p $ \textbf{random matrix}.
      \item A matrix $ \Matrix{A} $ with all entries constant is called a \textbf{constant matrix}.
\end{itemize}
\subsection*{Basic Definitions \& Results}
\begin{itemize}
      \item If $ \RMatrix{X} $ and $ \RMatrix{Y} $ be $ n\times p $ random matrices, then
            \[ \E{\RMatrix{X}+\RMatrix{Y}}=\E{\RMatrix{X}}+\E{\RMatrix{Y}}. \]
            Furthermore, if $ \Matrix{A} $, $ \Matrix{B} $, and $ \Matrix{C} $ are $ m\times n $, $ p\times q $, and $ m\times q $ matrices of constants, respectively,
            then
            \[ \E{\Matrix{A}\RMatrix{X}\Matrix{B}+\Matrix{C}}=\Matrix{A}\E{\RMatrix{X}}\Matrix{B}+\Matrix{C}. \]
      \item If $ \RVector{X}_i $ has mean $ \Vector{\mu} $, then the \textbf{covariance matrix} of $ \RVector{X}_i $ is
            \[ \Matrix{\Sigma}=\Var{\RVector{X}_i}=\E[\big]{(\RVector{X}_i-\Vector{\mu})(\RVector{X}_i-\Vector{\mu})'}. \]
      \item If $ p $-dimensional $ \RVector{X}_i $ has mean $ \Vector{\mu} $ and $ q $-dimensional $ \RVector{Y}_i $ has mean $ \Vector{\theta} $,
            then
            \[ \Cov{\RVector{X}_i,\RVector{Y}_i}=\E[\big]{(\RVector{X}_i-\Vector{\mu})(\RVector{Y}_i-\Vector{\theta})'}. \]
\end{itemize}
\subsection*{Covariance Matrix I}
\begin{itemize}
      \item The covariance matrix $ \Matrix{\Sigma} $ is
            \[ \Matrix{\Sigma}=\begin{pmatrix}
                        \sigma_{11} & \cdots & \sigma_{1p} \\
                        \vdots      & \ddots & \vdots      \\
                        \sigma_{p1} & \cdots & \sigma_{pp}
                  \end{pmatrix}. \]
      \item The elements $ \sigma_{ii} $ are variances.
      \item The elements $ \sigma_{ij} $, where $ i\ne j $ are covariances.
      \item $ \Matrix{\Sigma} $ is symmetric; that is, $ \sigma_{ij}=\sigma_{ji} $.
\end{itemize}
\subsection*{Covariance Matrix II}
\begin{itemize}
      \item Let $ \Matrix{\Sigma} $ be the covariance matrix of a $ p\times 1 $ random vector.
            \begin{enumerate}[(i)]
                  \item $ \Matrix{\Sigma} $ is positive semi-definite; that is, for any $ p\times 1 $ constant vector
                        $ \Vector{a}=(a_1,\ldots,a_p)' $,
                        \[ \Vector{a}'\Matrix{\Sigma}\Vector{a}\ge \Vector{0}. \]
                  \item If $ \Matrix{B} $ is a $ q\times p $ constant matrix and $ \Vector{b} $ is a $ q\times 1 $ constant vector, then
                        the covariance matrix of $ \RMatrix{Y}=\Matrix{B}\RMatrix{X}+\Vector{b} $ is
                        \[ \Var{\RMatrix{Y}}=\Matrix{B}\Matrix{\Sigma}\Matrix{B}'. \]
            \end{enumerate}
      \item Note that the covariance matrix of $ \Matrix{\Sigma} $ is \textbf{positive definite} if
            \[ \Vector{a}'\Matrix{\Sigma}\Vector{a}>\Vector{0} \]
            for any constant vector $ \Vector{a}\ne \Vector{0} $.
\end{itemize}
\subsection*{Covariance Matrix III}
\begin{itemize}
      \item In general, the covariance matrix $ \Matrix{\Sigma} $ is positive semi-definite.
      \item Therefore, the eigenvalues of $ \Matrix{\Sigma} $ are non-negative, and are denoted
            \[ \lambda_1\ge \lambda_2\ge \cdots \ge \lambda_p\ge 0. \]
      \item Eigenvalues are also known as characteristic roots.
      \item Write $ \Vector{v}_i\ne \Vector{0} $ to denote an eigenvector of $ \Matrix{\Sigma} $ corresponding to the eigenvalue
            $ \lambda_i $ for $ i=1,\ldots,p $.
      \item Recall that eigenvalues and eigenvectors of $ \Matrix{\Sigma} $ satisfy
            \begin{equation}\tag*{($\star$)}\label{L1Star1}
                  \Matrix{\Sigma}\Vector{v}_i=\lambda_i \Vector{v}_i
            \end{equation}
            for $ i=1,\ldots,p $.
\end{itemize}
\subsection*{Aside: Interesting Result}
Let $ \Matrix{A} $ be a symmetric matrix. Suppose $ \Vector{e}_1 $ and $ \Vector{e}_2 $ are eigenvectors
of $ \Matrix{A} $ with corresponding eigenvalues $ \lambda_1 $ and $ \lambda_2 $, respectively, such that
$ \lambda_1\ne \lambda_2 $. Then, $ \Vector{e}_1 $ and $ \Vector{e}_2 $ are orthogonal; that is,
\[ \Vector{e}_2'\Vector{e}_1=0. \]
\begin{framed}
      Since $ \Matrix{A}\Vector{e}_2=\lambda_2 \Vector{e}_2 $, we have
      \begin{equation}\tag*{(1)}\label{L1E1}
            \Vector{e}_1' \Matrix{A}\Vector{e}_2=\lambda_2 \Vector{e}_1'\Vector{e}_2
      \end{equation}
      Now,
      \begin{equation}\tag*{(2)}\label{L1E2}
            \begin{aligned}
                  (\Vector{e}_1' \Matrix{A}\Vector{e}_2)'
                   & =(\Matrix{A}\Vector{e}_2)'(\Vector{e}_1')' \\
                   & =\Vector{e}_2' \Matrix{A}\Vector{e}_1      \\
                   & =\Vector{e}_2'(\lambda_1 \Vector{e}_1)     \\
                   & =\lambda_1 \Vector{e}_2' \Vector{e}_1.
            \end{aligned}
      \end{equation}
      Using~\ref{L1E2}, we also have
      \begin{equation}\tag*{(3)}\label{L1E3}
            \begin{aligned}
                  (\Vector{e}_1' \Matrix{A}\Vector{e}_2)'
                   & =\lambda_2(\Vector{e}_1'\Vector{e}_2)' \\
                   & =\lambda_2 \Vector{e}_2'\Vector{e}_1.
            \end{aligned}
      \end{equation}
      Equating~\ref{L1E2} and~\ref{L1E3} yields
      \begin{align*}
            \lambda_1 \Vector{e}_2'\Vector{e}_1                     & =\lambda_2 \Vector{e}_2'\Vector{e}_1        \\
            \implies (\lambda_2-\lambda_1)\Vector{e}_2'\Vector{e}_1 & =0                                          \\
            \implies \Vector{e}_2'\Vector{e}_1                      & =0\text{ since $ \lambda_1\ne \lambda_2 $}.
      \end{align*}
\end{framed}
\subsection*{Covariance Matrix IV}
Without loss of generality, it can be assumed that the eigenvectors are orthonormal; that is,
\[ \Vector{v}_i'\Vector{v}_i=1,\text{ and } \Vector{v}_i'\Vector{v}_j=0,\; i\ne j. \]
\subsection*{Covariance Matrix V}
\begin{itemize}
      \item Write $ \Matrix{V}=(\Vector{v}_1,\ldots,\Vector{v}_p) $ and
            \[ \Matrix{\Lambda}=\diag{\lambda_1,\ldots,\lambda_p}=\begin{pmatrix}
                        \lambda_1 & 0         & \cdots & 0         \\
                        0         & \lambda_2 & \cdots & 0         \\
                        \vdots    & \vdots    & \ddots & \vdots    \\
                        0         & 0         & \cdots & \lambda_p
                  \end{pmatrix}. \]
      \item We can show that~\ref{L1Star1} can be written $ \Matrix{\Sigma}\Matrix{V}=\Matrix{V}\Matrix{\Lambda} $.
            \begin{framed}
                  First,
                  \begin{equation}\tag*{(1)}\label{L1E21}
                        \Matrix{\Sigma}\Matrix{V}=\Matrix{\Sigma}(\Vector{v}_1,\ldots,\Vector{v}_p)\\
                        =(\Matrix{\Sigma}\Vector{v}_1,\ldots,\Matrix{\Sigma}\Vector{v}_p).
                  \end{equation}
                  Second,
                  \begin{equation}\tag*{(2)}\label{L1E22}
                        \Matrix{V}\Matrix{\Lambda}=(\lambda_1 \Vector{v}_1,\ldots,\lambda_p \Vector{v}_p).
                  \end{equation}
                  Now,~$\ref{L1E21}=\ref{L1E22}$ if and only if
                  \begin{align*}
                        \Matrix{\Sigma}\Vector{v}_1 & =\lambda_1 \Vector{v}_1  \\
                                                    & \vdotswithin{=}          \\
                        \Matrix{\Sigma}\Vector{v}_p & =\lambda_p \Vector{v}_p.
                  \end{align*}
                  That is, $ \Matrix{\Sigma}\Vector{v}_i=\lambda_i \Vector{v}_i $ for $ i=1,\ldots,p $.
            \end{framed}
      \item Note that $ \Matrix{V}'\Matrix{V}=\Matrix{I}_p $ and
            \[ \Matrix{\Sigma}=\Matrix{V}\Matrix{\Lambda}\Matrix{V}'=\Matrix{V}\Matrix{\Lambda}^{1/2}\Matrix{V}'\Matrix{V}\Matrix{\Lambda}^{1/2}\Matrix{V}', \]
            where $ \Matrix{\Lambda}^{1/2}=\diag{\sqrt{\lambda}_1,\ldots,\sqrt{\lambda}_p} $.
\end{itemize}
\subsection*{Covariance Matrix VI}
\begin{itemize}
      \item We can also show that the determinant of the covariance matrix $ \Matrix{\Sigma} $ can be written
            \[ \abs{\Matrix{\Sigma}}=\abs{\Matrix{V}\Matrix{\Lambda}\Matrix{V}'}=\abs{\Matrix{\Lambda}}=\prod_{i=1}^p \lambda_i. \]
            \begin{framed}
                  \[ \abs{\Matrix{\Sigma}}=\abs{\Matrix{V}\Matrix{\Lambda}\Matrix{V}'}=\abs{\Matrix{V}}\abs{\Matrix{\Lambda}}\abs{\Matrix{V}^{-1}}=\frac{\abs{\Matrix{V}}\abs{\Matrix{\Lambda}}}{\abs{\Matrix{V}}}
                        =\abs{\Matrix{\Lambda}}=\prod_{i=1}^p \lambda_i. \]
            \end{framed}
      \item The total variance of the covariance $ \Matrix{\Sigma} $ can be written as
            \[ \tr{\Matrix{\Sigma}}=\tr{\Matrix{\Lambda}}=\sum_{i=1}^{p}\lambda_i. \]
            \begin{framed}
                  \begin{align*}
                        \tr{\Matrix{\Sigma}}
                         & =\tr{\Matrix{V}\Matrix{\Lambda}\Matrix{V}'} \\
                         & =\tr{\Matrix{\Lambda}\Matrix{V}\Matrix{V}'} \\
                         & =\tr{\Matrix{\Lambda}}                      \\
                         & =\sum_{i=1}^{p}\lambda_i.
                  \end{align*}
            \end{framed}
      \item The determinant and total variance are sometimes used as summaries of the total scatter amongst the $ p $ variables.
\end{itemize}
\subsection*{Comments}
\begin{itemize}
      \item This lecture has presented some basics, much of which is a revision of ideas
            encountered in linear algebra.
      \item Please take some time to digest this material before the next class.
      \item In the next class, we will start to look at principal components analysis.
\end{itemize}
\makeheading{Lecture 2}{\printdate{2022-09-08}}%chktex 8
\section{Principal Component Analysis}
\subsection*{What is a Principal Component?}
\begin{itemize}
      \item The first principal component is the direction of most variation (in the
            data).
      \item The second principal component is the direction of most variation (in the
            data) conditional on it being orthogonal to the first principal component.
      \item The third principal component is the direction of most variation (in the
            data) conditional on it being orthogonal to the first two principal
            components.
      \item For $r > 1$: the $r\textsuperscript{th}$ principal component is the direction of most variation
            (in the data) conditional on it being orthogonal to the first $r-1$ principal
            components.
\end{itemize}
\subsection*{Definition}
\begin{itemize}
      \item Let $ \RVector{X} $ be a $ p $-dimensional random vector with mean $ \Vector{\mu} $ and covariance matrix $ \Matrix{\Sigma} $.
      \item Let $ \lambda_1\ge \cdots \ge \lambda_p\ge 0 $ be the (ordered) eigenvalues of $ \Matrix{\Sigma} $ and let
            $ \Vector{v}_1,\ldots,\Vector{v}_p $ be the corresponding eigenvectors.
      \item The $ i\textsuperscript{th} $ principal component of $ \RVector{X} $ is
            \[ \RV{W}_i=\Vector{v}_i'(\RVector{X}-\Vector{\mu}),\; i=1,\ldots,p. \]
      \item That is,
            \[ \RVector{W}=\Matrix{V}'(\RVector{X}-\Vector{\mu}), \]
            where $ \RVector{W}=(\RV{W}_1,\ldots,\RV{W}_p)' $ and $ \Matrix{V}=(\Vector{v}_1,\ldots,\Vector{v}_p) $.
\end{itemize}
\subsection*{Some Results}
\begin{itemize}
      \item $ \E{\RVector{W}}=\Vector{0} $.
            \begin{framed}
                  \begin{align*}
                        \E{\RVector{W}}
                         & =\E{\Matrix{V}'(\RVector{X}-\Vector{\mu})} \\
                         & =\Matrix{V}'\E{\RVector{X}-\Vector{\mu}}   \\
                         & =\Matrix{V}'\Vector{0}                     \\
                         & =\Vector{0}.
                  \end{align*}
            \end{framed}
      \item $ \Var{\RVector{W}}=\Matrix{\Lambda} $.
            \begin{framed}
                  \begin{align*}
                        \Var{\RVector{W}}
                         & =\E{\RVector{W}\RVector{W}'}-\E{\RVector{W}}\E{\RVector{W}}'                     \\
                         & =\E{\RVector{W}\RVector{W}'}                                                     \\
                         & =\E*{\Matrix{V}'(\RVector{X}-\Vector{\mu})(\RVector{X}-\Vector{\mu})'\Matrix{V}} \\
                         & =\Matrix{V}'\E*{(\RVector{X}-\Vector{\mu})(\RVector{X}-\Vector{\mu})'}\Matrix{V} \\
                         & =\Matrix{V}'\Var{\RVector{X}}\Matrix{V}                                          \\
                         & =\Matrix{V}'\Matrix{\Sigma}\Matrix{V}                                            \\
                         & =\Matrix{\Lambda}.
                  \end{align*}
            \end{framed}
      \item $ \RVector{X}=\Vector{\mu}+\Matrix{V}\RVector{W}=\Vector{\mu}+\sum_{i=1}^{p}\Vector{v}_i \RV{W}_i $.
            \begin{framed}
                  Using $ \RVector{W}=\Matrix{V}'(\RVector{X}-\Vector{\mu}) $, we multiply $ \Matrix{V} $ from the LHS to get
                  \begin{align*}
                        \Matrix{V}\RVector{W} & =\Matrix{V}\Matrix{V}'(\RVector{X}-\Vector{\mu}) \\
                                              & =\Matrix{I}_p(\RVector{X}-\Vector{\mu})          \\
                                              & =\RVector{X}-\Vector{\mu}.
                  \end{align*}
                  Rearranging,
                  \[ \RVector{X}=\Vector{\mu}+\Matrix{V}\RVector{W}=\Vector{\mu}+\sum_{i=1}^{p}\Vector{v}_i \RV{W}_i. \]
            \end{framed}
      \item $ \sum_{i=1}^{p}\Var{\RV{W}_i}=\sigma_{11}+\cdots+\sigma_{pp} $.
            \begin{framed}
                  From the second result,
                  \begin{align*}
                        \Var{\RV{W}_i}                        & =\lambda_i                       \\
                        \implies \sum_{i=1}^{p}\Var{\RV{W}_i} & =\sum_{i=1}^{p}\lambda_i         \\
                                                              & =\tr{\Matrix{\Lambda}}           \\
                                                              & =\tr{\Matrix{\Sigma}}            \\
                                                              & =\sigma_{11}+\cdots+\sigma_{pp}.
                  \end{align*}
            \end{framed}
\end{itemize}
\subsection*{Key Results}
\begin{itemize}
      \item Consider $ \RV{W}=\Vector{v}'(\RVector{X}-\Vector{\mu}) $ with $ \Vector{v}'\Vector{v}=1 $.
            \begin{enumerate}[i.]
                  \item $ \Var{\RV{W}} $ is maximized when $ \RV{W}=\RV{W}_1 $, the first principal component.
                        \begin{framed}
                              Note that $ \Var{\RV{W}}=\Vector{v}'\Matrix{\Sigma}\Vector{v} $.
                              We can write
                              \[ \Vector{v}=c_1 \Vector{v}_1+\cdots+c_p \Vector{v}_p=\Matrix{V}\Vector{c}, \]
                              where $ \Vector{c} $ satisfies $ \Vector{c}'\Vector{c}=1 $.

                              \underline{Note}: $ \Vector{v}'\Vector{v}=\Vector{c}'\Matrix{V}'\Matrix{V}\Vector{c}=\Vector{c}'\Vector{c}=1 $.

                              Therefore,
                              \[ \Var{\RV{W}}=\Vector{c}'\Matrix{V}\Matrix{\Sigma}\Matrix{V}'\Vector{c}=c_1^2\lambda_1+\cdots+c_p^2\lambda_p, \]
                              which is maximized when $ c_1=1 $ and $ c_2=\cdots=c_p=0 $; that is,
                              when $ \Vector{v}=\Vector{v}_1 $.
                        \end{framed}
                  \item If $ \RV{W} $ is uncorrelated with the first $ k<p $ principal components,
                        $ \RV{W}_1,\ldots,\RV{W}_k $, the $ \Var{\RV{W}} $ is maximized when $ \RV{W}=\RV{W}_{k+1} $,
                        the $ (k+1)\textsuperscript{th} $ principal component.
                        \begin{framed}
                              \begin{align*}
                                    \Cov{\RV{W},\RV{W}_i}
                                     & =\E*{\Vector{v}'(\RVector{X}-\Vector{\mu})(\RVector{X}-\Vector{\mu})' \Vector{v}_i}                                                                           \\
                                     & =\Vector{v}'\E*{(\RVector{X}-\Vector{\mu})(\RVector{X}-\Vector{\mu})'}\Vector{v}_i                                                                            \\
                                     & =\Vector{v}'\Var{\RVector{X}}\Vector{v}_i                                                                                                                     \\
                                     & =\Vector{v}'\Matrix{\Sigma}\Vector{v}_i                                                                                                                       \\
                                     & =\Vector{c}' \Matrix{V}\Matrix{\Sigma}\Vector{v}_i                                                                                                            \\
                                     & =\lambda_i \Vector{c}' \Matrix{V}\Vector{v}_i                                       &  & \text{ since $ \Matrix{\Sigma}\Vector{v}_i=\lambda_i \Vector{v}_i $} \\
                                     & =\lambda_i(\Matrix{V}\Vector{c})'\Vector{v}_i                                                                                                                 \\
                                     & =\lambda_i c_i.
                              \end{align*}
                              The maximum of the previous result under $ \lambda_i c_i=0 $ for $ i=1,\ldots,p $ is when
                              $ \Vector{v}=\Vector{v}_{k+1} $.
                        \end{framed}
            \end{enumerate}
      \item The proportion of the total variation explained by the $ i\textsuperscript{th} $ principal component is
            $ \lambda_i/\tr{\Matrix{\Sigma}} $.
      \item The proportion of the total variation explained by the first $ k $ principal components is
            \[ \frac{\sum_{i=1}^{k}\lambda_i}{\tr{\Matrix{\Sigma}}}. \]
\end{itemize}
The last two key results follow directly from the fourth result in the ``some results'' subsection.
\subsection*{Next Step}
\begin{itemize}
      \item The next step is to do some principal components analyses.
      \item We will do this in R.
      \item R code from class is posted on the course materials page.
      \item So don't waste your time writing down the code; rather, try to focus on
            understanding the analyses (while taking any necessary notes).
\end{itemize}